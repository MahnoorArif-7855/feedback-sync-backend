feedback,date
"User story: I have my data in KBC, it was tough but it works, I’ve cleaned them up (in workspace and then in transfromation) an now I want to build some dashbouards, reports etc in my tool of choice. Snowflake writer does this, but you have to select table by table in awkward navigation.",
"Hey guys, the new description panel is great but it’s confusing for clients on the level of trasnformations.
It says Author:Keboola and it’s not entirely clear that we talk about the component (as typical user wouldn’t even know it’s a “component”)
Similar but not that problematic applies to component type. it says “Transformation” which is kinda obvious - might be of some help to say it’s python/r/sflk/ but even then these pieces seem redundant/confusing on Transformation level?",
"We had a second call with Fuga and Gerrit provided comprehensive feedback:
https://drive.google.com/file/d/1u2dGTCB6ef6n1M6i6oQqd-6oxqFq9tFi/view 
Feedback
great documentation with backlinks
tutorials, online help, quick response on tickets
did not have time to play with transformation yet
option of multiple workspaces for transformation is great
lack of documentation of application and triggers
missing templates(confused with different names - scafolds)
debugging is great on component level, would welcome even more debugging
(jira connector(or BQ writer) said there is an error with conversion but it did not say which table)
option of selecting the whole data set is not available for him (each table need to be selected) CHECK IT
not happy setting up SSH tunnel, interested in two sided TLS comunication
posible to use Google interconnect (cubernetes) or vpn
automatic schema integration - it would be good if it get the data types from data source (BQ writer)
change data control works well
interested in the costs (how much would cost what he as done so far)
successfully connected to jira, zendesk; would like gitlab 
missing google sheet connecter (it turns out there is G drive connector for that) 
like time travel, snapshots
Stitch can connect to G sheet, check the revision history and based on that can create CDC - the date you choose (kbc can only pull data from today) 
love you can do component testing, notification, timezone 
their company is investing in snowflake
is about to try Stitch, Fivetran, Integrate io, seedata, mulesoft, data student (conversion from airtable to BQ), airbyte
have many tables in Airtable
provide rohlik usecase (TASK)
 
Did not like
hard to find specific component (if you are not sure what you are looking for) 
missing connector microsoft excel sheet stored G drive CHECK IT
BQ writer does not have Date (only timestamp)
hard to delete internal table 
if you read G sheet it automatically add _timestamp, if you do it again it add another _timestamp column
BQ storage comp - would welcome help with setting up array function 
elastic search allows only SSH 
 
Tricks 
in G sheet are more colums merge together it would bundle the data together
interested how to read files with specified name convention but it is changing based on the name of month (stored in Gdrive)
 
Kudos
snowflake
interface with jira
data takeout
flexibility of mapping
email notifications
have issues with tricky API with weird authetification (using cookie) CHECK IT (33min) 
is there a way how to flexibly create an authentification? 
great tool, strong cantidate to use it
like dxc use case 
like he does not need devops team to maintain the stack",
I see schedules…. but in this Q2 they are disabled and I have no way to see it from the overview screen… Screenshots provided.,
"Features:
Oath in CLI takze user muze autorizovat treba google analytics pri setupu templaty, ted musi jit do zalozeneho Flow a tam to dokoncit nebo bypassnout na UI kdyby se otevrelo Oauth okno uzivatel se autorizuje a autorizace se ulozi rovnou s vytvorenym konfigem ktery se prida do flows
HTML or markdown support in template dialog descriptions (plain text today, need to be able to have URLs
BDO could be link into description
Google big query writer (user needs to parse it now)
Search through description in configurations so that we could remove the technical ID which we place now into name of the configuration and move it to description (ideally finish global tags feature)
Snowflake writer - by default have our snowflake provided (so user doesn't have to do anything)
New popup that will ask me if I want to keboola provide or my own and want to provide credentials
UI/UX tunning:
copywriting (in progress with UX writer)
keep everywhere name Template
button naming
etc.
info panel on template detail page as we have on component detail page
show more logos of used components on template list / template detail (logos of all data sources and data destinations), max. 3 sources displayd, if more source show + icon 
snowflake logo should be different for data source, data destination, transformation
add tour for user to guide him and explain that some options are optionals
if I save one configuration we have to better visualize that is was configured (right now there is only little small green check icon)
when creating flow put organize components into phases where possible to have better visualization (Monika will prepare it)",
"I’m imagining a custom, isolated demo project that we can place on the website, exactly like this one from Airbyte: https://demo.airbyte.io/workspaces/b734c3d7-ece6-47e0-8f07-c4be707fbcfa/connections 
How hard would this be to implement? The sales team feels quite strongly about making it a near-term priority. 
Thanks! 

Jordan",
"Reported here:
https://community.keboola.com/post/trigger-based-orchestrations-62542c967b8281d8f20a13a2
There are to examples when our trigger cannot be used, but mainly the issue is that there is no way to reset the state programatically or via a button in the UI (ideal)
https://keboola.atlassian.net/wiki/spaces/SUP/pages/67010561/Event-based+trigger+debugging#Trigger-skew
info from David:
""
pokud vim tak je to tak, ze kdyz prijde nejakej update v dokonce i v cooldown periode nebo behem dne, tak se ta tabulka oznaci jako ze byla updatovana a tim se to cely rozhodi. Ten trigger by se mel spravne chovat tak, aby registroval updaty az po nejakym definovanym case, jinak je to na nic. Nemuzes porad resetovat state rucne. O api callu nevim, ja to delal znovuvytvorenim. Event trigr cekajici na vic nez jednu tabulku proto nikde nepouzivame a ja vzdycky doporucuju udelat si umelou tabulku, kde si to nasimulujou
""",
"is not possible to modify the bucket name that's created automatically. I'm asking to the devs of the component if they can help us out here !

https://keboola.zendesk.com/agent/tickets/22889",
"https://keboola.zendesk.com/agent/tickets/22057
I would like to create a new weekly orchestration, which would have only one task inside - our daily orchestration. Then I'd like to use variables in the weekly orchestration, that would set all incremental parameters (for extractors, transformations and writers) to full load. This would be perfect, as we wouldn't have to create and maintain duplicate configurations of components or a duplicate orchestration.
I initially tested this using Keboola CLI and was able to change ""period_from"" parameter of Hubspot extractor. Although I hit a problem with setting the ""incremental_output"" parameter of the Hubspot extractor, as it expects an integer (0/1), but variables are strings. I get an error when trying to put my variable under the ""incremental_output"" parameter in the config.json of the Hubspot extractor.
Similar problem with changing incremental output for transformations - the parameter expects boolean.
Is there any way around this? It's very important for us that this works to keep the project scalable and organized.
------------------
Submitted from: 
https://connection.eu-central-1.keboola.com/admin/projects/3089/branch/3275/orchestrations-v2/401692515
Also related: https://keboola.zendesk.com/agent/tickets/16612",
"Right now, it’s not possible to perform incremental syncs with Google Calendar in Keboola. Actually, it is possible via the “legacy method” but it’d be great to add the newer functionality. 
https://developers.google.com/calendar/api/guides/sync ",
"Jupyter notebook can be exported to a pdf (or even to a presentation). This is nice for doing reproducible data analysis and create report out of it. 
use case:
1. perform a data analysis with some chart on a dataset. Include both description in markdown, code and its results
2. save it to a pdf file using latex
3. schedule a run and periodically send pdf using email.",
"We have several triggered orchestrations from PowerApps (through KBC API), but this way we aren’t able to send error notifications to people who are set to get them in an orchestration. Could you add the option to send all notifications even for manual orchestration run?
Thank you!",
"This is just to open the topic up again. I am tired with writing simple one-off registered components to do something specific for a usecase that cannot be used elsewhere. Just because the transformations cannot have JSON config/secret value. This limitation just leads to our customers leaking their credentials over and over again, because it's still just easier to have it within transformation. I do that myself sometimes, because I just don't want to clog our Dev portal Vendor and need to do something quickly.
This used the be the coolest feature Keboola had and back in 2018 on every demo everyone would go WOW when showing this. I could do quick integration using this in minutes just during the demo, imagine this. I don't think there's any valid reason to not have it there. Clients are asking for it on their own (even those who didn't know custom science before, just search through zendesk).
I think it would be nice to just add option to a Python/R transformation with user config. That would solve everything, or switch the Python/R transformation to a special component mode that would enable that. Having code blocks would be still quite nice. Then you could easily create a component or a button that would create a component from a transformation. (for instance we had one that generated lambda function from transformation).
I will write this as a component myself, just to make my life easier otherwise. ",
"When having CLI for KBC, it would be nice to also have a tool for easily accessing developer portal from command line as well. 
It could support commands such as 
list all (public/private) components for XY stack + filtering (get components with specific feature/autor/stack/...)
create component
get component detail -> get component configuration schema/features/...
This would be mainly productivity booster for internal debugging. 
It seems to be of a similar functionality as components listing in Looker, but in CLI form.
Maybe it might be just implemented into KBC CLI?",
"Dear all, 
we have troubles very ofter with GoodData API time out.  For the same config it randomly runs x fails.   I blame GD API for that. 
Would that be please possible to implement ""retry"" in GD extractor in case of timeout ? .  
We tried and if we run it again after while it works. 
Thank you in advance for considering this. 
Tereza",
"As GDrive ex is the most used component, I can imagine that ability to download CSV, and not just the Spreadsheets, would be great a feature.
In my experience, I’m always thinking about using S3 when dealing with how to periodically store and extract CSV files, but that means running new service besides GDrive I’m already using.",
"Hello, 
there is serious bug in storage graph where you cannot find the nodes with new transforamations and therefore it is useless. 
Also it would be great to include the writers as well. 
Thanks
Josef",
"We would like to be able to use Keboola CLI / Keboola as a code as part of a standard merge request / code review workflow and be able to test the changes in Keboola before merging to master. This would basicaly mean that GIT branches = Keboola branches.
Example: 
1. Keboola configuration is beeing pulled to Github via Github actions
2. A data engineer wants to develop a new feature in Keboola project
3. Data engineer pulls the repo to local using GIT
4. DE creates a new branch with GIT on his local copy (or they created it earlier in UI)
5. Data engineer is done with the development, pushes the changes (in his GIT dev branch) to Github Keboola repo
6. Push worflow is triggered, changes (the newly created dev branch with newly developed feature inside) gets pushed to Keboola project
7. The development branch is now visible in Keboola, so it can be run in isolated environment
8. Data engineer sends merge request
9. Code review is done using comments added to Github etc.
10. Code review passed, development branch is merged into master on the Github repo
11. Push workflow is triggered
12. Changes (to master branch) are pushed to Keboola project
13. New feature is now in production",
It has all qualities of apps (e.g. there is input mapping) but it's listed among data sources for some reason,
Currently I have to authorize each sheet that I am extracting it would be nice if after authorizing a excel file i could add multiple sheets without having to reauthorize for each sheet,
"Hi, 
Some clients prefer to have A/D authentication when assigning database users.  Would be great to have this option.",
"Currently this is only available for synapse/exasol but it would be beneficial to:
1. prep the tables upfront and then run pipeline. At the moment the development flow is kinda hit & miss efforts when user is triggering jobs step by step in order to develop.
2. create empty tables for data templates so the mapping is complete and Monika does not have to add dummy transformation to create tables as a part of the template (and then user should delete it - imagine the struggle for us to communicate this to users)",
"Seriously, this is an industry standard, why we have to do this via support tickets?
https://connection.keboola.com/admin/account/
Thanks.",
When loading data with an autoincrement to snowflake it would be ideal to be abe to add a row to the data for a timestamp when it is loaded.  This way i would not have to create a transformation where i upload both the old data and the new data just to identify the differences in the data,
"Hi,
I would appreciate a filter for sub-accounts in the manager account.
Right now the extractor automatically downloads data for all child accounts. We have quite a few customers in our manager account and we are usually interested only in max 4 sub-accounts in one KBC project.
Because of this extractor, our customers have data of our different customers in their projects and we consider this as a security issue.
Michal",
"Idea based on the recent experience - for quite some time, the Xero extractor has not been extracting any data, but it always ended up with a success, so the problem has been recognized too late, when it affected subsequent processes.
It would be great if user could force error if extractor is not returning any data, or even if it is not updating particular table (it can easily be getting some basic dimension data, but not data for important fact table).
You would need to set up some transformations to check for the last timestamps of the data and issue a warning when it's too old. Ability to set it up directly in the UI would be supr dupr feature, which can prevent potentially big issues for the companies using KBC.",
Feedback from our client Telly: It is top top top wish and need to be able to run each code block separately. ,
"To my knowledge there are multiple options how to ""migrate the project"":
CLI backup and restore - super finicky, often causes errors, data migration is super slow.
Keboola migration app - some stuff is not migrated so it requires quite a bit of manual setup, such as data catalogs https://help.keboola.com/management/project/migration/#what-wont-be-migrated
Migrating through Keboola as Code - does not handle the data.
would it be possible to improve the migration tool? I will ask Leo to put together a list of issues he has bounced on. 
Btw. We have created bunch of QA scripts that might come handy:
config comparison
data catalog comparison
orchestrations meta and configs
transformation configs (io, outputs, queries)
tables comparison - PKs, Row num, count distinct PKs
orchestration notifications (apparently we had issues with those)",
Seeing the difference in table size/ row counts between the input and output tables of the snowflake writer would go a long way in determining if the incremental load is behaving correctly or balooning the table.,
"Some APIs are simply not returning data correctly or vary the datasets returned between jobs. One would expect we could simply store columns that are present and mark the missing ones as nulls. - we could easily overcome issues with different returned data - templates, automations, complicated data sources (shoptet, hubspot, wordpress, etc.)
I would imagine user could turn this option on the table level or bucket level - to allow fillings of nullable columns.",
"It would be great and totally makes sence for data flow templates if users would be able to select snowflake database provided by keboola if they don't have their own. It's the same option as in the UI. 
Moreover, as we want data flow templates to work properly and not only as simple copy-paste feature, I think it will be necessary. 
More context here: https://keboolaglobal.slack.com/archives/C02C3GD6Q64/p1645449684888299
Thanks.",
"Hi in orchestration/flow there is identical icon for snowflake transformation and snowflake writer, it would be nice to distinguish it, it can be confusing for more complex workflows.",
"When I go from Orchestration to job detail and click to the link in header it takes me to flow instead of taking me back to orchestration.
When I have more than 10components it very messy in flow builder view. The flow should automatically switch to list view inside builder when detecting that they are more than X components",
When I use templete and am redirected to flow builder this message will be shown and that's confusing because the flow is already done,
I have noticed that some components have a JSON Editor and some do not.  It is really convienient to be able to click on the editor and see and change the parameters used in the component.  Would it be possible to have a JSON editor available for all components?,
"when the DB table is bigger and you have to scroll horizontaly (left-right), it is quite painful when you have to scoll to the bottom to scroll on the scollbar (if you use stupid mouse instead of touchpad)",
"Hi,
Our keboola setup is that all flows have configured an email for notifications on errors and longer processing times, and all development on specific components happens from inside a development branch. We often have multiple active branches that need to execute flows.
The fact that the developer needs to remember to manually disable the notifications (and re-enable them before merging) is error prone and causes a lot of unnecessary alerts, which spams our monitoring system because it does not know whether it’s the alert is coming from Production or a development branch.
Is it possible to make the default behavior of dev branches to not send notifications?

Thanks,
Inês",
"When adopting Keboola, large customers are particularly thinking about the sustainability of the entire platform. One of the big topics in discussions with architects is the maintenance of data pipelines in Keboola Connection. They are concerned about the changes that will be coming over the life of the data pipeline and any changes. They are worried that the whole data pipeline will fall apart if they change something in a few years.
They often try to use Keboola Connection as a HUB and playground for unfamiliar users. However, the entire DWH core tries to build outside the Keboola Connection using its own resources. This is due to a lack of analysis of possible changes (data object, transformation) to the multi-project data pipeline.

Typical Data Pipeline from customers PoV:
Source Systems -> KBC as HUB -> Snowflake DWH with Data Framework (L0, L1, L2) -> KBC Projects for end-users (L3)

Customers want to use Data Frameworks, where they define a BDM and this framework generates the necessary scripts for Snowflake. This framework is then the entire center of the DWH world and Keboola is relegated to the periphery as the HUB and sandboxes for users.",
"Enterprise customers often incorrectly assume that Keboola has tools that are capable of analyzing the potential impact of data object changes within Keboola (across all projects).
Among other things, we lack a capable built-in tool that can analyze SQL transformations.
The product team recently introduced me to an SQLFlow component that has been tested and declared usable within hackathon. According to the product team, the implementation should take a few MDs.
Together with the built-in data lineage, I consider the implementation of SQLFlow to be extremely important. Customers often blame us for this missing part of data governance.",
I’m not able to see history/rever when I edit notification settings in the flow.,
It would be great if the component’s state was versioned. If I click on the “Clear State” button the state is gone with no way to revert it.,
"Hi,
I found a request that is same to my problem - https://community.keboola.com/post/variables-with-hidden-values-in-transformations-60dabde8c3a2fa5952de6695  . Basically, I would appreciate if passwords for ad-hoc transformations in Python could be hashed like they can be in e.g. generic extrator. I tried to create a variable with # in name, but that doesn’t seem to work for transformation variables.
Thank you!
Best regards,
Jan Richter",
New date format is hard to read. I prefer old one. For example try to find lowest date in attachment. Did you manage it quickly?,
"Transformations failing because the pkeys differ after they are defined seems counterintuitive. They should only be maintained in one location, even if we can initially define them in the transform.",
"Input and output mapping information in the Job log is confusing sometimes.
It only shows tables that went in and out but sometimes users would expect that it’ll display also the details of that tables as at the moment of the execution. But it shows total row count and size no matter what filters were used etc.
It might be great to display something like:
tableName (filter: x, rows: y, size: z, max timestamp: m)
Those filters might be handled by seeing exactly what version of Tr was executed and then eventually checking settings of input of that version but.. 
Also for outputs seeing in the log if it’s incremental etc. would be nice for the overall debug purposes so that user doesn’t have to jump back to the config all the time. 

Anyway that max timestamp in the input tables feels super important when debugging as it’s an only thing I think might help to be able to do retrospective testing/debugging. 
When something fails etc. it’s crucial to test it on the version of data available at the moment of the failed run. Yup, not possible for full load tables but… might be given some thought/brainstorming on how to make this better.",
"We discussed how Keboola and Snowflake partnership can work and how to position ourselves together etc. which @Pabu01 can surely comment on in details.I brought back some topic(s) which feel crucial now as Snowflake would like to bring us to their existing accounts first (we talk Siemens, Schwarzkopf and such) - where client already has data in SFLK but needs to actually start using it. And when it comes to new accounts it would typically be someone running on Teradata or BigQuery.
Need to be able to connect Keboola to existing Snowflake instance in a way that they’ll see their existing data from the Storage explorer or at least have it accessible in workspaces and transformations. I imagine having an app where you’d list down projects you want to create with list of schemas/tables/… that should be accessible from it. Or just creating one project that sees it all and then proceeding via data catalog as normally. Not sure what blocks this on our end but it feels like the deals won’t be doable without this - as I dunno how to explain to these giants that we want to help them to work easily with the data but first they have to move it from one place in their SFLK to another.
Same applies to other backends (Teradata, BigQuery in future) - there are many opportunities where clients would like start using SFLK but are on TD/BQ now and thus would need to have Keboola on top of their current TD and alongside have Keboola on top of their new SFLK - they could run jobs on their current backend while creating new stuff in SFLK and also use that to easily manage migration of data etc.
Being on GCP seems “mandatory” to be successful - most of the big retailers run on GCP - would be willing to use SFLK but everything else (the rest of data stack) needs to be in GCP while providing the same features on top of BigQuery and the Snowflake (related to previous points)
Snowpark resonated a lot even among the Sales team so it seems that only the bare fact of us supporting it might be a big selling point. I drafted a document with notes from testing (work in progress) some description of how it could help our clients here.
NOTE: I phrased it as best as I can in writing but am aware of it not being 100% clear - I’d love to pass the feedback in person/on call directly - does it make sense? Please let me know @product (@padak @tomis). Surely @Pabu01 (and I hope for @fisa) should be there to highlight the potential there is on the market…",
https://support.keboola.com/hc/en-us/requests/23016 ,
BQ writer does not support Date? only timestamp - why?,
BQ writer - can we help set up an array function?,
can we connect to Google sheet and its revision history and select the date it should start getting data (Stitch supports that),
how to connect to Microsoft excel sheet stored at Google Drive?,
Is there a way how to read files with specified name convention but it is changing based on the name of the month? (stored in G drive),
"When you delete the configuration/transformation that is part of orchestration, it will stay there and cause crash, as it no longer exists. But the orchestration still has it in the tasks.
Could you set it up to be automatically deleted or at least add some warning, when the configuration is part of an orchestration to not forget to delete it from there.

Thank you
Natalia ",
It appears failure emails now just list the fact that a failure occurred and now require you to log into keboola to view what the error actually is. We have a couple orchestrations where we are forcing errors for data quality purposes. It would be helpful if the error message was displayed in the email once again,
"Issue: Currently, trying to extract a large number of files (>10 million) from S3 causes our extractor to run out of memory and fail. This is because the extractor will always list all of the files. 

Request: Update the S3 Extractor to handle pagination.",
less verbose error than needed - the issue was actually that I have used “master” branch instead of “main” - but error message did not tell me the issue was in branches.,
read only access enabled but this error occurred,
"There is a link under Workspace Parameters:
This workspace came with some pre installed packages. Read more in the documentation.

It leads to a site where we mention that users can add packages in the UI (it’s from Transformations) which instantly confuses the user as they see it, go back to workspace and dont see the option to add packages in UI. ",
"Whenever I click on Graph in Storage there is only empty screen. 
https://keboola.zendesk.com/agent/tickets/23220",
"This can just be a UI change - It’s not safe to run concurrent DDLs in Firebolt, so we need to try to prevent it. ",
"Problem: Its really hard to set safe environment for change management based on Keboola. We have branches, which are great, but how do we implement testing as a part of standard routine? How could Keboola help to identify its safe? Common data engineering practice is to implement testing and profiling information for the merge.
our CLI approach is fine, but that does not solve the issue - users do have to set up their own test framework and we don’t help them at all.
Whereas dbt will help us out in a long run, there is a big apetite from our existing users to set up such safe testing environment. Moreover there is a Datafold that just published free dbt change monitoring suite. Please check out their quick video:
https://datafold-public.s3.us-west-2.amazonaws.com/homepage-demo.mp4

Idea - what if we instrument data-diff to provide similar diff-like artifacts? That could be the right move to facilitate change management through branches - and possibly help out to push branches to be used more by our clients.",
"Back at it again with Dynamo… 
Reason: To put it nicely, our customers don’t quite understand how to effectively design their Dynamo Schemas. As a result, the customer’s choice of partition_key and sort_key can make querying almost impossible  
As a next step beyond the simple query function, Keboola would have the BEST DYNAMO EXTRACTOR on the market if we implemented CDC using the Dynamo Streams API. I checked, and no other ETL provider offers that ability. Documentation can be found here: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html 
Thanks, 
Jordan",
"In the new Storage UI is very easy to drop buckets. In the old one I was able to delete only one by one.
Now I can select multiple buckets and run batch action to drop them.
It will be great when user have option to restore them.
They are asking our support to restore it. It is made by L2 and 24/7.
In last couple of months I has been restoring buckets one or twice per 2 weeks.
It is manual work directly in Snowflake and we restore only data and raw table in storage (missing additional metadata and primary key setup)",
"Not all extractors support incremental (or real incremental) fetching - e.g. to continue where extractor left.
But for most cases (imho) this could by solved by dynamic variables. E.g. to have an option to fetch data from “last two days” or “last four days”, etc.
Basically I don’t care if extractor downloads some data again, I am ok with downloading data daily with “last two days” interval.
Example:
SELECT * FROM “users” WHERE “createdAd” >= {{createdAt}}
and “somewhere” where {{createdAt}} is defined I want to do something like:
createdAt = format(“Y-m-d H:i:s”, timestamp - 86400 * 2)
Let’s say I run extractor right now (Sat, 18.6.2022 14:56:00), the variable will be set to “2022-06-16 14:56:00”.
If I have feature like this I will no longer care about “native support” of incremental fetching in extractor, just define dynamic variable and every time I run extractor, the variable will be set to “-2 days from now”.
I am happy to provide more description if needed.",
"If someone shares a workspace with me, I am able to delete it.
There is no way to restore it. Not even for the author of the workspace.
That is not a nice behaviour  ",
"Guys, not sure whether this is relevant or not, as it is related to the way SNFLK works. But you might know who is the right person to talk to in this case to maybe let SNFLK know…
There was a ticket here: https://keboola.zendesk.com/agent/tickets/23307
The problem is our fwd'ing of an error from SNFLK, saying “Numeric value is not recognized”. The user does not see any context where the error occurs. Please see the ticket as it will give you more context.
Thanks.
Pavel Synek",
"When attempting to utilise the Google Drive component I have noticed that it can only read Google Sheets, however, we have a number of files stored in CSV and Excel formats that we would like to read. 
As we are a Google Workplace based company, we would like to utilise the existing infrastructure so wondered if it would be possible to work on expanding the capability of the Google Drive component so that it can read those file types. 
This would significantly support a number of our ETL process as we do have clients that share files via Google Drive that we would like to be able to read. 
I am happy to provide further info if required. 

Regards
Luigi",
"Hi, we had a meeting with Makro and they provided us with some product feedback: 
Rest API 
It’s quite difficult and not very intuitive set-up within generic extractor 
Missing information about failures 
If you get an failure message after trying to run something in KBC, you do not get an information what’s the issue and you need to search for it, which delay the final outcome
Missing auto-restart for some incomplete transformations
In case some transformations won’t be completed due to some reason (time limit, incomplete transformation etc.), there will be an auto-restart for incomplete transformations.  
They are happy to provide you with more detailed feedback, if needed. 
Thanks,
Anna",
"Hi there,

I often have the task of doing some changes on our schema in a branch (e.g. adding a column). The data on storage is mostly written to a Snowflake writer already configured. 

On such cases, I would like to be able to include the changes of the Snowflake writer on my branch, however because branch-storage data is separated from production-data, new/dropped columns do not show up unless I create a new configuration row.
Since I don’t want to delete and create a new configuration row every time our schema changes, our process now involves changing the Snowflake writer only once the branch is merged. This is obviously error prone and untested.

Is it possible to add a “refresh with branch storage” button for configuration rows on Snwoflake writers?

Thanks,
Inês",
"Hi,
last run (manual) was succesful and flow shows that there was an error. Is it correct behaviour or it should be succesfull in the flow as well?
Thanks 
Dave",
"Hi there, 
I just started using Keboola and took the intro tour, realizing that some UI elements are a bit off (see screenshot). Jobs is actually pointing at Transformations (but this issue happens with other elements as well, starting at Use Cases). 
It’s also not possible to go back from this step (that blue arrow left), as it is in other steps. 
Hope this feedback helps you improve your product  ",
"Input mapping of python workspaces check table size before loading. If the table is huge it will not load it because it wouldn’t fit. But it doesn’t care about filters I use. So when I have gigantic table and want to load data that changed in last 1 hour into my Python workspace I cannot do it because it will fail saying that the table is too big. 
That makes the filters useless (in a way) and is really annoying that I first have to run a SQL transformation to slice the table to be able to then load it to Python workspace.",
"Guys, I got a feedback from DPD, that the most important feature for them would be getting as close as possible to real-time analytics. Their logistic use case depends heavily on keeping the staff on the move without using any static depot. Therefore the situation changes randomly and routes need immediate adjustments. Therefore, if you have a point addressing such UCs, please add DPD to it as well.
Hope at least this brief info is sufficient to give you at least an idea, what they are asking for.
Thanks.
Pavel",
"I would like to suggest to implement the automatic termination of orchestration job after some time, for example 3 hours of running the specific job. The notifications are not sufficient. Thank you so much for consideration.",
the bar chart is nice to see average job runs. but some client use the same config so frequently that we only see the last day or two and have to scroll through the log. could the chart be enriched with some time zooming to see say average per day throughout last month?,
"This is particularly useful for high frequent jobs - if we run every 5 mins then this chart becomes not useful at all. What would help:
zoom in/out
toggle to move timeline to the past/present
 i.e. we need to optionally see more jobs and with different granularity.",
"Hello there, 
I need to merge my account associated to contact@alexgenvoese.it into this one. 
Is it possible? 

Thank you",
"Hello, 
when we use the output mapping pop up and type in the name of the table, the text always disappears in the middle of typing. After the text disappears, the second typing usually works. It's a minor issue, but still pretty annoying. 
Best Regards, 
Zdeněk H.

https://keboola.zendesk.com/agent/tickets/23556",
"Credem Banca is moving to the cloud and they will choose Google as a cloud provider.
Potential prospect in the banking sector in Italy 24-50 ARR",
BQ writer is there a way how to select the whole bucket or do I need to select each table seperately?,
"it would be great (for us, keboola support) to know if the project is BYODB or not. What I can see is just “snowflake” but it would be cool to see ID of it or something…",
"Conduit rozšířil CDC konektory a zda se, ze se to vážně chytá. Je to cely open source postaveny na Go.
 Imho bych radši splachl cely CDC/stream data delivery jednim resenim než znovudesignovat každý konektor zvlášť a resit milion veci kolem. Podíváš se na to? Dokáže to jet jako docker bokem, cely se to do ovládat přes API a dává to ven metriky co se daji pak pěkne zobrazit v UI. A mělo by to jasný impact na revenue…
https://github.com/ConduitIO/conduit
Tady je list konektoru vcetne komunitnich:
https://github.com/ConduitIO/conduit/blob/main/docs/connectors.md",
"Use “_timestamp” to eval freshness and display it on the storage screen.
This could be a paid feature/we could charge for it as regular queries…
This is the transformation that calculates it on the fly:
Keboola Data Quality Dev - Transformations / Data Quality
User could turn it on and we could set notification if it goes above threshold.",
"we have received “feature not bug” response on this issue:
A bug in the platform that occurs when user deletes column (or table is replaced and column is missing). However, user datatype casting still persists the column in the config - hence the error message.
reply - it's not bug that IM is not updated when column is deleted, it's just how it works.

I think this is poor UX because the platform should be able to cope with this automatically, at least to ignore config columns that are not present in the data… or null them…
more info incl. screenshots:
https://keboola.zendesk.com/agent/tickets/23659",
"The ThoughtSpot connector is outdated - it uses obsolete method of importing data into TS that is not used and supported anymore.

At this point, as TS said, the connector is unusable.",
"I really like the possibility of seeing which configurations have suffered changes with making a project diff, however changes on SQL code of a transformation are often the most important differences we have and these are not readable at all through the UI.
Often I end up copying the diff and pasting it on an editor and format it by hand.",
"Offer an option to load data for analysis into DuckDB - we could omit the whole CSV unload and use Parquet unload - much faster and natively supported by SFLK and DuckDB.

Outcome: 
1. faster loads of the data into workspaces as an alternative to load data into SQL warehouse and query from it.
2. Use DuckDB marketing power and general data community craze to increase visibility.

Resources:
https://duckdb.org/docs/data/overview#parquet-loading 
Magic Data Apps with Snowflake, Streamlit, and DuckDB | by Felipe Hoffa | Snowflake | May, 2022 | Medium",
"I've got a question about the MSSQL Writer. We currently have a Writer set up for one database and this writer contains about ~100 tables that run on a daily basis.
However we've run into an issue where if one of the tables fails, then only the preceeding tables are written into the database and the ones that are set up after the table that failed are not. 
Is there any way to replicate the Orchestrations feature ""Continue on Failure"" in the MSSQL Writer? Or is it Keboola best practice to separate one big writer into multiple writers, so that only a subset of tables will not be written into the database (We'd like to avoid this due to the sheer number of tables, and it does not solve the underlying issue)?",
"For incrementally loaded tables on storage, it would be super useful to keep track of how many rows were loaded/changed to a table each time a transformation runs.
Currently this is not visible anywhere, but an histogram similar to ‘Latest import time’ would be great to detect any patterns or spot bugs that are otherwise difficult to identify.
Another option would be to simply add this information to a transformation’s log.",
"Either enable anyone to use query history extractor or even better enable org/project admins to create SFLK role for MONITOR EXECUTION rights on database (i.e. project level). 
query history is frequently asked thing and we would be able to deliver that the same way as telemetry and metadata Ex.
That would:
enable anyone to use extractor for query history
enable customers or PS team to help integrating 3rd party tools for data quality, observability governance and lineage.",
"imagine we would be able to collect other jobs and provide an overview under a singkle umbrella.
first, it might come handy if I am using 3rd party tool for ingest, data prep, reverse ETL.
Second, it might come handy in dbt initiative:
register external job (start+end)
create a child job within each dbt runs executed in Keboola - in this way we would track the job history for model creation. aka to see what is going on in each dbt execution.",
"I am submitting this request on behalf of client Harri. 
Given the business of Harri (OEM), they would need more “rockets” = sizes of dynamic backends for BYODB Snowflake than 3. 
They have processes for development, where XS would be enough. 
But on the other they have also super demanding transformations with strict SLAs where they would need some larger backends. 
And a lot of different use cases and clients and different SLAs and requirements. 
So they need to use the most cost/speed effective solution as possible. Because they basically sell it as “product”. Therefore Harri requests more sizes of dynamic backend than three to chose from. 
I learnt from tech it is only UI limitation. 
Would you please help to find some solution so Harri can chose from more than three backend sizes for Snowflake transformations? Thank you.
If yes, what would be time estimate for this? ",
Duration of one step of the flow took 15min but the subtask (generic extractor) took 47 sec. The same with transformation.,
"https://www.querybook.org 
just opensourced - collaborative notebook",
"Can we please the name of the tab to “Templates”
“Shopify to Datastudio” template 
Please change Datastudio to Data Studio
Please change the Google Sheet logo to Data Studio Logo
The H1 on that page says “Add New Use Cases” - who is adding these use cases, because the customer is not adding a new use cases they are more searching for a template. ",
"Every table in storage could have a list of sources that write data in it or a simple log from which could be transformations or extractors listed and they could be accessed from there. It would help to prevent asynchronously made changes.

The same feature could work concerning writers. It would be nice to know in which writer the table is present.",
"This is a nasty bug if validated.
More info here:
Ticket: Nullable parameter of column does not… – Keboola.com – Zendesk",
"when I edit tasks in transformations I click on Save and then there is no CTA , something like ""go back"". The only way are breadcrumbs which I dont really find as a good option",
Add flow or flow task retry value and retry delay parameters,
"Old job on previous version (first image)
Job on latest version (second image)",
"Hello,
I would like to kindly suggest to implement shorter refresh period of the backgound telemetry data for our organization in your servers at least every 1 hour instead of 2 hours. It would really help us to monitore our processes from different systems.
Thank you so much.",
"Hi all it will be very awesome if can platform accept my login across all tabs in browser.
Steps to reproduce:
login to platform
open multiple tabs (storage, trans , extractors)
close your browser at Friday
open your browser at Monday
login via Gogle MFA 
most of Friday tabs will stay on login page (even after multiple refresh)
Some tab will “take“ login from another tab usually storage.

Kind regards M",
"We have multiple cases when we would like to create triggers of the external infra and grab the results. Those tasks should imho not be billed the same way. Imagine waiting 3 hours for some external job and being billed for that.
I am thinking for dbt cloud trigger. Dagster and Airflow uses the system of triggers and sensors, which does not matter that much, but what matters is that it would allow us to bill those differently.

This is what is actually happening with our Orchestration trigger. We bill our clients for it like there is some job crunching data… See a screenshot of credits billed per Orchestration V2 component…
I think this should be an async trigger and webhook back, but we dont have infra for that?",
"Display limit on max. parallel jobs in Project setting detail page. 

We are often ask about “how many parallel tasks” can be executed and then can only guess whether it’s the default 10 or more for the project. It’s important for the users so that they could also create their flows accordingly.
There might be other details such as list of components for which there is a custom setup of memory for the given project etc. ",
Keboola currently supports only classic SSH tunnel. While some competition supports reverse SSH tunelling like https://fivetran.com/docs/databases/connection-options#reversesshtunnel.,
"Hello,
we often download table from Keboola and share it on Sharepoint. It would be nice to have a option to export table directly as *.xlsx format.
Best regards,
Jan",
Would it be possible to include into UI some beautifier app which would help to read the log?,
"Hello,
we have encountered a problem that during an event triggered orchestration.
If there are changes on a table during a cooldown period after a first change which triggers the orchestration, the changes are ignored and there is no trigger afterwards.
Expected behaviour would be to have some kind of queue or something that triggers the update if there were any changes during the cooldown period.
Could you please look at it and let us know if this change would be possible. This causes some issues especially during an end of a working day since it can cause some transactions to be processed too late.

Thank you very much,
Jirka Lorenczyk",
Portable.io (US only) uses a nice button in the notification section - allowing users to add Slack notification on a few clicks. Pretty neat!,
"Add link to source table in data catalog. (user story - user realizes, that his pipeline is using shared table, has access to both projects and want to see, hiw is this shared table created)
Add link to trigerred flow in external project (not really sure, how to achieve this, need to check it out)
Links between workspaces and transformations - let the user click back and forth between worspaces created from transformations and vice versa, now the navigation seems just one way (need to check)",
"The P3LP customer is missing an approval role in the CICD in the Keboola project. Both senior and junior developers have access to the project, and the customer does not want junior developers to have the right to merge changes without prior review by a senior colleague.",
"re:data (data observability/quality nad MDS a dbt) ted spustili uplne dementni sluzbu: hostuj artefakty... podporuji dbt, pandas profiling, great expectations, jupyter a dalsi. je to de facto fazona nad S3 kde to nahazeji, nic vic ale v MDS je hlad i po tyhle kokotine.
https://www.getre.io/#product 
a my to cely mame davno pred ksichtem... komponenty generuji artefakty a muzem nad tou storage postavit jen par preview funkci. Takovy ACDSee na datovy artefakty. Plus by se dala zpoplatnit ta storage $$. at si tam klidne sypou co chteji, nebo namountuji svoje S3/Blob na tyhle data
pak dava smysl napsat jednoduchou profiling app, great expectations app a zobrazovat to krasne v platforme",
"Sending feedback request according to: 
https://keboolaglobal.slack.com/archives/CGCKT849E/p1662093017987929

“Znovu zkouším keboola CLI, a tentokrát to funguje bezchybně, super práce!
Jak ale strukturovat python kód, ať to je ideálně validní i v IDE?
Mám to rozdělený do jednotlivejch bloků. Třeba jsem vyhodil importy zvlášť - tak to předtím fungovalo.
ale v IDE se to moc nelíbí, protože se to importuje v jiným souboru.
nemáte někde sepsaný best practice?”",
"Hi Keboola, I would like to have an option to run a dry-run of orchestration - it would be processed without any data, in separate db schema for etc. Our main pipeline runs multiple hours, having an option of dry-run it (ad-hoc or periodically every evening) would really help to find syntax errors, missing input mappings etc.",
"Guys, on Mall we opened the question of how to streamline the data sharing between two organizations (data feed from Mall to Heureka) within KBC. This is something similar we have already solved in the past between Manifesto and Storyous. I believe this might open up another opp to increase the stickiness of the platform.
Do we have any possibility to allow for that? Is the process documented anywhere?
Thank you.
I am referring to a previous feedback received from Tomas Fejfar in Slack here:

https://keboolaglobal.slack.com/archives/C02PGBVTF54/p1664201310736369",
"I found out that writers cannot run with parameters when re-running a job. I have used five writers with parameters in a flow that i wanted to partially re-run, each for one table (because it is not possible to specify more tables in one writer’s parameters). However, it attempted to load all tables five times instead of loading five tables. (Jobs 24807554 and 24807680)",
"Ability to retry/ use conditions to retry the flows
Při failu – celkovém, opakování celého flow? (úroveň reportu nebo konektoru) – výpadek systému, zpoždění/nedostupnost dat, atd.",
"multi-account settings - having clever backfill mechanism in the case of failure.
when account fails - you can download duplicate data by default (download 3 days history every day, so if one day fails, next day the data is backfilled) and let automated Keboola deduplication adeduplicate it 
Backfill data - have an ability to turn on automatic backfilling of the data from a past.
backfill - it's standard that customers backfill their data first (weeks/months) and then set up incremental loads. Really depends on concrete case - how much of the data is backfilled. 
Comment from PS team: Backfill is something that bothers us in general, since there is no framework for components/flow param that would allow to define and execute it. again, I think we shall ask comp team if they can draft any solution that would allow component team and customers to build such logic in the components. Otherwise this will remain a manual task for a person, or some super complex logic within components and weird settings within
Advanced anti-sampling -like feature to allow overcome GA limits
Ad connector – obejití limitů (např. počet sloupců – dimenzí/metrik) -> stažení na pozadí několika reportů (cca 15-20) a vracení co nejmenšího množství reportů 
Comment from PS team - this sounds similar to anti-sampling feature.All of this is not something PS or CF can help, aside of gathering better feedback and sending it to a platform feedback ticket.",
"Lukas Jirman rika: 
Mall prechazi na Google KMS Vault
Use case je pouzit Google KMS pro logovani users do keboola pokud s nami zustanou. 
Pokud budou prechazet na BigQuery tak bysme je mohli hodit do control plane modu kde bychom nasadili pouzili jejich Key Vault",
"Nastavim si komponentu, hezky vidim v liste, ze je to data source, kdo je jeji autor, link na dokumentaci a ze nema data.
Pustim ji, job uspesne dobehne a dokud nedam refresh, porad vidim, ze nema data…
Kdyz dam refresh, vidim show data a muzu na to kliknout, hodi me to do storage, coz je fajn, ale kdyby to melo nejakej hover, kterej mi rekne, ze ten link vede na bucket ve storage a ten ma nejakej name, rowcount a data size, pripadal bych si orientovanejsi.. Ten jeden klik je mega rychlej.",
"Slack thread for details: 
https://keboolaglobal.slack.com/archives/C02PGBVTF54/p1664390946017419",
"Databases are often available and used in multiple versions simultaneously. However, it is not clear which versions our extractors support.  I would appreciate a section under developer documentation where we include the list of supported versions.",
"I would suggest to make Snowflake DWHM manager a public feature that is documented?

It seems to me, clients love it. And the sooner they get it the better? As basically this is what every team could use? 
Recent examples from clients: 
- I realised only now that Rohlik does not have it, and even did not know about it. And would utilised it.
- Planetly was sharing their concerns during onboarding and finally DWHM solved it. 
- Now Kindred was saying that the only reason why they would need BYODB Snowflake is exactly the management of roles. But we have a solution, which nobody knows about.
- And also it is difficult to migrate when you start without it (Košík, Rohlík)?

So why not to make it official it it is that valuable and loved feature? So it is not “podpultové zboží” anymore? 
And we are now tracking direct queries as part of consumption. It can be also a nice “license” tiers differentiator? 
https://keboolaglobal.slack.com/archives/C02PGBVTF54/p1663774413985529",
"All described : https://docs.google.com/document/d/1GD8d979WzMllZKPERaE2fcqA0C6fw6YAE_bSvqWoIfk/edit#heading=h.82sur8wh0kca 
Motivation
Improve user experience of Keboola users.

Introduction
Components in Keboola all have configuration parameters so the component is instructed on what it should do. Many components share common configuration parameters which should all be named, described the same and should do the same thing.
Currently there is no set standard for these common configuration parameters which is confusing to users, as they are named and described differently in different components.

Desired Outcome
A Confluence page with a list of all common configuration parameters with their :
Name
Short Description
Documentation
Name in JSON configuration (see below)
The Name and short description will be used in the configuration UI
The documentation will be propagated to keboola documentation (http://help.keboola.com )
The documentation will be linked to the developer documentation (http://developers.keboola.com ) as part of the publishing process of a component to strictly enforce naming conventions.",
"Fully described here : https://docs.google.com/document/d/1uG7fZnXy1XSzIoaDeFArjeWDdjO36uopyHOL44Gimqc/edit#heading=h.bp52bbjqq2ak 
Motivation
Improve user experience of Keboola users and developers.

Introduction
Relates to effort of Standardization of component configuration naming. 
Once naming is set, the configuration UI should be split into UI blocks that can simply be used in components. If configuration parameters are always appearing in the same UI blocks, users can easily understand and set up multiple components in a row without having to understand new concepts that they already understand. Developers can also use UI best practices by copying the UI blocks and do not have to reinvent the wheel when making new components.
Desired Outcome
Developer documentation of all UI blocks (that can be slightly modified if necessary) there will be a list with what the JSON schema code is and how it looks in keboola. The blocks can then be visually separated (see appendix)",
"Fully described here : https://docs.google.com/document/d/10fuLn5Nb45no1DNSJ2AkFkOzgkHmSUCkpapzTrft-GA/edit# 
Motivation
Improve user experience of new/non-technical Keboola users while enabling power users to still do all the tweaking they need.
Introduction
We have many components that can be infinitely configurable. This is great for power users as they can do all they need with the components, but this in turn makes configuring components very difficult for newbies and non-technical users. We need to provide a UX so both get what they want; a Simple UI with advanced options.
Desired Outcome
A way to define simplified UIs for specific components and switch to an advanced view if necessary. The simple UI will prefill the advanced UI, so the component will still have the same underlying Configuration JSON file. ",
"Fully described here : https://docs.google.com/document/d/1WU19fpu7iUyDXeqxXQ9YU8i12CwNyIeGPvHmTa5CtkA/edit# 
Motivation
Improve user experience of Keboola users.
Introduction
Currently there is no way to separate the elements within JSON configuration Schema UI, this makes components with a lot of configuration parameters very confusing for users to configure, see example of 2 components 1 visually separated and 1 not.
Desired Outcome
Have the option in the JSON SCHEMA to visually separate the UI element when the property is ""type"": ""object""",
"In MySQL extractor (and any another DB extractor) an option to set time_zone would be helpful. Now, we have to switch most of our configurations to the Advanced mode and convert given column to the desired time_zone.",
"Video provided…
I had to use WTF hack to be able to name the phase",
"I am sending this ticket on behalf of a customer. A reporter is Košík - Zdenek Hanik <zdenek.hanik@kosik.cz>. You can contact him for more details. 

They developed a custom component and when they were looking for an icon, as per Slack, we concluded it might be worth product feedback request. 

Ve zkratce jde o aplikaci, která bude trigerovat orchestrace na základě updatu více tabulek, což v defaultním nastavení orchestrací/flows nefunguje úplně správně, nebo tak jak bychom chtěli.

Vidím tam hlavně tyto věci, které nám vadí:
Pokud u orchestrací, které běží denně jsou jako trigger nastaveny dvě a více tabulek, tak když se jedna z nich manuálně aktualizuje během dne, tak už se ta tabulka bere jako aktualizovaná pro orchestraci, která má běžet až následující den. Pokud tedy nemáš nastavený dost dlouhý cooldown, čemuž se chceme vyvarovat. Takže orchestrace na update této tabulky již „nečeká“ a data jsou napočítané bez ní.
Taky se může orchestrace spustit víckrát během dne, což taky nechceme.
Pokud se nějaká tabulka, která je někde nastavena jako trigger smaže, nebo se přestane aktualizovat. Tak se orchestrace nespustí a nebudem o tom nijak informovaní (už jsem takto asi měsíc neposílali nějaké data  )
Tohle jsem všechno v aplikaci ošetřil, plus doplnil ještě nějaké drobnosti.
Máme to nastavené zde:
https://connection.eu-central-1.keboola.com/admin/projects/3407/components/zh6.app-orchestration-trigger/483785658
Tady je dokumentace:
https://bitbucket.org/zh6/orchestration-trigger/src/master/ 
Eliška",
"Hi Keboola
We have run into a critical limitation of the S3 extractor in that it offers very little flexibility in searching/filtering/using wildcards to define which path and file to extract
This has been raised with support team https://support.keboola.com/hc/en-us/requests/24817 and looked into at length by Tyler Smith - both of whom have concluded there is no possibility to extract the required files even with the use of additional processors
The structure of the folder cannot be changed by the customer (Deliveroo) as the same structure is in place for all their customers.  And quite frankly there is nothing odd/illogical about the structure.  Currently, this is a blocker for progressing (an important) integration, developing a custom extractor appears to be non-trivial and we are hoping that S3 should be a widely used extractor and therefore would benefit from enhancing",
"Hey Team, 
Our customer - Nicholas Clancy (nicholas.clancy@engineroom.com.au), Product & Software Engineering Manager from Virtus Digital (rebranded as EngineRoom) - a marketing agency in Australia is unhappy with lack of usage/PPU visibility on the platform.

”I'd like to discuss moving to a monthly-billing cycle rather than 6-monthly. This is not because we are unhappy with Keboola and/or don't want to commit long-term, it's simply because there is a lack of a billing portal in the platform itself, there is no visibility on how money is being spent, how much runway we have left with credits, etc. 
Moving to a monthly billing cycle would enable us to monitor our platform usage better accordingly and be more beneficial for cashflow.”

I do share the consumption with them and chat with them often, but they want this part to be rather automated, present and available at any time, so they wouldn’t need to reach out to me everytime. 
I hope you find it insightful and take it into consideration for your next Sprint. 
Best,
Maria",
"I made a conflu page of how it currently works: https://keboola.atlassian.net/wiki/spaces/SUP/pages/2803925011/Reset+SQL+workspace+password 

IMHO there should just be one API endpoint to reset this shit. (yeah, it’s my fault)
see slack here if you want the gory details: 
https://keboolaglobal.slack.com/archives/C02PGBVTF54/p1666707900841789",
"HI gents why user can’t load primary columns settings in BQ extractor on one click from Storage as it works elswhere?

Have a great day. M",
"This was recommended by a customer, and it’s a super awesome idea. Basically, just a UI feature that allows people to “lock” production configurations so that it’s harder to accidentally change.",
"Hello,
I would like to give some feedback on the v2 transformation (SQL snowflake primarily).
The “Used in“ feature in extractors / writers (see clipping below) would be also really helpful in transformations.
In comparison to the old version of transformations, looking for some text (using Ctrl + F) is not so convenient. The search in the pop-up is cumbersome.
An option to disable a SQL block and/or a table on output mapping would be great. This would be helpful when testing minor changes in the whole configuration.
When copying a transformation, sometimes the version history of the former transformation would be needed. The copy always starts with clean version history.
Maybe the “QUALIFY“ SQL keyword could be also highlighted in red. In the same way as SELECT, GROUP BY, WHERE … (see clipping below)
To end on a positive note, I have to say, that I love the product. These are just small things that are sometimes annoying and it would be greatly appreciated if these could be fixed / added.
Best Regards,
Zdeněk H.",
"Feedback from Rohlik (high priority).
The “Processing” notification is applied to individual jobs in Flow but not to the Flow as such. Thus users receive notifications when some of the jobs takes significantly longer (but it can mean nothing in context of the entire Flow - it can be negligible). Rohlik says that they frequently receive these “false positive” notifications and need to be notified ONLY if the WHOLE Flow takes longer than usual.",
"I need to run the same workspace IM in 5 different projects. 
I need 5 tables. 
Expected
I can somehow “copy paste” the IM
Actual
I need to do it manually
Suggested
Allow Import/Export IM / OM as JSON (UI only?)",
"adding a new IM
there are tables foo, foo-bar, bar-bar in a bucket dodo
I filter out foo
I see foo, foo-bar in dodo bucket
I click to add dodo
Expected
What I saw in the dropdown is added. I have foo and foo-bar in IM
Actual
Everything from the bucket is added. I have all three tables in IM",
"viz. 
https://keboolaglobal.slack.com/archives/C022PNH855X/p1667932598949949Read only storage In short there is not a clear process how to set up 
requires admin permissions
no way back
must be activated on all projects
native types 
no choice to set if extractor will be used with native types or not, its automatic
we cant select what fields will have native types, we need to set for only some columns
no conversion function to set a format of date and json structure
Harri moved already several projects to Queuev2 (and like dynamic backends, which is a main driver). They would like to move also production projects. @Eliška Svobodová to check with a tech what is the expected downtime (as it influences their customers",
"Hi, I am confused what is Automate feature in Components for since, in fact it just create Flow. Therefore I would expect it is called something like “Schedule“ as called in Flows. I would appreciate to have basic information  what will happen (that it actually creates a Schedule in the Flow.
Thanks a lot",
"Like we have on db extractors, would be good to have a refresh sync action to re-fetch the structure from google

From ZD ticket https://keboola.zendesk.com/agent/tickets/25402

Thanks.
marc",
"It looks like dropdown I always try to click on it  
Martin",
vedet jaka orchestrace transformaci pousti,
"Hi, we are running optimization algorithms and want to use the Pyomo library. This library is not battery-included and solvers have to be installed for it. For us this means we want to do 
RUN apt-get install -y -qq coinor-cbcin the docker file in order to have the CBC solver. We can’t install it purely for the script due to no authorization. We found out we can do a fork of your Python component and change the definition of the docker to include this command, but then we only have this as a transformation (which we have to deploy using CLI, but that seems okay so far). It still leaves us unable to have a workspace for dev purposes, meaning we have to run this on other tech stack for dev and hope we can publish the custom component and make this work in production runs. Any guidance on how to solve this would be appreciated. I hope we can get this to work.",
"Hi,
Carvago (Tomáš Trnka) was asking for enabling read-only storage on their main and biggest project as it would help them a lot. After a discussion with our tech through support (#25230) it would be risky to activate it on requested project as it would take quite some time to activate and if anything happen, there’s not way to get back. Due to missing rollback feature they decide to not move forward for now. 
Is there anything we can take a look at this to see how to help them to start to use this feature?
Many thanks,
Anna",
"Ticket: Dear Keboola team, as my company… – Keboola.com – Zendesk
Bartosz Do Trong
bartosz.do.trong@stileo.it

Dear Keboola team,
as my company recently joined with Glami, your client, and I had a pleasure to cooperate with the BI team, I'd like to share some thoughts and findings with you which I perceive your platform is missing and whic could have sped up our work a lot. Please consider those as platform improvement ideas - I'd be happy to discuss further.
MySQL Connector:
initial query to run at the connection (common for all underlying queries). This could be data formatting change, session variable etc.
ability to set a variable at connection (component) level for further use in underlying queries (e.g. when running similar query on different components - this could have been storing the name of source component)
ability to copy query from one component to another (currently there's only an option to clone entire component)
templated queries - same queries to be run on different components (connections)
storage table truncation in the orchestration. Currently there are cases when we need to purge the table but then to fill it incrementally from different components. We either clear them with simple transformation loading 0 data into output table (which consumes too more resources that actually would be needed) or we run one non-incremental load and then all the others in the next orchestration phase (which reduces the parallel run of all components as it requires at least time of 2 phases of loads)
more orchestration task failure options: currently there's only ""continue on failure"" boolean option but I think it would be handy to have ""retry"", ""retry X times"", ""continue"", ""continue with notification"", ""abort""
components folders: ability to organize components in a buckets according to user structure, not only component type
Hope you'll find at least some of these insightful. Looking forward to hear from you,
Best regards,
Bartosz Do Trong",
"Currently, we have only exclusive locks on SAPI job queue. That means that only one job can work with one resource at any one time. 
It’s been good enough for a long time. Until https://keboola.zendesk.com/agent/tickets/25474 (CSAS). 
She needs to export file to ABS from Synapse. But export job locks the source table. So any other jobs would have to wait until the 2TB table is exported. 
This would be solved by exclusive and shared locks. The jobs that need to read the table would have shared lock. But when someone wants to write into the table, they’d need exclusive lock. And that can’t be obtained if someone has shared lock. So the write job would need to wait. But that’s usually not a problem. ",
"Ahoj, 
posílám feedback od Taste od Přemysla Horáčka.  
Dříve (u těch původních orchestrations) v případě warningu/erroru chodilo do mailu upozornění a bylo tam přímo vidět, jaká komponenta krachla. Nyní, u těch nových flows přijde taky upozornění, ale jediné, co z toho dozvíš, je hláška ""Container job finished with warning.""
Myslím, že to původní bylo lepší, když jsi hned viděl, co se stalo.
Anička",
Can you update or create a new google sheet component with upgrade. The upgrade will be creating a copy of a specific sheet to the clients drive before writing data into the sheet. The new google sheet writer will work as the normal one just with the additional job - copying specific google sheet (which is public/visible to everyone). The creation will be just one time per config. ,
"Enable users to provde runtime value for variables in Flow UI or to set it for all tasks in UI. It’s enabled for transformations and I know it’ll at some point be added for other components, too (I hope) but it’d be really great to have that in Flow builder. Currently we have to use a orchestration trigger app just to run a Flow with custom value of variables.",
"Hi, I would like to ask you to change an order for the list of Roles while I am inviting a new user. Now, the order is: Admin - > Guest → Read only - > Share. 
Is there any chance it represent the “strength” of the role, therefore Share -> Admin - > Guest → Read only, i.e. similar to our documentation 
Thanks a lot",
"I’d like to filter only OUT buckets in Storage UI - but when I filter OUT it lists tables as well and there’s no way to see a list of out buckets only.
thanks!",
"Hi, by any change, would be possible to add button for editing all queries in transformations in the same way how you added the “copy all queries” and “show all queries”? Because when I'm using this features I get he code in certain format which is slightly different then one showed inside the json which I´m able get from the KBC extension tool. So I´m able to copy/show all the queries, but then If I need to apply some changes them I need to go block by block and copy paste the queries. Or I can edit it through the extension, but It is even more annoying. So sum it up I would appreciate the possibility to edit the code at the level of all queries, but with the similar code format as I get from “copy all queries” button. 
Let me know, what you think about. 
Thanks in advance.
Kristyna Jaško",
"Hi,
if now transformation return dataset with duplicates in primary key  - one row is pickeced randomly and others are threw away, which hide errors in transformations. 
From my point of view, if I setup primary key the transformation should failed instead of this.
But probably I´m not only one who sent you tickets about this - so there are some reasons for this behaviour I suppose.
So my suggestion is, that there could be option to turn on warning, that occures when rows are threw away.
Best regards,
Jan",
"It feels too big to be just part of maintenance and maybe it could be useful for clients as well. 
The goal is to be able to quickly notice if a lot of jobs start to fail on a similar error message. 
Usual causes:
some external API is down (FB API for example)
deploying broken version of component (that’s the main problem to solve from #prod_24_7____inc_7902)
some problem in platform that’s used by a subset of components (e.g. we removed YAML support and missed few components and all of them started failing imediately)
If thought out correctly, this could be an insanely good user experience for when we screw up - they could receive a ticket saying “we’re seeing elevated number of errors on XY component, our team is already notified”… way before they notice themselves. ",
"Most of the SAPI does not have pagination possibilities. This creates issues when querying the metadata of large projects. Pagination would make the fetching and handling this data much simpler.
Example: 
The list tables endpoint : https://keboola.docs.apiary.io/#reference/tables/list-tables/list-all-tables  can give large amounts of data when fetching all metadata,buckets,columns,columnMetadata. Pagination would make that much more manageable.
I know I can list all buckets and then list all tables from each bucket, but if one bucket has a ton of metadata and column metadata i can see those responses becoming large as well.
The issue popped up when a single request of the list tables endpoint caused a memory error in the metadata extractor, this would be way easier to solve if we can pagintate and not download all the data at once.
Apart from the metadata extractor usecase, even standard users using tools like postman can be affected as the responses are too large ",
"Hi there,
I have a feedback about minor (but a bit annoying) issue with rerunning of the new version of orchestrations/flows.
In the V1 orchestrations, when I used the rerun functionality (to rerun only part of the pipeline) only the configurations which were checked as “enabled” were set to run in the rerun config.
However, when rerunning the new version of the orchestrations/flows, now all configs are check regardless of whether they are enabled or not (see attached screenshot).
I would appreciate if this could be fixed to work as in the previous version.
Thanks for your help.
Kind regards,
Jan",
"Sometimes it can happen that multiple processes are trying to write into a storage table. Table is locked and therefore one process is waiting. However, this is not displayed anywhere in the lock. It would be nice if there were a clear message indicating this. 

it can be seen for instance https://connection.keboola.com/admin/projects/9039/jobs/926599291?q=durationSeconds%3A>1200+AND+params.config%3A857768865 at timestamp Nov 30 12:26",
"Hello guys, i wonder why in the new transformations whenever i get the error it's not providing any hints as of where the error occurred? Not even the phase nor block name.
In the old transformations whenever i think there is usually a ""go to query"" button. Is there a plan to perhaps make error more traceable?
Slava
Ticket: Hello guys, i wonder why in the new… – Keboola.com – Zendesk",
"Zdravim, Kristyno,
no-code data manipulation jsem zatim testovala jen okrajove, planuju to jeste poradne otestovat, az budu mit vic casu - urcite uz muzu rict, ze je to dost powerful a uzivatelsky privetive a akorat si musim zvyknout, ze rychle odpovedi na data muzu najit tam. Premyslela jsem i o tom, jestli stoji za to to predstavit nekomu nedatovemu, aby si to zkusil.

Vic jsem zkoumala zmenu v souvislosti s native data types, kde by to tedy chtelo lepsi dokumentaci, protoze mi chvili trvalo zjistit, jak s novymi tabulkami pracovat. Kolega z toho byl hodne zmateny a finalne jsme to udelali tak, ze odkazujeme tabulku s nativnimi data types pomoci cele cesty. Kolegovi se ale moc nezamlouva, ze tabulky nema moznost dat do input mappingu a neni tak zrejme, z jakych dat transformace cerpa. Navic mam ted problem s nahranim tabulky s native data types z mysql extractoru, s chybou, ktera mi nedava moc smysl, jelikoz to taham z naseho backendu a tam by se ta chyba nemohla stat - bylo by fajn si moct zvolit, jestli chci nahrat s native data types nebo bez.

Nejdulezitejsi vec pro me je aktualne to, ze od zapnuti features mi hlasi vsechny orchestrace, ze trvaji dele nez obvykle. Promitne se toto v billingu?

Dam urcite vedet, jakmile se dostanu k dalsimu otestovani no-code manipulation.",
"Hi,
I’d like to raise a feature request that would allow users to break the alias link between two tables and assign a new table to it without having to delete and recreate the alias. The reason for this is to support MPA and change management procedures that we have recommended for Evans and will prescribe for future enterprise clients. 

Thanks,
TJ ",
Because we will probably never get support for functions in endpoint section we could mimic it by enabling variables in the generic extractor. It may be just about addition of particular flag in developer portal.,
" Rohlík sees our platform not very reliable, examples:
Issues from spring, a lot of Sev 1 tickets, escalation to CTO.
Another escalation this week for failed orchestrations due to our bug.
And there were more so Rohlik started to create a list of various issues they faced in the last month reported in this ticket (thx Petr Huňka).
I mean, we usually sort it via Zendesk sooner or later, but it does not help them that a data driven company is without data.
https://keboolaglobal.slack.com/archives/CTS84CKKL/p1668161072630069",
"Hi guys,
are you able to enhance the current solution to the point that unlink and link the bucket name will not have an impact for changing input mapping respectively.
Thanks a lot,
Tomas",
"posledni rok preklopeni do multiprojektove architektury (core BI tym Zdenek & Ondra
Transformace
napocty delaji ve starych transformacich, nove projekty jedou na nove fronte a novych transformacich
dependence ve starych tranformacich nepouzivaji a vsechno maji jako clone
v nove transformaci to stoji vice penez
u kazdeho codesnippetu mit PLAY button
v novych transformacich maji hromadu input mappingu a output mappingu a je to gulas 
read-only storage zapnout pro odstraneni IM a pro odstraneni OM vypisu zavolame OpenAI a nechame vypsat ktere tabulky jsou pouzity u kazdeho code snippetu
z IM na radku tabulky bude reference na proklik na dany codesnippet
search nefunguje v novych transformacich a vyhledavat je pain, ve starych je rychlejsi, musim zobrazit show all queries a kdyz dam browse search tak to nehleda v popupu ale v pozadi
uzke okno show all queries
moznost editovat kod v show all queries
meziokno pro explore data je zbytecne a lepsi rovnou hodit usera do storage - data sample
data sample v transformacich nepodporuje filtrovani
ve starych transformacich lze prokliknout z error hlasky na konkretni blok (v emailu)
ve starych transformacich neni videt proklik v Last use ktera orchestrace to pouziva
u Python transformace musim davat ID file do kodu i kdyz mam file otagovane
moznost zaskrtnout najednou nullable a convert empty values to null (todo)
do prehledu transformaci da sloupce pridat used in
Orchestrace
notifikacni emaily se obcas nezobrazi v Teamsech kvuli prilozene tabulce = dodelat Teams notifikace
trigrovani orchestraci https://keboola.atlassian.net/jira/servicedesk/projects/HELP/queues/custom/46/HELP-645
Dry run ve flows bez extraktoru a writeru - pouze transformace z pohledu syntaxe
Storage / Data Catalog
v data samplu chybi filtery pokud jdu z transformace
pomoci regularnich vyrazu
datove typy - zapnout betu
BUG prejmenovani tabulek ve sdilenych bucketech - kdyz prejmenuji nazev bucketu v master projektu tak se nepropise do projektu do ktereho to sdilim
v add table search zobrazi i jiny vysledek vyhledavani nez hledam, projekt 250  eu central
table usage v prehledu abych videl jestli tabulka je nekde pouzita nebo ne
do Usage dat i transformace (jen nove:-)
v UI drzet poradi sloupcu a moznost si to seradit jak potrebuju
prejmenovani sloupcu ve schema - musim dropnout sloupec a zalozit novy (todo)
lepe videt jestli je tabulka sharovana a do jakeho projektu => automaticky tag na prehledu tabulek
nejde vyfiltrovat prazdny string
Components
pri kopirovani konfigurace chci konfiguraci zkopirovat pouze s 3 config rows z 10 => rozsirit modal abych si mohl vybrat ktere config rows si zkopiruju
stejne ikonky pro snowflake componenty ve starych orchestracich
prevadeni casove zony z TZ na CET u dat z MySQL, musim udelat custom sql v componente => nutne nastavit TZ primo na MySQL serveru nebo mit moznost pridat vice queries - append processor ",
"vyzkoušeno s místní tabulkou mi to funguje.
konverze Datumu v CZ formátu DD.MM.YYYY  nefunguje.
Viz obrázek mám nyní dvě funkce druhá je nefunkční (např. špatná kvalita dat) chtěl bych před ní dát filtr a omezit data nelze dokud nesmažu  tu druhou.
Přidal bych funkci data-typovaní tabulky. Myslím si že hlavně číselné typy budou dělat problém.",
"Hey guys, let me just follow up with you on request of Skip Pay (Lukáš Horňák, who reported the issue). They have concerns about security in relation to Snowflake workspaces lacking MFA. With Tomas Fejfar from Tech we found a workaround in IP-range limitation set up in Snowflake, however it may not be a proper solution. Therefore, this is to propose a consideration of an Active ID (or SAML authentication or whatever other technology/protocol) that would enable making sure SNFLK is only accessed by an account recognized (even via MFA) at Keboola.
Hope it all makes sense. If not, please let me know and i will be happy to clarify.
Cheers
Pavel",
"It would be helpful if boolean values were saved in the same way they were saved in the database. 
The conversion from false to empty value is causing us some issues.
For example - we use “experiments” on our orders, “true” means that the experiment was active and used, “false” means that experiment was active but on used, and “null” means that experiment was not active or not applicable for the specific type of order.
So we lose information due to the conversion to the empty value.
https://help.keboola.com/components/extractors/database/mongodb/mapping/",
"Hi, we are finding ourselves utilising AirTable for a few business related elements and wondered if there is any plan on building out connectors to read/post data with AirTable? While this is not business critical, and therefore not a high priority, it is something we are likely to use more and more. 
For me, I am wanting to support the team in streamlining data processes so would be looking to build some proof of concept for them. 
Thanks in advance
Luigi",
Allow for adding a limit/throughput for how quickly API calls can be made. This will help alleviate any rate limiting from vendors who only allow so many calls per minute to be made,
"Hello Keboola hero,
I am newcomer, I really think, Keboola is super userfriendly platform for ETL but I miss (or just can not find) some functionalities which I used a lot when I did ETL on my own. Here we come to my question, is there any possibility to set extractors in a way - try extract data at specific time, if error/warning occures wait one hour and try again till for example end of the day. I used to set even some preferences like if I extracted majority of data (but not all), I processed them but after some time I tried to extract the rest of the data and then update tables. I really miss this functionality, is there any way I would the the same in Keboola? one more think, which is not so important for me, but it would mada my life easier - my coding is not readable for others, I would approciate functionality to format my code in the workspace according used language.
Thank you very much for your help.
Dasad",
"Hi all, 
imagine a situation like this you wanna work with “Jobs“ and ad to transform all matching tables.
If you click on the folder name it will set all tables from the storage (NOT all matching to search term).

Will you plan to do something with that?",
"Good morning,

I have started using Google Drive component in the data pipeline and would like to suggest an improvement…

Right now to be able to process a sheet, it needs to be “extremely clean”, that means every column where data appears needs to be named (no possible notes), no empty columns in between etc. Moreover it is a common practice to leave the first column and row empty to make occasional calculations. If any of the above does not apply, the component throws an error.

The feature would be work best when the first cell would be selected (i. e. B2) and then specification which rows/columns/range/up to the last non-empty row/column to transfer.

Thanks, Jan Klasek",
"Onboarding
difficult is the concept of workspace - each workspace for each transf.
Flows
pop-up is very small for choosing the configuration https://keboola.atlassian.net/browse/UT-406 
notification - processing longer than usual for some orchestration [link]
many task in one phase is a mess
how many task are in one phase https://keboola.atlassian.net/browse/UT-407 
speed in flow builder is slower compared to the orchestration
table trigger - 2 tables from 5 updated and orchestration started and it should not - [link to job ID]
missing trigger option - if table from shared project is not updated => trigger it based on event, but if specific target table was updated/not updated today/specific time
they want to try AI autogenerating documentation
Transformations
copy the transformation config is not available in transformation detail https://keboola.atlassian.net/browse/UT-408 
enabling/disabling codeblocks 
shared variable to be used directly in the code blocks
cannot hash credentials in Python transformation - better to have environment variables vs locking the python code and if editing the configuration the credentials must be rentered
CTRL+F is not working in code snippet (in progress)
possibility to download the older version directly without creating configuration https://keboola.atlassian.net/browse/UT-416 
SQL transformations - syntax formatting of code, I prepare it in DBraver and paste it into Keboola
notifications for current job not whole transformations - it would saves time and effort https://keboola.atlassian.net/browse/UT-409 
choosing multiple transformation might accidently open the transformations and I have to do it again
searching trough SQL script as well for specific string
Syntax check after saving the code block - showing green or red (https://keboola.atlassian.net/browse/UT-417 )
Workspaces
autorefresh of tables in IM SQL workspaces - be able to set the refresh at certain time
Storage
on search I cannot search table where name is equal orders (now it works as contains) = adjust it or show only orders table first and then the rest of the tables https://keboola.atlassian.net/browse/UT-411 
enable sorting for other columns in storage not only last change https://keboola.atlassian.net/browse/UT-412 
using Storage in CLI (plan detail session with Tamas tamas.horcsin@rohlik.cz coleague from Hungary)
wildcard search in data sample for more advanced searchnig - we plan to use gooddata for that quick exploration
in search filter add COLUMN https://keboola.atlassian.net/browse/UT-413 
Dev branches
merging different/more branches together before merging into production (need more details)
Jobs
not able to move back when I click in jobs graph
would be great to see runtime per codeblock in jobs detail
some error are hard to read (execution terminated, error per configuration rows for example google drive with 5 tables and I dont know which one was causing the problem https://keboola.atlassian.net/browse/UT-415 
in filter when filtering error it displays success jobs even there is error inside 
more detailed log (about the progress job if there is some bin log) - widget in job detail whats done and whats still in progress (number of tables)
Dashboard
useful info to have there: how many configurations, tables, jobs, what is running now, etc..
General
small clickable area in checkboxes https://keboola.atlassian.net/browse/UT-414 ",
Request asked here: https://keboola.zendesk.com/agent/tickets/26362 but mainly is add into logs how many rows/transactions were insterted into an extract,
"Hi I am talking with a person from marketing to set up authorization for google analytics, facebook, google ads, youtube etc.  It would be great if there was some sort of short video or instructions set up how to authorize an account that I could send to them to give the correct access rights for extraction to the account they provided to me",
"It is slightly annoying to see relative dates on job run runs in the job search. I would prefer all of them to be set as exact DateTime, like what happens after they are older than 24 hours. I know some people prefer or are ok with the relative dates. But if there was a custom option in the settings to set this, that would be nice. 
Now i have to click open each job to know the exact time.
This is the lowest of priorities, but I just wanted to record this feedback.",
"Some services like Xero or MS sharepoint refresh also refresh token on access token refresh. We deal with this by storing that token in state and using it in consecutive runs. When the component fails we loose this token because we can’t save the state.
MS is not such an issue because the refresh_token is invalidated after some time from the refresh. But in Xero this interval is 30 minutes which means the component needs to be reauthorized each time it fails.",
"Hey,
With some components it would be useful to have a ""Partially failed job run"" or call it something else. Maybe this will be solved once we have artefacts for job runs. But basically if a writer, e.g. salesforce writer is writing to an object and has 10 successfully updated records and 5 unsuccessfully updated records, then it would be good to tell the user that it partially failed and then return what went successfully and what failed in either the output tables or some artefact. Then they can fix the records that failed.",
"Hi guys. I know this has been reported before but reporting again - Please why don’t we display description of tables linked from catalog? It feels like easy fix and it’s just absolutely sad that we don’t have it there. It makes it impossible to actually implement data mesh.
Currently we deal with this with Actum and P3 as they implemented perfect multi-project strategy but are screwed as the users have no idea what’s inside of individual tables they link from catalog….",
"Actual behaviour:
Define table to be loaded using Oracle source, load fails for whateawer reasons (native types allowed)
Fix issue in load (and remove some columns)
Run load again, error happens as some columns are missing
Expected behaviour:
3. Run load again, no error happens as table/column metadata should be saved only after succefull data loads",
error messages look like this https://prnt.sc/8TlUzs1KG9Ow but it would be much more readable if it would look like this https://prnt.sc/mGOyrBmZwwaj ,
"possibility to create &merge changes between projects
automatic sync with git from project
User privileges in dev branch - merge role in UI
https://drive.google.com/file/d/1nnP0TqiDsqJXaPGMARFTjkyRRY9HbREz/view?usp=sharing (from minute 50:00)",
"Currently, if a client is trying to pull data from multiple ad-accounts in the same extractor, it takes a very long time. Would it be possible to parallelize the extraction process for different accounts?

Ref ticket: https://keboola.zendesk.com/agent/tickets/26601",
"Hi, it’s impossible to terminate the terminated users' workspace. Even as admin.

Petr",
"Hi, I found a possible security issue that I would like to discuss with you.
Previously, we had created a configuration in the snowflake writer called ""Metabase - Customer Care - hourly"" with workspace provided by Keboola, in which we wrote data. Subsequently, it was no longer needed, so we deleted the configuration. We had the credential stored separately. Today I tried logging into the workspace with them and found that even though we deleted the configuration, the workspace still exists including the previously written data. I understand, that you cannot automatically delete the snowflake workspace as it might be used also by other writers, etc. But currently we have no idea how many workspaces like that exists and what data they contains. Also we dont have access to them anymore and event if we have, we cannot delete them. That is not a good situation from the security perspective.
So it would be ideal if there was an overview of all available workspaces provided by Keboola and whether they are part of any Snowflake writer configuration. Alternatively, the last time any data was added to them or the last time any code was run on them. And of course, this overview should have an option to select and delete the workspace permanently.",
"Submitting this ticket on behalf of customer TV NOVA. 

Petr Šimon likes Development branches. TV Nova has a solid multiproject architecture, and he claims that a branch works only within project. So one cannot develop or test what happens if data are shared across more projects. So he would need to have it on “organisation level”

If you need more detail, the contact is petr.simon@nova.cz",
"Looks like everyone is jumping on VS code as IDE, which is a neat way to offer another interaction with the platform for specific audience.
I would say we have keboola as a code, but without storage and ability to run things. We have api which is too complicated and we have dbt through CLI. In a nutshell, VS code extension could offer integrated solution for code geeks, allowing them to seamlessly see project as a code, see storage and run components, all with integrated GIT and SQL editor… 
News: https://www.databricks.com/blog/2023/02/14/announcing-a-native-visual-studio-code-experience-for-databricks.html",
"search in VScode for configuration
missing the possibility to look to the storage, table names and id’s, schemas => If I want to check quickly data I open postman to check list of columns https://connection.eu-central-1.keboola.com/v2/storage/tables and the response in json
search for tables with similar columns
developers changes something in database they need to accomodate the change so they need to delete the column in each json and push changes
(nice-to-have) creating table not so much
(nice-to-have) when the transformation wasnt set up correctly they need to setup primary keys correctly afterwards
using a lot mySQL extractors with custom queries (select * + CZ/SK), its stored in json file, it would be better for formatting to have separate json file for those custom querries
issue with invalid state of project eventhough nobody touched it
(nice-to-have) Data catalog - initiate sharing to another project from CLI - changing json files or command for that",
"Hi gents, few days ago I reported issue with “Input Data Mapping“.
Tomaš report me that issue was resolved.
I would like to confirm that, but Im facing following issue.
https://connection.eu-central-1.keboola.com/admin/projects/3637/transformations-v2/keboola.snowflake-transformation/533151483
open Input mapping dialog select one table (result pic 1)
select another table (result pic2)
 
I know how to solve it “just click on“the corner of popup and you can confirm it.

Kinda poor UX",
"Is the HTTP data source only for CSV?
A small note from my side: it would be nice to name ""HTTP"" and ""Generic"" a bit better to convey that the first one is only for CSV and the second is also about HTTP (e.g. ""CSV HTTP"" and ""Generic HTTP"").
Would be nice to have a UI for the Generic datasource - I spent a lot of time trying to set one up...alternatively, setting up a json-schema + frontend validator would make development much faster.
In the end I kinda gave up on that task since it felt too much effort for a simple well-formatted JSON (and considering JSON over HTTP is a very typical data source).
By comparison, I was able to do the desired tasks much faster in Apache NiFi.
To be fair, I tried the Google Analytics datasource and it worked very well and relatively quickly (which is much harder to do in NiFi).
By utilizing processors, they can pull in more than just CSVs from an HTTP server. Our Generic extractor is very clearly distinct from the HTTP extractor.
That being said, he is right - you should submit a feature request on his behalf regarding ""a json-schema + frontend validator""",
"For tables with many rows, I always have to scroll down than move right and scroll back up to fill some filter. I know it is not quite standard, but what about putting the selection bar on top of column names?",
"Ticket #26916:

“for dbt remote snowflake, whats the possibility to use external table storage mapping or sharing the tables to the storage without manual uploading them
thanks”",
"@Petr Šimeček I have noticed at least 2 times in recent days customers asking for us automatically uppercase columns. Can we have this option? Its painful for BYODB and users with other tooling. Some of them actually use writers/transformations just to uppercase, which is stupid 
Martin Zajic
I don’t get it, you can’t create table table with uppercase column names? I’m not sure where to point with such a problem and where this options should be. We have by design everything case sensitive if database is case sensitive.
If someone have problems with columns not being uppercase than don’t use lowercase characters.
Martin Fiser
All extractors create lowercase tables. our storage slaps in-c. in lowercase, se even if the extractors handle this or through processors its not going to help.
Martin Fiser
so customers hesitate to use us as a data acquisition layer. - they still have to use transformations to conform data tables, or re-save them via writer.",
"Hi, we would like to connect our reporting directly to Keboola Storage, to save some time and resources we are considering naming tables and columns in local language and connect our reporting tool directly. 
Unfortunately its not possible to name elements with diacritics (“alphanumeric characters and underscores are allowed in column name“) in Keboola storage, yet I  guess this should be no issue for Snowflake => can you please allow this?",
"Hi can we add custom standard DB UI to the ProgressDB component? kds-team.ex-progress-db. I should be possible to just copypaste standard ui, it’s built on that php framework.
It’s needed for earthLink 40k$ deal. 

BTW we will be creating more db extractors → it’s worth to continue working on the generic UI capabilities so we can be on par with the standard custom UI and get off your back",
"Hi, we’ve discussed this in context with Snowpark but it’s valid also for other use-cases.

When user creates a shared code it’s only available in the respective project. However, it’s often needed to use the same shared codes across the entire organization - and creating+maintaining such codes in multiple projects is just overwhelming and feels unnecessary. It’d be great to either be able to mark specific shared code as “global” and display it among shared codes in all projects.
Other option I see is to create an organization-wide schema under each user can register a UDF/procedure/… and such functions are then accessible in any workspace(transformation) under that organization - ofc this comes with a need of some catalog. That’d be ultimately better solution than shared codes but it’s limited as our shared code can be anything and not just a function/procedure.
Some combination of both would be great. Or maybe take it as 2 different features - the second approach is surely a must for Snowpark as we discussed.

A context: we developed the “data quality testing” shared codes and during the EmPower got quite a strong feedback and interest of users and one of the questions were around maintainability and easiness of adding new tests. As it is developed as shared codes you’d have to always edit the master template and run that template in every project where you want to update it (for example) and also you simply need to register the same shared codes over and over again.",
"The Hands on Data Quality template creates shared codes in project where it’s run.
We’d need to be able to UPDATE existing shared codes (if present) if users run the template multiple times. Mainly in order to be able to update the master template and thus add new versions of the shared codes easily.
At the moment the client (not sure if CLI or python client?) doesn’t support “updating” of existing shared code I was told by David Esner. 
We’d need that to be able to easily add new tests to our DQ testing template.",
"Hi,
if a transformation is deleted, it is not removed from orchestrations automatically.
I think that this can be an improvement because if I delete a transformation, I don´t want to use it anymore, and keeping it in orchestration makes no sense.
This also can prevent the second problem - if a transformation is deleted, all transformations onward are “Cancelled“ (is there any reason for that, even in the case that deleted transformation is set as “Continued on Failure“?).
Best regards,
Jan",
"We already have storage API. I know its not an ideal way to get data off the platform. But what if we build upon metric layer technologies (dbt/cube/transform, potentially dbt one) and boost ur storage API to serve data, potentially with the cached views/metrics?
This would make us a true hub to access data by 3rd party systems, possibly with ODBC/JDBC drivers/SQL syntax that would make it easier to connect to BI tools, Streamlit apps etc.

Nexla has something similar already. We could ride the wave of the metric layer (dbt) and deliver a nice stickiness feature.
Data API | Nexla",
"Currently in verbose mode it does not print new lines properly. Old transformations provided pretty formatted detail in some log messages. Allow for additional formatting (MD) both in log message and log detail. GELF is a JSON message and allows sending arbitrary elements. We use it now for additional information and stack trace in the detail, not very usable because it does not respect newlines or formatting. 

One immediate usecase is Oracle writer that actually prints entire SQL and log on failures in that detail, or Python transformation (My forked Custom Python version), that prints details about package install and doesn’t clutter the log like the official Python TR does:",
"Ability to create migration scripts on UI level and allowing users to opt for the new version. This is currently done for TRv1/v2 migration but it is only UI level. I can imagine this could be sync-action type of job / component. These scripts should live with the component, not outside of it.

There is this ancient tool https://github.com/keboola/config-migration-tool somehow hooked to a product/ deprecation but it has multiple issues:
Extremely complex, php
The migration is tied to component deprecation. We do not want to register new component each time a structural change is made in UI to enhance UX. These changes are simply convertible
Because it is linked with new component version for users it is next to useless
They need to change all credentials again
They need to relink all transformations that are using the results because a new bucket is usually created.

There is no way to provide additional information about migration to the user through the platform. Only generic “Component is deprecated” message is displayed:

In an ideal world users should be able to opt in for new component (or UI) version or opt in for automatic updates (Already ready in UI for Python TR).",
"Ability to test new UI without deploying to production first. RIght now there is no way of seeing the result and testing buttons/ sync action dropdowns and new elements without deploying to production. Something similar UI team has available would help (https://kbc-ui-git-jakubk-ut-521-with-preview-keboola.vercel.app/app)

Ideally allowing to register multiple versions in developer portal and the ability to use selected version on project level.",
Documentation is good but the ideal component is configurable without actually having to lookup the docs. All it takes to enable this is allowing for those little i elements that reveal additional information in the tooltip. Already present in some custom UI components:,
"It could be a new format type of an object element. E.g. “type”:object, “format”:”section”
All custom UI based components have these sections.
There should be an option to make it collapsible (like on the screenshot). We could use it to hide the advanced settings so the UI is simpler
It would allow us to define the standard UI blocks -> initiative that was started last year
https://keboola.atlassian.net/servicedesk/customer/portal/3/HELP-632
https://keboola.atlassian.net/servicedesk/customer/portal/3/HELP-634",
"Some apps need only metadata about tables. Currently we need to mimic this via sync actions and share token features.  E.g. View Writer, Looker DataStudio wrapper app",
"JSON schema element group that is excluded from configuration. Otherwise it can also mess up templates / validation. With the introduction of sync action buttons we now have such elements that are not part of the configuration. We also quite often create helper elements to allow for some UI magic.

Currently these elements are saved within component configuration but not needed for the component to run.
I can imagine this related to https://keboola.atlassian.net/browse/HELP-847",
"Hi guys, I have some feedback here  

Comparison of available/consumed limits (minutes)
Would it be possible to display on billing-consumption dashboard somehow comparison of available/consumed limits (minutes) - basically inform clients whether they run of the credits/minutes and needs to take any action (maybe some trend that they will soon run out of credits - something similar to this report)
Also, it would be nice to have some “action button” which would redirect them to billing page/contact form and guide them how to purchase more credits/minutes.
Similar to Over Limit Active Contracts Last Month in this report. 
contract renewal
I would appreciate to have a information when my contract ends so I can take any action - probably only contract related. On the other hand, I am not sure whether this information is not confidential for defined users only?
Purchased minutes expiration:
this is more idea/question - have you ever considered defining “minutes expiration”? Story: I have PAYG and I need more credits - I will buy more credits (ideally the more I buy the cheaper it is) and I have an expiration date for these credits and therefore I am motivated to use KBC more intensively.
Thanks a lot,
Roman",
"For example, glancing at a job that starts at 8:54 am and ends at 9:45 pm, it’d be much easier to tell wether it took 1 or 13 hours if it ended at 21:45.

I think even our American customers that are maybe accustomed to the am/pm standard won’t mind since they are clever enough to know how to read times such as 21:45.  And if they’re not, then I don’t think they’ll have much success with keboola anyhow:)",
"Hello, are there plans to have edit possibility and save button in the “show all queries” interface of transformations v2?
It can work well to just edit one of the code blocks, but also its a pain to copy all the blocks to IDE at once, edit them and then have to select each code block separately to insert it into the transformation UI.
I’ll rather have just one code block at that point to not have to do the hassle and insert each block separately. But then I loose the added value of having neatly organized code in blocks…
So there currently is missing the feature to edit either a block or all of the blocks at once and either insert one changed block or all the blocks at once - as per individual situations.
So, do you please plan to have “edit all queries at once” in the future?
Thanks!",
"Hello,
when I click on a table in any MySQL extractor (and also SQL Server extractor) configuration using the wheel on my mouse two new tabs with that table are opened. I’m using Chrome on Windows.
Zdeněk H.",
"Hi guys, just a thought - the message displayed when creating SFLK WS in project with read-only storage enabled is a bit confusing.
Read-Only Access
Enabling this option grants read-only access to all project data without the need to define specific tables.
Maybe “without a need to configure input mapping and copy data into the workspace schema” or something like that? It doesn’t sound great but “to define specific tables” is absolutely confusing to me.",
"We have shared this idea before, but I didn’t find feature request for it. Modification of “data apps” so it can be linked to a configuration and receive configuration context.
The result of the app would be update of the configuration
It would receive token that has privileges to modify the linked config
It would receive the existing configuration of the component (credentials) like a normal component job.


Usecase:
This would give developer freedom to create complex UIs / configurators to accompany the generic UI.
Generic component configurators - imagine Generic Writer interactive configurator, allowing previewing resulting requests, selecting sections from available options, etc.
Advanced configuration when generic UI is not enough
PDF parser
Processor configurator 
Anything else.",
"Hey guys, 
here`s quite an interesting feedback from the customer (P3)…see a screenshot attached.

Thanks
Pavel",
"The feature for saving a description when there is changes is very hidden.  I hate that it’s not the default option or that there isn’t a way to force it to be a default option.

in an ideal state, i’d like it so that every single change in transformation code has a accompanying description/message.  but it’s hard to enforce across a team (or even myself, once i hit save i can’t go back and edit the description).  so it would be cool if there was an admin feature where you cannot save transformation code without adding a description. ",
,
"Hello, I just found out that when I run a transformation in a DEV branch, the output mapping creates a special branch bucket.
I understand that and it makes sense. Worse case is the fact that what the output mapping and input mapping sometimes shows in the branch transformation UI is not true. The UI mapping still leads to the PROD buckets.
But the output mapping does not work with the PROD buckets and if a branch bucket is created and is included in the input mapping then the input mapping also is not working with the PROD tables.
In a workspace, the input mappings loads the tables created in the branch - if the branch bucket matches the PROD one that is in the input mapping.
This is quite confusing that the mappings load something different that the UI says. In my opinion the branch mappings should also show that the mappings are working with the branch buckets. Its misleading now and I had to analyse what is happening there for a while to find out what data do I have in a workspace now.
I’m sharing an example in the screenshots below. Input mapping in a branch shows a table with 189M rows. But we created this table in a branch bucket and it has 0 rows and thats what I get when I load the workspace. I think I should see the one with 0 rows in the input mapping - and have it visually clear (the yellow DEV color in the mapping) that its the table in the dev branch bucket. And i would know I have to delete the bucket to get the PROD tables I need.
Thanks for any response and consideration!
Regards,
David",
"Hello,
having a multiproject organization of 60+ PROD projects, the so called “shared code” feature makes absolutely no sense if I can’t use a shared code in multiple projects..
(If I talk now only about shared code apart from other must-have mutliproject features, such as running a configuration from different project in one projects flow…)
Nothing else seems to me as a better use case for this. I am just designing a data quality process for different projects where I need to map different tables stored in a project to a transformation to run a query every day and see how the hash_agg of a table looks like in time. If it is not changed and values in the table are completely static, I don't need to update this table from a data source every day.
And if i don't want to share all those subject tables to one project and run the transformation with hash_agg there, then I am copying similar SQL query to all of those projects. And this is exactly where I see my best use case yet for shared code - but shared between projects.
Thanks for any consideration!
Regards,
David",
"Related to https://keboola.zendesk.com/agent/tickets/27489

Currently are workspaces set as TRANSIENT (1day time travel, no fail safe). This is because workspace for transformations would be quite expensive with timetravel. 
This customer used workspace to incrementally construct a table. Accidentally they switches to full load and were not able to reconstruct the data afterwards (after 24h+). 
It might be nice to have the option to differentiate between job and workspace workspaces and consider the workspace as “storage” with all the guarantees. This needs a switch in workspace creation, some updates to UI, etc. ",
"If we had incremental backfill support a lot of problems would disappear. Now we deal with “we need to load 1TB from this table and it fails on memory or timeout or whatever”. 

Incremental backfill would work like this:
you setup component in “incremental backfill” mode
you set maximum amount of data to extract
you schedule it to run every X minutes/hours
each run runs ~ “SELECT * FROM table WHERE primary_key >= $lastValueFromState ORDER BY primary_key ASC LIMIT 1000”
That way it loads data by batches without any user intervention until all data is loaded
Advantages:
the data fits to memory
the job finishes faster
if job fails we loose only 1000 rows, not 200GB of data from job running for 10 hours
Ref.: https://keboola.slack.com/archives/C052GEZ8UQ2/p1681124238462369",
"WHY: to effectively work in development branches also with storage objects.
Use case: I adjusted the transformation with new logic and new column in the table and when I hit merge I expect Keboola will call storage API and table will be merged together with transformation so I dont have to go to production and add column manually or add one more extra transformation which will do these changes (trough CLI)
WHAT:User can select in dev branch if one/some/all newly created tables, changes in tables will be merged into production or not
ETA: H1 2023",
It would be really useful to have a table column metadata base type for structs/objects and lists (both probably encoded as JSON strings in CSVs). It can be rather inconvenient to do flat tables and/or translate to Snowflake obejcts/arrays at the start of transformations etc. This could be easily automated.,
"Hi team,
I have been trying to update only one list in a sheet instead of the whole sheet, and there is currently no way to achieve it. 
Can you please add this option?
I’m aware that most people likely don’t use Keboola for the same purpose, but we are using an output from the transformation to update input in a Google Drive sheet with many lists. 
Thank you in advance for looking into it, and have a pleasant remainder of your day!
Best regards,
Tomas",
"WHY:I want to allow/invite users to merge changes into production
WHAT:Separate user role or invitation for merging into production -> we will create only merge request where I can add comment and invite other colleague to review my changes and merge it. Email will be sent with link to DEV branch. I can also do force merge but I need to provide mandatory comment why I do that. CLI is not supported
DEFINITION OF DONE:Admin can invite user in the project to review and merge changes in branch to production
ETA: H1/2023",
"Hi there, 
as promised, here`s a ticket related to Renomia and their PoC on building a DWH being performed by Revolt BI. They keep running into troubles. Some are submitted as ticket to our support, some are not. Anyway, if the PoC goes well, we might see additional 80-10K USD ARR from this customer. If not, they might decide to build it on a dedicated DBT without KBC - I already got the warning from the customer. So current consumption might even drop down. 
Therefore, please have a look at the file here (created by Revolt): https://docs.google.com/document/d/1ahcJno32e4cZVEaqiW81tB2ZnGM9VWSAQhVhbHEAPlY/edit
I think we should give them some feedback and/or schedule a call with Revolt and Renomia to discuss our point of view with some outline on what we can do about the points highlighted by Revolt and some ETA for the points we can squeeze into out backlog.
Many thanks.
Pavel
cc @Azzurra Valtorta ",
"I am currently working on a project that has enabled Read-Only storage, which is being rolled out to all projects (https://changelog.keboola.com/2023-04-24-general-release-of-read-only-access-to-storage/). However, I am feeling a bit confused by this recent change.
Even with Read-Only storage enabled, I still have access to input mapping in the UI for Snowflake transformation. I'm not entirely sure what the use case for this is, since all tables are available directly in the workspace.
I've noticed that when I create a workspace from a Snowflake transformation without making any changes to the default settings, the workspace does not match the behavior of the transformation. Specifically, the workspace does not have Read-Only storage activated by default, even though the transformation does. This discrepancy is quite confusing and could lead to errors if not addressed.",
"Hey team,
There are 2 errors that sometimes occur with extractors output.
Column name length error (max 64)
duplicate column names error
With the column name length we have to add extra code to reduce the length of columns and this makes the columns less readable. Would it be possible to increase the length? Or create some standardised way of dealing with it and let the Storage handle it; e.g. shorten the name and add a “Full Name“ metadata field? This also happens with storage extractors and people have to add processors which deal with this that might not be intuitive to alot of users.
With the Duplicate name errors, they occur if a out table has the columns “name” and “Name” for example, its not ideal for them to have it this way, but the storage lowercases the columns and then there is a duplicate error, maybe this should also be solved by storage?

Thanks",
"When you have native datatypes feature turned on and you are loading data from Oracle DB, and table that has new rows since last load the load fails, with following error:
During the import of typed tables new columns can't be added. Extra columns found: ""XZ"".
 
> Could you allow adding new columns for on all connectors (crucial for our use case is Oracle and MySQL)",
"Problem
CSAS needs to find a substitute for direct access which was in Synapse. Currently, we do support read-only storage which provides access to whole storage in transformations and workspaces. The downside of this solution is its destructive behavior against automatic generation  of data lineage. For this reason, CSAS can’t use it. It’s legit comment from their side.
It’s not possible/hard to clone our logic around KEBOOLA_$PROJECTID_RO role creation because integration with Kebooly Connection is required. E.g. unlinking the shared bucket cannot be processed without Keboola Connection integration.
What I need
The problem can be easily solved if Keboola will still take care of KEBOOLA_$PROJECTID_RO role creation and its maintenance, but RO role will be not granted to WORKSPACE and transformation user. This will avoid any user of accessing storage directly in Keboola Connection, but allows to explore data outside of Keboola Connection (aka “direct access”).
How it can be implemented 
Workspace creation already support parameter to choose whatever you want to use readOnlyStorageStorage https://github.com/keboola/connection/blob/38899b43c5dbf829a2d46b9a57ee8cbdac2e1d4f/legacy-app/application/src/Storage/Request/Workspace/CreateWorkspaceRequest.php#L40
Both (transformation, sandbox) utilize this request to create a database workspace. If false will be sent everything is done. Sandbox creation is UI stuff only and can be changed on UI level. Transformations are executed in docker runner and change on docker runner is required.
How it will be used
If KEBOOLA_$PROJECTID_RO exists it can be granted to particular users in Snowflake. User access/role assignment will be done in Azure Active Directory as far as all direct users will be also logged via AAD to Snowflake. This will move all maintenance around uses to CSAS side.

Urgency
Resolving how to access Snowflake in direct access mode is required for having successful Snowflake adoption in CSAS. Thanks for help.",
"A feedback submitted on behalf of Gopay (for more details jiri.zezula@gopay.cz is available):
They work a lot with folders, so would appreciate some folder management, like “collapse all”, “Expand all”
And also sort them according to more criteria, such as “most used” , so the most important are on the top  ",
"Hi, I would like to submit (part of) my assignment for Common Components and Processors and ask for an advice. 
I have managed to extract fundings and customers tables, but I am still not able to extract sales tables. It seems the problem is caused by using tabs/spaces within some records. I have tried to use escape character processor, but it did not work. Could you please give me hands on on that? 
The AWS S3 extractor: https://connection.north-europe.azure.keboola.com/admin/projects/15011/components/keboola.ex-aws-s3/44469199
All the best 
Tereza",
"Hello, after the generic data destination job is launched, I am receiving Internal Server Error - ExceptionId exception-cd07895f930141c01450be469da6a7ab.
Job Error
Job 44743865
ExceptionId exception-cd07895f930141c01450be469da6a7ab
https://connection.north-europe.azure.keboola.com/admin/projects/15467/queue/44743865
full_message: Traceback (most recent call last): File ""/code/src/component.py"", line 344, in <module> comp.execute_action() File ""/usr/local/lib/python3.10/site-packages/keboola/component/base.py"", line 250, in execute_action return action_method() File ""/code/src/component.py"", line 182, in run self.send_json_data(in_stream, endpoint_path, request_parameters, log=not has_iterations) File ""/code/src/component.py"", line 267, in send_json_data for json_payload in converter.convert_stream(reader): File ""/code/src/json_converter.py"", line 57, in convert_stream data = self.wrap_json_payload(data) File ""/code/src/json_converter.py"", line 66, in _wrap_json_payload return json.loads(res) File ""/usr/local/lib/python3.10/json/init_.py"", line 346, in loads return _default_decoder.decode(s) File ""/usr/local/lib/python3.10/json/decoder.py"", line 337, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/usr/local/lib/python3.10/json/decoder.py"", line 355, in raw_decode raise JSONDecodeError(""Expecting value"", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 1 column 10 (char 9)
Thanks!
Regards,
Šimon Paulík",
"Hello, we are excited about your new Display & Video 360 extractor. However, we can not use Custom authorization because of some internal policies. 
Do you have ETA on when Instant authorization will be working as expected? 
Thank you,
Andrej Kováč",
Summary says it all. I think it should work as in pure Snowflake workspaces where the user gets to choose whether to preserve non-conflicting tables or clean the workspace before loading.,
"The current docker image (with ID ""developer-portal-v2/keboola.python-snowpark-transformation:1.3.4"" uses version 0.11.0 of the ""snowflake-snowpark-python"" library, but the current version is 1.5.1. There are some nasty bugs in the old version. Please upgrade the docker image ASAP. https://pypi.org/project/snowflake-snowpark-python/#history",
"I usually find it useful to connect to the Snowflake workspace using tools like DBeaver so I can inspect the database objects more easily as I work with them using Python Snowpark. These password resets make this very annoying because I have to change the password in DBeaver on every reload. Also, I don't always need the VM running Jupyter, I can run the Python code locally on my computer (and I actually prefer to do so because Jupyter is not a great IDE). It would be useful to have the option to just load/unload data into/from the Snowflake workspace without even restoring the Jupyter VM (basically to use this as a vanilla Snowflake workspace, but be able to copy mappings from Python Snowpark transformations).",
"Hello, per suggestion of Carl Lundberg I'm requesting an ADP Connector. I have a prospective customer wanting an ADP Connector for their data warehouse and sadly I don't think I can get them on Keboola w/o this ADP Connector. Hopefully the next time I'm writing a proposal involving ADP, y'all will have this connector!",
"Hi,
I would like to ask for feature request, regarding comments in Snowflake tables and columns. Currently, when I create object with comments in transformation, descriptions are not transferred into Storage, and subsequently not transferred into DB objects behind Storage. When I type description in Storage manually via Keboola GUI, it is not passed to Snowflake as well.
Thanks.
Matej",
"Hi,
I would like to request new feature for dbt transformation. We would like to use on-premise GitLab as code repository for dbt transformations. We're having issues connecting to GitLab via HTTPS because GitLab uses self-signed certificate, which platform can't currently use. 
We would like to have this possibility, since dbt transformations are currently not usable for us at all.
Thanks.
Matej",
"Hi suggest to globally change the error message in case of following errors:
Job ""43796901"" ended with user error: ""Failed to process output mapping: Failed to load table ""in.c-kds-team-ex-salesforce-v2-31969089.Lead"": Some columns are missing in the csv file. Missing columns: MasterRecordId,LastName,FirstName,Salutation,MiddleName,Suffix,Title,Company,Street,City,State,PostalCode,Country,StateCode,Latitude,Longitude,GeocodeAccuracy,Phone,MobilePhone,Fax,Email,Website,P""""
e.g. https://connection.north-europe.azure.keboola.com/admin/projects/1614/queue/43796901 https://keboola.zendesk.com/agent/tickets/28444
We keep answering the same thing on Zendesk only because it is not clear to the user why it happened and what to do. 
This happens in many cases:
User changes output schema of existing transformation
Changes list of columns in db extractor, in Salesforce or all other extractors that allow you to control output columns
Component does not handle change of schema itself

Suggestion:
Change message from the generic response to something sensible where the cause and clear action is specified. E.g. The resulting schema of table XX contains fewer columns than the one already existing in the Storage (path to table/link). This may be caused by a change to the configuration. To fix this either modify the configuration to output required columns or drop or modify the resulting table (link)""

Add option to enable jobs to store table with subset of expected columns → this needs to implemented on the side of our every extractor anyway. (often the job returned schema depends on data available, when there’s no data for that column in specified period, it is not returned). If user confirmed this behaviour (on table / or configuration?) the storage job would fill in NULLs/empty values for all columns that are missing in the provided result.

Share with @Pavel Synek who is collecting similar issues as part of his Customer enablement initiative",
"Please improve the overall configuration required for the SCD transformation. It seems quite silly that the PK and the column needs to be redefined in the configuration. The configuration should use the PK native to the table and compare all the columns unless there is an option to specify it.
Might be a stretch on this one, but there is anyway to run SCD transformation against multiple input tables that will be great.",
There should be an option to select which output bucket to output the SCD transformation. Not everyone will want to output results into the same bucket and it is a pain for clients to redirect the SCD tables into a different bucket.,
Is there a component to extract events from Google Analytics 4. The current setup in keboola is mainly built for the older version of Google analytics - UA. Could you please assist me in getting the GA4 events into keboola?,
"Hey guys, I just got some feedback from Hanilec Daribazar, Togi (working at CNC) on Dev branches:
Every time she prepares something in the Dev branch, they want to review the output table created by the transformation. But it is saved into the Storage, while the transformation shows the original table. It does not make sense to her and took her a long time to figure out she would need to go first into the Storage and open the newly created table from the Dev branch…. So it would be great according to her opinion if we made it more connected within the Dev branch itself.
Another thing is related to setting up a new configuration of generic extractor into the Dev branch. It gets the table from the source, but Input Mapping of the transformation within the Dev branch can`t be changed. She needs to change the Input in Dev branches…. Is that possible?
Thanks.
Pavel",
It would be nice to have the ability to add comments to every step/task within a flow to allow for better documentation and transparency.,
This was very confusing to my monkey brain,
"Hey guys, GymBeam and CNC both shared the same wish, to be able to search for tables and transformations through all projects available. They both said such a feature would make their lives much easier.
In case you need more details or get in touch with those people, please let me know, or follow up with them directly:
https://keboola.atlassian.net/wiki/spaces/PROD/pages/2965864449/2023+SMB+Interviews
Thank you
Pavel",
"Hi, KBC,
I have a suggestion to better navigate the transformation. Currently, after switching to a new queue, I'm struggling with the fact that when a transformation has multiple phases (separate scripts) and lots of input tables, I don't know which phases (scripts) the table is entering. So I have to go through code by code and use ctrl-F to find if a given input table is contained in this particular script. 
It would be enough if after hovering the cursor over the imput there is information where the table is (in which code-phase).
Thanks a lot
BR
Filip",
"Hey guys, maybe this is an anti-pattern (please let me know if it is). I got a feedback from a new user (Slovenian Heureka), who gave me this:
“I did not understand, if you keep the same table in Transformation and you need just to slightly adjust it along the way, that you have to have it in both input and output mapping. This seems not clear enough to me.”
….it made me think that a Transformation without Output Mapping does have any usable result at all as it has no output….therefore such transformation is wrong. So how about making sure the customer has Output Mapping set up by disabling the “Run Job” button or letting them know, before they actually do asking “Haven`t you forgotten to set up Output Mapping.?” Just thinking out loud…
Thanks
Pavel",
"Hi, I would really love if I am able to filter in Data Catalog (Shared with you section) based on original project which from is the bucket shared. Thanks, Lucie",
"Hey guys, just forwarding a feedback i got from David Ešner and Pears Health Cyber some time ago. Both referred to struggling when uploading multiple files into the File Storage. It takes a long time and lots of clicks to get it in. 
It would have been great if there was a possibility to add a files in a batch. Same maybe applies for CSV files in case they are all in the same format.

Thanks
Pavel",
"Hey folks -I'm testing Salesforce to Snowflake and I have a request.

When Keboola creates a table it makes the table and column names case sensitive, like this:

CREATE TABLE IF NOT EXISTS ""Account"" 
(""Id"" VARCHAR NOT NULL , 
""IsDeleted"" BOOLEAN NOT NULL ,
""MasterRecordId"" VARCHAR NULL );

Putting "" "" around object names forces them to be case-sensitive  - you always have to type ""Account"" and ""IsDeleted"" to reference them.It would be much easier for subsequent queries to write the CREATE TABLE statements w/o the """", like this:

CREATE TABLE IF NOT EXISTS Account
(Id VARCHAR NOT NULL , 
IsDeleted BOOLEAN NOT NULL ,
MasterRecordId VARCHAR NULL );

I'd make this the default, and perhaps provide a toggle for someone who wants case sensitive object names.",
"Hi I was trying to cancel a waiting flow, which was a just accidentally double assigned. The Flows wait until the currently running finishes. But When I opened the Flow, went to All Runs, where I saw it, I couldn't click directly on the job to Terminate it. I had to go to Orchestration and click on it there, where I could Terminate it. Would be nice to have the click-through from Flow screen as well.",
"Hey guys, 
I remember people ask for a better possibility to search for stuff in KBC. Now i came across the same thing. How about following ideas:
Put the Transformation / Configuration into Favourites (then when you search for anything, Favourites could come up on the first places while typing the keywords + we could possibly have a list of favorite components/transformations on the top.
search also within the descriptions of the Transformations...this would also allow for better organisation of all the stuff within projects shared by multiple people.
Thanks.
Cheers
Pavel",
"Hey guys, how about following the Google or Dropbox experience in sharing. I have prepared a transformation for Kristyna and wanted to share it or the output table with her. Right, I have shared the URL with her. But how about the possibility to share a Transformation or Output Table direcly from the UI via a button (same way like in Google). Well, it might not help skilled users to share more easily. On the other hand, we might reach new business people come into Keboola and get their files from it....this might help to increase brand awareness a bit...
Hope it makes sense. If not, I would be happy to clarify more.
thanks
Pavel",
I want to get data from an API wicht token is refreshing. The data must be imported in Oracle. How can i do that?,
"i need create same filter as there https://connection.north-europe.azure.keboola.com/admin/projects/6753/components/keboola.ex-google-analytics-v4/13724424/rows/24597641
and there is ga:adwordsCustomerID!=(not set)
but in thi sGA4 editor it is not allower dimension filter like this",
"Submission of the assignment in Best practises. I was a bit confused by the assignment and didn't understand it, as the component was already created and just needed a bit of editing. Thus, I only created the required flow.
https://connection.north-europe.azure.keboola.com/admin/projects/15855/flows/45891716",
"Hi, as a first step I loaded data from MySQL and set tzhe primary key while loading. Then I geocoded data with Google/Open Streetmaps. A simple transformation in Snowflake did the trick of splitting the data into successfully geocoded.. and the others. The biggest problem was identifying what is in the empty string  Since Flows are currently not working, I assembled everything in an orchestration - here is the link: https://connection.north-europe.azure.keboola.com/admin/projects/15918/orchestrations-v2/46040736
Thank you!",
"Hi, my project was deactivated and I'd like to try out keboola again. 
This is the second time it has happened to me and it is being really difficult to accomplish my goal.
My goal: I'm part of a growth agency where we build things for customers. I've seen working keboola, the something doesn't convince me and build the solutions with other tools, then I have new challenges, I wonder if keboola will help, I want to give it a try, and keboola deactivated my project. Then there are 4-5 days in the middle until I say ""I will put an extra effort to try keboola one more time, I will email support even if this is just awkward and anti-agile"". Well here I am. May I have a new project, where I have to set up things again and pass through the onboarding and default configuration again?
Thanks!",
"This script has been running for a while, now suddenly it gets an error claiming: Unsupported feature 'lateral table function called with OUTER JOIN syntax or a join predicate (ON clause)'.
But I only have an inner join and it has been working until now. You know why?",
"When trying to run component on SFTP server, there is an issue with verification through attached ssh key.
What is the proper format to store private key? Could you please provide sample so we can check everything is correctly set?
We are able to login to SFTP with this ssh key pair without any issue but Keboola.
Thank you for your assistance.",
I would like to back up data from GA3 to BQ. Currently I export them to google sheets and would like to save them to BQ.,
"Hi, 
I would like to submit my BDM model assignment: https://drive.google.com/file/d/18nmeU8XPaHoBH1hiHW_1Dvtu7i-VI1ao/view?usp=sharing
Best regards
Tereza",
"Hi!
It seems that, despite not having used much of my free credits, I have supposedly run out of steam.
Could you be so kind and check whether this is some kind of error?
I have only really used credits on trying to get the generic extractor to run (which is still not working, so it'd be a bit frustrating to be out of credits now)",
"Hello,
I got this error, can you help me please:
Job ""591862123"" ended with user error: ""Cannot import data from Storage API: Application error. Please contact our support support@keboola.com with exception id (kbc-eu-central-1-connection-fb633bd9a00d6b648c3a98fc8ee8f4d3) attached""
Best regards,
Jan",
"Hi, is it possible to end up with ordered table in google sheet? Or how does it work, because we save table ordered but writer it messes that. 
Thanks. 
Michal ",
"I have copied this component from one thats already working. Disabled some parts of configuration, but they runned anyways and deleted some data that i am missing now. I tried to do some restores but cant find them anyways .Can you help me fix this? Thank you Dom",
"Hello, I would like to ask where I can find how much PPUs the project consumed during the last year. Thank you in advance.",
"Hello team,
It would be great to have a flow timeout. Similar to transformation query timeout.
Thank you
Petr",
"Hello,
I would like to have a question about the KBC Metadata extractor. Is there any sort of ""guarantee"" that metadata provided by the extractor is complete? I am namely asking because I would like to monitor the number of all tables of a specific type across many projects and using the metadata extractor is the way my colleagues and I found to be the most plausible. However none of us were sure if all the tables in our projects are being returned by the extractors.
Thank you kindly in advanced,
Matěj",
"Hello, 
Receiving strange errors on this component. I copied the configuration from another that was at least running successfully but now I just received two strange errors. Feel free to join the project and look at the two error messages for this configuration. 
Also, there seems to be a strange behaviour where it doesn't fetch all the tickets based on this job/configuration: https://connection.north-europe.azure.keboola.com/admin/projects/15553/queue/43497837 
Is the component working as it should? I noticed it was developed by Revolt and not our team so curious if this is a legacy component that needs to be re-worked? ",
"Dear Support, we see in our project DA-development that we spend 2000 minutes and the account is locked. When i double check the Jobs, there is not such a Job with such big minutes spend. Could you double check that.",
"When I run this flow, I'm not seeing the data updated in the Snowflake table.",
Need help understanding how to populate #data with the correct string/variable needed to support OAuth for Xact calls,
"Good day, I am trying to run the generic extractor configuration locally.
https://github.com/keboola/generic-extractor
Is the doc current?
–>
https://developers.keboola.com/extend/generic-extractor/running/#running-locally
Building and Running the Image........
Because docker-compose.yml does not contain the extractor service
Thanks",
"Hello,
I tryed to reopen this ticket
https://keboola.atlassian.net/servicedesk/customer/portal/9/SUPPORT-289?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJxc2giOiI5NmQxMTgwNzY4NzFiMTdiYWRkZjRkNmVmYmFmZWEyMDk0NmQ1NDUyODI0YTZlNGNkN2E5YjgwMmQ5Mzg2MjUyIiwiaXNzIjoic2VydmljZWRlc2stand0LXRva2VuLWlzc3VlciIsImNvbnRleHQiOnsidXNlciI6IjEwNTAwIiwiaXNzdWUiOiJTVVBQT1JULTI4OSJ9LCJleHAiOjE2OTA4NzgyNDQsImlhdCI6MTY4ODQ1OTA0NH0._xpX5NY5kOI7LxS1rI8s8JRHLKiqnlTO934GIl3W3a4
by mail, but that's not working. Please can you reopen it?
Thank you
Navrátil",
"Hi, I'd like to report a small bug when table columns for load into Snowflake gets reordered.
Steps: IN DEV branch create a transformation that created a new table in KBC. Use that table in new DEV Snowflake component to load into Snowflake - it takes the DEV version of table, which has correct schema, and for some reason completely reorders the columns both in the component itself and in the created table in Snowflake. After merging the branch to PROD, it repaired itself.
Kind regards,
Jan",
"Hi, it is possible to get some developer account to test new component or flow? Before I create it for clients?",
"okay thanks, it means when I have developer branch it doesnt consume minuteS?
– 
S pozdravem
Jiří Franěk
---------- Původní e-mail ----------
Od: Kritiga Ravishankar <jira@keboola.atlassian.net>
Komu: ads.growthhacker@gmail.com
Datum: 18. 7. 2023 11:40:18
Předmět: https://keboola.atlassian.net/browse/SUPPORT-239#icft=SUPPORT-239 Developer account
—-—-—-—  
Reply above this line.  
Kritiga Ravishankar commented: 
Hello, 
Thanks for reaching out to us. We do not have separate accounts by user type. however, you can always test a new component / flow using the development branch feature without affecting the current production set up. 
In case this recommendation does not address your request, please provide more details so that we can help. 
Best, 
Kritiga   
View request • Turn off this request's notifications 
This is shared with ads.growthhacker@gmail.com. 
Powered by Jira Service Management 
Keboola.com • Blog • Documentation • Help & Support 
Dělnická 27, 170 00 Prague 7, Czech Republic   
Sent on July 18, 2023 11:40:09 AM CEST  ",
"Hello, I would like to ask you if something happened at 16. june, because my flows took 3 hours instead of a few minutes. This caused that I was in minus minutes and I payed for increase of minutes. Second question I have is, that why my flows took 1 minute to get 1 month old data when I manually run the flow and it took couple of minutes, when flows run at scheduled time? I am sending screenshot in the attachment about what happened in June.",
"I'm not sure how to add the required Salesforce parameters (soql_query and object) to my Keboola salesforce data source component.  What am I supposed to set these parameters equal to? Any clarification would be appreciated, thanks!",
"Hi,
please, can you help me, how to describe tiktok dimensions and metirics, so I would have  advertiser_id and campaign_id in one report? or if I split it into two reports (one with campign_id and another with advertiser_id) what is the connecting dimension between these tables?",
We are looking to expand our usage of keboola outside of the free tier. Just wondering if we could see your SOC 2 report?,
"Hello, using Buffer API I am trying to import data to a table through a receiver but I keep getting the following error:
{
""statusCode"": 507,
""error"": ""buffer.insufficientStorage"",
""message"": ""No free space in the buffer: receiver \""stream-receiver\"" has \""50.2 MB\"" buffered for upload, limit is \""50.0 MB\""."",
""exceptionId"": ""keboola-buffer-YORpZnhHdqfOeBO""
}
The receiver used has the following conditions:
{
""count"": 10000,
""size"": ""5MB"",
""time"": ""30s""
}
How can I proceed to fix this?
Thank you for your reply.",
"Hi, 50% of the hustle with working in KBC is having to load data I need to look at/investigate/troubleshoot/fix somewhere to a sandbox/workspace.
Majority of my requests from the company is that the data we provide don't match or are wrong and to investigate how wrong are they.
I often overuse the storage Data Sample for this or use our ""DWHM_MALLGROUP"" Snowflake database where we push many of our primary/analytical tables.
But 100% of these requests lead to one of our transformations. If only there was a database instance of the transformation I could access right away and look at the data that get processed in the transformation every day...
Ultimate state is to have a tab in the transformation with some bare minimum db console (embedded Snowsight?) but that might be too much. For start it would be super neat to have working credentials to the workspace with the data that the transformation is creating and dropping every day.. I don't see why we can't keep that instead of having to load different transformations to another workspaces constantly...
Thanks for any consideration!
Regards,
David",
"Hi guys, is it possible to rename the schemas in the read only workspace?  Currently they are called as id of the bucket, having the possibility to have it names as are the buckets would be great for improved UX.
Thanks.",
"Afternoon,
Looking for assistance with ""Gmail Reviews"" task. Transformation is failing and am unable to reconcile.
Time '| Recommend' is not recognized
Error: ""odbc_prepare(): SQL error: Time '| Recommend' is not recognized, SQL state 22008 in SQLPrepare"".
Reason of Failure: The query contains an invalid syntax, specifically Time '| Recommend' which is not recognized by the database.
Fix: The syntax should be corrected to a valid syntax that the database can recognize.",
"Hi there, 
a few days ago I sent you a question about the completeness of metadata provided by your extractor. You replied that the old extractor will be discontinued and sent me a link to the docs for the new one. I went through the metadata provided by it and found that it stores some information which is incorrect. Namely it provides a keboola url to a non-existent project.
I'll gladly elaborate further if you'd need more specific information.
Sincerely,
Matěj",
"Hi, could you please add the following?
When I create a workspace from an existing transformation and want to use variables from the transformation. Could you please add an option to copy variables to the workspace as well so I do not have to create them in the sandbox manually?
Best regards,
Matěj",
"People still don’t understand how it works. That means either they are stupid (very unlikely) or the feature is not well thought out (more likely). 

Over and over people crash and burn on the fact that if you update on of the tables in trigger “out of band” then your trigger will fire “too early”. ",
"This is a feature request. Processors are super powerful feature almost no one uses because it's poor UX.
Technically any component can be used as processor - chained. The only piece missing is better UI.
The UI can be similar to normal component -> processors could have JSON schema defined. Everry component would have an UI option to add a processor. Button to select a processor from list it would show popup with UI or json config. By default they would be collapse. It can be just list of processors before and list of processors after. Click would lead to config detail popup. At the same time it would be possible to switch the whole processor section to JSON for backwards compatibility and to be able to share whole processor setup via text.
There could be also processor templates - some processors are very often used in combination with other:  move_files - create_manifest - skip_line, xls2csv - move_files.  
This is a precursor to enable linking DS transformations as a processor (chain with component). Very often you need to do just some simple thing - hash one column, rename something, add some value, etc. These are proprietary things specific for usecase and the need to register a component to do this does not make much sense. I can write a Python transformation and just hook it up to the process I need (I can already do this by just injecting whole config as processor btw). This would be a killer feature. This would even help dataquality usecases as we may have dedicated tool but there still can be very specific things one would need to write on his own. It can be done via transformation, that this transformation can be just linked whenever it's needed.",
"I would like to ask, optionally submit a request for the possibility to insert JSON credentials to BQ as it is in the UI. 
Moreover, Can we also use this in inputs for our templates? 
Thanks.",
"I have few issues:
it is not able to cast types, or how can I proceed? This means I cannot use > or < in the filter logic. (screenshot - how to filter columns where installments are greater than 3?) I am afraid this is the case for dates as well.
I am missing AVG and MEAN in the functions. Especially the average.
I have tried casting user datatypes but no luck. Why don’t we use the same input mapping UI that allows doing so directly there?
I have a reservations around its speed. Also when I click to an existing transformation it errors when I want to add a new filter.
Thanks!",
Where I can find Storage overview in Dashboard when using CLI is showing me Latest CLI activity? ,
"Configuration has been changed, I can potentially ask “what has been changed?”
E.g. before merging a branch or when comparing two versions.
See attached image.",
Development branches currently lack the option to either rebase on latest master version or to merge latest changes from master.,
"When I click to workspaces (specifically in this project https://connection.north-europe.azure.keboola.com/admin/projects/241/transformations-v2/workspaces) it takes couple of seconds to load and the loading icon is just next to the ""Overview"" (see scrshot) and user has no idea that something is loading.
It'd be better to switch to the workspaces tab and show the loading circle in the middle of the page?",
"Link to MLFlow server is only displayed in MLFlow workspace which is now redundant as MLFlow is accessible from standard Python WS and Transformation.
Would be great to have that link in ML/AI services tab.",
"Users don’t have any good way how to monitor how many resources their Python transformation consumes - no report of how it utilized cores or how much memory it needed. Thus they have no good way how to decide on whether they need larger backend (whether it’ll actually help anything) other than failing on memory. 
In general it’d be great to provide such information in job log or something but together with the dynamic backends option it feels even more important to have it. ",
"Client uses MySQL extractor to connect to ClickHouse. Everything is fine, but he can't see the list of tables. ClickHouse uses ""table_schema"" column name, but MySQL uses ""TABLE_SCHEMA"", so it doesn't work.
We need to modify component to support ClickHouse without problems.

https://keboola.zendesk.com/agent/tickets/25516
https://keboola.slack.com/archives/CQ1ASK06M/p1669817284526989?thread_ts=1669043363.187159&cid=CQ1ASK06M",
Allow user to choose size of SFLK workspace (I guess it’s already somewhere in a backlog but just to be sure),
"This seems to be a known issue. Please see this Slack thread: 
https://keboolaglobal.slack.com/archives/C02PGBVTF54/p1644487278906609We’ve run into the issue again during a PoC with a potentially huge opportunity. Can we please add this as high as possible to the backlog? 
Thanks,",
"Request from customer today - 
Enable functions for endpoints in Generic Extractor so that something like this is possible: 
""endpointUrl"": {
    ""function"": ""concat"",
    ""args"": [
     ""activities_"",
     ""20220722"",
     "".json""
    ]
   },
   ""jobs"": [
    {
     ""endpoint"": {
      ""attr"": ""endpointUrl""
     }
    }
   ]",
"When new sql transformation (project with new queue) fails, I want to be able to get easily to the problematic query in the transformation, as it was working in old projects (Show details → Open query). Or at least to related code block.",
"Hey Product Team,
I have yet more unhappy customers PayCaptain and CloudTalk that are disappointed they can’t see their real consumption in the Projects and having a lot of confusion and negativity floating around Telemetry Extractor. They don’t like the fact that the Telemetry Extractor is not always set up correctly, needs maintenance, is not trusted. 
And it’s just an extra something that they must do to see how much PPUs they’re using, instead of just just click and see them updated on a single page. 

Then I, as a Customer Success Manager, am wasting hours of my days trying to calm them down by sharing best practices, trying to hush the fire with countless emails and adding them to our GoodData Telemetry Dashboard as a temporary solution to see the real consumption. And then they ask me “Well, it’s your fault anyway so what are we going to do about that?” (see email from Jan Cienciala below)

I know we have many more customers like that. 
Would love if we could prioritize this.
Best,
Maria",
I am really missing Graph functionality on Storage when using new projects. Ideally you should be able to select depth - first level of impacted transformations vs. all dependencies.,
"Hello, it would be great to synchronize behaviour of COPY and PDO adapters:
Allow initQueries in COPY adapter to for example increase timeouts per session
Add useConsistentFallbackBooleanStyle param into UI parameters and set it by default to TRUE! When copy adapter unexpectedly fails (timeout, connection/db issue) the PDO adapter should export same values. (Not empty string for false values which is quite big problem in the input mapping // convert empty values to null)
This feature request is based on the ticket #21975",
"When on the project diff screen of a development branch, it would be very useful to have links that would take me to the changed configurations.",
"Dark Sky API is currently not working, this is the customer feedback:
“It would be nice to regularly check if the components in the ""Keboola app store"" are still relevant.
It has been the second not-working component in the last two weeks. The first one was revolt-bi.ex-smartsupp
Can you please redirect this to the product team?
Thanks”",
"Just an idea - if we were to set a variable in SQL transformation in branch, they could do something like SET limitVar = CASE(@branchId IS NOT NULL, 1000, NULL); SELECT * FROM source LIMIT @limitVar; . That would have the same effect with very little effort as the “global env in connection”, don't you think?
(the job already has branch ID in environment variable, so it'd be just a matter of adding the variable to the SNFLK transformation component)
Context: 
https://keboolaglobal.slack.com/archives/CBX9MB2NT/p1654688296855749?thread_ts=1654679628.852609&cid=CBX9MB2NT",
"the component included part is too big I think, can we make it smaller? Btw, why is the short desc hidden? Is it less important than included components?

I suggest:
make the included components smaller to 1/3 of actual size. 
show only components names and description show as tooltip
make more info block longer (I can delete the header from those desc, for example SHOPIFY-TO-DATASTUDIO to make it look better) 
What do you say?",
"Hi, I would like to ask you for a help. I'm trying to set up all extractors in a template to get data from last 6 months. but I have a problem with linkedIn. This extractor needs 6 parameters. Start date, start month, start year and end date, end month, end year.  SO basically parsed current date and parsed current date minus 6 months. Can you implement this in the CLI, please?
Thank you.",
"benefit - easy to see the diff before merging. We could even have this feature as a part of the merge process - take all tables and run their data diff.
GitHub - data-diff: Efficiently diff rows across two different databases.
since we know what is a respective table (mirrored version) between prod and dev branch, user could literally press a button…


comm I had with Jordan today:

Jordan Burger  13:19
Hey! Just had a very productive meeting with some guys from Datafold. We were talking about a basic integration between the two of us for less-technical users in the UI.TLDR: They now support the ability to do DataDiffs on duckDB tables using their open source library!!!! So, we can bypass the whole warehouse role issue & have a component that spins up a duckDB, pulls in two tables, and outputs the datadiff!!
fisa  13:23
I had a discussion with them 2 months ago. Even though the diff library is great and would be a good component/tooling, we have struggled to use BYODB +datafold quality checks and set it up in the way it does not interfere - they required future grants
Jordan Burger  13:24
But that's what I'm saying ... we don't need to do anything with ROLES or GRANTS. We can simply fork their open source library, and make a new component that runs the data diffs on duckDB!
fisa  13:24
Snowflake | Datafoldhttp://docs.datafold.comSnowflake | DatafoldSetting up Snowflake on Datafold
yeah, but there is a limited value in the data diff
I was more focusing on their data quality checks
Dont get me wrong, data diff is cool, but mostly for migration
Jordan Burger  13:25
They don't really have data quality checks - it's just custom SQL scripts that trigger notifications.
fisa  13:26
exactly, but they have observability around the tests
what would be the use cases for data diff?
I could see only our migration
New
Jordan Burger  13:27
I do a data diff every time I change a transformation. The idea is that if you make any change to your code, you should run a data diff before merging it so that ""revenue doesn't double"" accidentally.
fisa  13:29
but it just compares the rows
so literality you dunno what is the impact - like is the sum of all double (issue) or has it risen by 30% (expected_ rows stay the same, values change. You know they will change, thats why you have done changes in the transformation. but you have to do custom QA to evaluate total sum for example
in other words, row diff is good for some cases, but not a silver bullet. you still have to define custom QA
and thats where their tests come in (edited) 
and with read-only we dont eed duckDB

`python -m data_diff “snowflake://KEBOOLA_WORKSPACE_18660457:{{PWD}}@keboola.west-europe.azure/KEBOOLA_7167?schema=in.c-kds-team-ex-shopify-18160527&warehouse=KEBOOLA_PROD_SMALL” order “snowflake://KEBOOLA_WORKSPACE_18660457:{{PWD}}@keboola.west-europe.azure/KEBOOLA_7167?schema=WORKSPACE_18660457&warehouse=KEBOOLA_PROD_SMALL” orders --update-column _timestamp -v`

this is what I have tested in the past. and it works and its super fast
it would be cool to have a component that takes two tables and runs a diff, or a streamlit
do you know any other use case where it would make sense?

oh, that component would be super useful when testing prod/dev mode!
basically you just serve the table and it should know the alternate version of it!

Fisa",
"Yet there is a million ways to die in the west top up. 
Places I tried to click marked below. 
Luckily I remembered that we added billing to the “profile image” menu, so that’s how I got there in the end. 
Anyway, optimizing for conversion is fine, but in this case it feels bit too much. ",
,
"I would like to see better syntax highlighting and code completion for python, SQL, (is it possible for json configs as well?). Also JSON formatting is weird in comparison to JSON editors, folding feature and alternating/highligting corresponding {} [] in the code (as VS code does) would be great - minimize issues, user errors, increases UX",
It is hard to keep transformations organised when having a lot of them. Introducing sub-folders on the Transformation Overview would help a lot to keep the structure clean.,
"It took me a while to understand where is the issue - but apparently user did not know that he can click on the row to edit setup - when you click on it in a wrong part it shows the table…
https://keboola.zendesk.com/agent/tickets/23658",
"Allegro / Mall asked us to improve BigQuery writer. They try to integrate Mall KBC with their existing solution in GCP and they are limited by our current BigQuery writer. They require these features:

1. Authentication using Google Key Vault - every several months they refresh keys and they re-distribute them into systems
2. Data imports/exports:
Exporting data in binary format (Avro compatible with BigQuery, Parquet)  = just for improving speed because csv is slow, we dont need to store them
Loading data with truncate/insert to BigQuery
Loading data with truncate/insert for partitioned tables (daily partitions, partitions may be from selected columns) to BigQuery
Loading data with truncate/insert for partitioned tables to BigQuery (historical daily partitions(date ranges), partitions may be from selected columns)
Loading data with drop/create partitioned table to BigQuery

= In advance option UI I am able to set: 
Create table from scratch
Update existing table without loosing metadata (partitions)
Just truncate and insert
Drop and create it again as it was (we can use API for that to get info about how the table was created what partitions amd use it to recreate it again)
In both a + b i am able to select if the partition is by column(which column) or if by date

3. Enabling add labels for create table statementsAllowing run scripts:
Run SQL script on destination before loading data (first truncate the table)
Run SQL script on destination after loading dataTrigger an API endpoint (deduplication as last step copying data to production table)
Adding labels on tables",
"(probably reported already)
Add the Packages config into the Workspace definition so that users can specify it there and thus it’s installed when workspace’s started/restored - thus user wouldn’t have to always execute the pip installs after they restore a workspace and also it’d be better integrated with “copy to transformation”",
Info tab in templates does not fit into the screen.,
"Test model feature says it requires input_example.json
It'd be great to also include some link to a documentation that describes how to log that - such as this one https://www.mlflow.org/docs/latest/models.html#how-to-log-model-with-column-based-example.
Or by using autolog mlflow.org/docs/latest/python_api/mlflow.html#mlflow.autolog
Also, when you actually include it and test the model the UI will only display the output. It'd be really great if the UI could also display the actual input. Because without that it doesn't tell the user anything - they'll just see the output but not what the model actually requires as an input (and in what format) so it just tells you how the output looks like which you cannot use anyway.",
"I am upvoting this feedback https://keboola.atlassian.net/browse/HELP-76 
This time by Košík. Thy want a clear differentiation extractors and writers. Without needing to click “components”. They are using it daily, so the most straightforward, the better. Additionally Snowflake icon is the same for Snowflake component and transformations and it is not user friendly. ",
"Scenario
I am on https://connection.keboola.com/admin/projects/258/transformations-v2/workspaces - list of workspaces
I am looking of a WS so I type a substring of the expected name
It found two WS
I open one…
it wasn’t the one I have expected
so I click on back button
And I return to the WS list page BUT without activated search, so I have to search again…
this is pretty annoying when you try to find something",
"Named ranges help to define specific part of the sheet so it helps to extract exactly the part of the sheet that is needed. This is similar to what we use in XLS processor (and now GSheets do support this feature as well)
https://support.google.com/docs/answer/63175?hl=en&co=GENIE.Platform%3DDesktop ",
" 
 
 Product Fruits 
 
You've got a new feedback via Product Fruits! 
Flow part 3 Variables: I hope this is in works but the Flows should have some convenient interface to handle overwriting of variables within tasks. Most common uscase: I run some product like PoS/Marketing analysis and want to provide my customers with customized data or dashboards. I have all data together in project but I want to set up writer to different schema for each customer. Everything is in the same table, I just need to filter to specific customer ID and change the destination schema. Ideal usecase for variables. Now there is no simple way to do that. I am thinking of creating an app that would take configuration and possible a list of variables to override so it can iterate through it. I would hope I do not have to create such workarounds and leverage native functionality of the flow. 
 
 
Screenshot
 
By david@keboola.com at 02/09/2022 14:52:09 
View the feedback
To reply to your user, you can reply to this e-mail. 
 
 
© 2020 Product Fruits. All rights reserved.Product Fruits 
 
 ",
" 
 
 Product Fruits 
 
You've got a new feedback via Product Fruits! 
Flow part2: The biggest benefit of the Flow would imo be to bring a global visual view of all flows.It is a very common usecase to group some tasks into separate unscheduled orchestrations and then trigger it from main wrapping process (or multiple). Example: I am sending slack notifications (1. Prepare data, 2. Run component) and these notifications I want to run from different places (Process A and Process dedicated to B). This sort of design is very common and leads to several flows now still displayed as a list. To find out from which Flow the Notifications flow is triggered I need to go through each configuration and find out. This is quite annoying. During health checks customers quite often ask how to organize orchestrations better. We need to tell them to help themselves with a designed set of prefixes in names that would group the configs visually. This is not sufficient however and still won’t help them to see the global picture. E.g. to see orchestrations that are not triggered at all. I think the new flow should offer this view. Display all orchestrations as nodes and draw links to those that are being run from another orchestration. Ideally, this view would also include nodes that represent tables which are being used for trigger. This would be a great help for anyone that wants to see what’s happening in the project. Flow visualization only from a single orchestration config is not much help otherwise. Perhaps it would be possible to switch back to list view. Last runtime could be on hover or displayed within the rectangle. Click through would lead to the current Flow view. I have screenshots but cannot include it here https://docs.google.com/document/d/1WdRwnZH0C4UDsn6B0JjPZjNAIWVMkmseC9TEaEdRzn4/edit 
 
 
Screenshot
 
By david@keboola.com at 02/09/2022 14:50:16 
View the feedback
To reply to your user, you can reply to this e-mail. 
 
 
© 2020 Product Fruits. All rights reserved.Product Fruits 
 
 ",
"Currently we even do not display the description at all, not even signal that the flow has been described.
I understand the need for real estate, so I suggest:
use UI element to signal a description is there
add a new tab to accommodate description, such as “readme”, “description” or “documentation”

i.e. popup is not enough and it does not promote a best practice to document everything.",
" 
 
 Product Fruits 
 
You've got a new feedback via Product Fruits! 
Hello, I would like to comment the Storage especially the ""Data sample"" view because I'm using it a lot when I need to do some quick analysis. Here are some things which I like and find useful: - I can use more columns to filter the data - Even for large tables the speed is quite good - The full screen mode is very useful, since usually tables I look at have a lot of columns - The comparison operator based filter (<, >, =, 
 Here are some things I would really appreciate and would be glad if the ""Data sample"" would be capable of: - filter data based on substring search (e.g. to find value 'foobarfoo' based on something like '%bar%') - also maybe some pop-up window with all the filters applied would be in some cases good (sometime I forget about some filter on a column far on the right and wonder about the search result) - maybe some ""clear all filters"" button - and maybe to have the posibility to export only the filtered data - also an row-count information would helpful in a lot of use cases Best Regards, Zdeněk H. 
By zdenek.hanik@kosik.cz at 02/11/2022 08:44:13 
View the feedback
To reply to your user, you can reply to this e-mail.| 
 
 
© 2020 Product Fruits. All rights reserved.Product Fruits 
 
 ",
definition of textbox in inputs.jsonnet for inputs array. It will allow us to guide a users. ,
"I wanted to point out that the definition of required within step group does not work nicely.
I'm referring to this:

define how many steps need to be selected by the user
one of: optional, exactlyOne, atLeastOne, zeroOrOne, allThe options - optional, all and atLeastOne work ok, but I do have a problem with usability of the zeroOrOne and exactlyOne. Even if the condition is for EXACTLY ONE, I can configure more steps within the step group. I would imagine that if there is such condition, it would disable the other steps from configuration until I reset configuration of the only configured step. Moreover, there is no description of the requirements in the builder. In the end, those 2 options cannot be used in templates. Can it be changed, please? 
If you want to try how it behaves, I prepared a test for you to try that it's really hard to use a template with those step group definition.
Template: Shopify Analytics for Thoughtspot version 0.0.1Maybe even better, can we show users those rules for step groups? The description is quite simple.
Optional: ""Configure any steps you want to include in the flow.""
ExactlyOne: ""Configure exactly one step.""
AtLeastOne: ""Configure at least one step.""
ZeroOrOne: """,
" 
 
 Product Fruits 
 
You've got a new feedback via Product Fruits! 
Hi a years old feature request I am not sure I posted already, so reposting just in case. Modify the orchestration trigger to react on actual table changes -e.g. trigger only if some rows are updated or at least if there were some data imported. Currently it triggers on all import events which means that if transformation or component outputs empty table it trigger. We are bypassing this by Python transformations every time. I haven't yet seen a usecase where a trigger on import event that did not bring any data would be needed. So we're doing this pretty much always. 
By david@keboola.com at 02/07/2022 08:50:15 
View the feedback
To reply to your user, you can reply to this e-mail.| 
 
 
© 2020 Product Fruits. All rights reserved.Product Fruits 
 
 ",
"It would be great, if next to Adding columns to storage tables we’d have the possibility to drop columns from storage tables. Now it is necessary to delete and re-create the table.",
After set up I do not see which backed size is currently used. (All rackets are the same. Only way how to be sure what backed size is clicked is by advanced configuration.),
"First I would really like to see a gantt chart instead of those steps, that gantt can be split by phases as per my previous request (Strangely I cannot find it in Jira now),

but When user gets email “longer than usual execution time” - he/she is wondering what caused it. We should display which part took longer on the orchestration screen… (the same way include +1 min next to it. Or in gantt chart we could display it as an orange part of the column…",
"Do you know how to override transformation variable in flow? See attached. You need to add something (what?) to some JSON. Impossible to do for normal user  
https://keboola.zendesk.com/agent/tickets/27612",
"The drop task button is wirdly positioned, etc.",
"Hi, based on my communication with support I am creating this feature request. I would need to regularly read csv files, which are being stored on Onedrive. Current extractor can only read xlsx files. My expecatitions are that it works like FTP extractor, so I can specify the folder name and I don’t need to specify the full name of the file but only some “part” like 'name_*.csv'.",
Add ability to rename column,
It would be great if text boxes could be added into the flow canvas. This could enhance readability. ,
"HI KBC,
Will it ever be possible to run individual parts of a transformation in one phase in Transformations sometime in the near future? In Legacy it worked because the individual subtransformations were separate, I miss this feature quite a bit in the new transformations. Sometimes it is necessary to release only one part and quickly get the table into GD, this makes the whole process extremely long.
BR",
"hi dev/eng  

on the project diff, it would be great if the prod project changes, that you can update the dev branch with the latest changes.  
see ticket #27718 that I submitted. 
I believe this is normal in a git environment, when your dev code is out of sync with prod and you want to get the latest changes in your dev environment so that it is aware of the latest changes in the project. ",
It will be great if you return task parameter function to QueuV2 orchestrations. We use this quite often for one complex usecase.,
see attachment,
"based on this ticket discussion 
https://support.keboola.com/hc/requests/22032
As I mention, Keboola stores everything as varchar in the storage and keeps metadata about the datatypes within metadata layer, not directly as a datatypes in the storage. This allows to use data throughout the whole pipeline without hurdles, including extractors handing our plain CSVs, python/R transformations alongside of sql transformations.
Usually users use casting in the first transformation queries to make sure they work with the format they want (and sometimes they leverage this so in one transformation they use the column as varchar, in other as a datetime)
I have also seen using CTEs to define the table details.
Snowflake is generally very good in autocasting types as well.
However, I would strongly encourage you to submit a feature suggestion through the help button. I could imagine users might be able to turn on strong casting on the table level?
Fisa",
"A few ideas to Keboola telemetry, mainly consumption:
It’s really surprising there’s no indication of credit consumption for current month (compared to monthly limit according to contract) right in Keboola UI. I’ve seen it on the landing page of Keboola project in some previous version 2 yrs ago, but for some reason it’s not there any more. Nothing fancy, just SUM of total consumption would do. I imagine this could prevent some unpleasant surprises.
Even better would be if Keboola had prepared nice, polished dashboards built on top of telemetry for the most used viz tools - maybe Tableau & Power BI? - that customers can then download and use.
When looking at consumption broken up by parent orchestration / user who started it, often the token_name is something like “[_internal] Scheduler for 392767948”. That’s not very useful, or maybe I’m missing something. I would expect either just ID of the orchestration or the orchestration name. It looks the same in your GD dashboard .",
"https://keboola.slack.com/archives/CQB468K63/p1681302322298489?thread_ts=1681301656.033739&cid=CQB468K63

marc  17 minutes ago

but I think that deserves a UI feature request . (not the first time it's been brought up)",
"As agreed on Slack, I am submitting a product feedback initiated by Rohlík  
The private workspace still exists even after the user is terminated and it is possible to log into it (we use read only storage, so load data is not necessary, so there is access to all data).
Shared workspace always has the same login, anyone can save their login and then log in even if, for example, they no longer work at the company.",
":joy: Rohlík was joking about an idea to organise a petition for ""old"" simple Keboola. They were sure a lot of Keboola users would sign it. 
It was mainly about getting back a simple structure of ""Extractors"", ""Transformation"", ""Writers"" in the top row (instead of ""components). As it now makes Keboola less straightforward to explain and needs more ""clicks"" to get where it is needed. ",
"Currently if you run the orchestration manually only you will get the notification. This is unexpected behavior when configuration for notifications has been set. When a pipeline takes some time to run I would expect to be able to manually trigger it, go do something else and only keep an eye on the channels that were configured initially.",
"Mini thing : imho “SLEEP“ should be either “ASLEEP“ or “SLEPT“.  

“SLEEP“ implies an action like something to be clicked so it goes to sleep, ",
"would it be possible for Snowflake workspaces to change host from:
""https://keboolashipmonk.us-east-1.snowflakecomputing.com""
to
""https://app.snowflake.com/us-east-1/keboolashipmonk "" ?
Or at least the connect button :)
Not a priority.
Thanks,
Matus

https://keboola.zendesk.com/agent/tickets/22742",
It makes it hard to locate erroneous SQL as code blocks are impenetrable for browser search. Either having error messages pointing to problematic code block or creating an ability to search for piece of code without a need to open each code block in turn.,
"Hello, I’d like to submit a request for a small feature regarding orchestrations - reset of event triggers.
When I have 2 tables set as an event trigger for an orchestration, this can be quite easily broken - see a model situation in a table below:
Date
Table 1 updated
Table 2 updated
Event trigger run
3.3.
1:00 AM
2:00 AM
2:00 AM
4.3.
-
2:00 AM
-
5.3.
1:00 AM
2:00 AM
1:00 AM
On 3.3. the event trigger worked fine, On 4.3. one table was not updated because of an error in other configuration. On 5.3. the event trigger is broken because it is not waiting for the table updated at 2:00 AM, because Table 1 was not updated at all on 4.3.
We have one clumsy solution for this using a component to delete the trigger and setup it again as it was https://connection.eu-central-1.keboola.com/admin/projects/300/components/ech.ex-trigger-component
We can do that manually but there is a risk of making a mistake and setting different tables to the event trigger as we create it again after we delete it because it is broken and there is no better solution for this.
Best UI I can imagine is to just have a button at the event trigger settings to reset what the event trigger is waiting for now. Maybe even have a visualization which table is the trigger currently waiting for and which is triggered - I would easily see during business hours if my event triggers are working fine because my tables are refreshed once every night so during the day the event trigger should be waiting for both the tables to update.
I can find this out comparing what time the tables were updated and what time did the orchestrations start - I can also select this from the telemetry data. But it’s a hassle especially in a large organization of projects.
Thank you for considering this!
Regards,
David",
"The /config_Id/raw view is amazing and I am using it every day. Sometimes I still use my extension though because it is annoying having to write this into the URL. Also we need to tell this to our customers via support, every time they are surprised this functionality is present. Moreover, some of the support guys still do not know about this so the customers are advised to use API to change something/update state, run debug.",
"Hash credentials for other applications as variable (for using in python, etc.)",
"For certain use-cases we’d need to process output mapping even if the component fails. 
It’d be needed to have the option to select only specific tables that’d be processed even on failed jobs.

See discussion here.

Immediate use-case is our Data Quality testing solution. 
We have pretty decent implementation of testing in shared codes but we need to be able to kill (abort) a transformation if some mandatory test fails. And then we still need to process output mapping that loads the results into a result table.  We are working on a solution that needs to be delivered to CNC by the end of year (and we already know CSMs have list of clients that’d want the solution, too). 
David Esner can add other details about use-cases he has for other components (not only sql transformations).",
When trying to compare run executions for an orchestration (or component) I would like to be able to quickly switch between prev/next runs rather than having to go back to the listing every time (then I forget where I was),
It would be great to be able to go to /raw mode in a specific configuration with a hotkey like g + r .,
"Hi - I wanted to amend permissions for an existing user from Guest to Admin and the only way to do this was to remove the user from the project and invite again, which is not ideal. I'd expect to have the ability to change the role as a drop-down list or similar without having to remove users.",
"Hello,
could we please get a more detailed error message in case of OneDrive Excel Sheets extractor job error. Here some value was copied across all columns and I guess that for Keboola it exceeded some limit for the amount of columns and therefore an error was raised. But these massage “Internal Error“ doesn’t provide any detail that could help solving this issue.
Thanks,
Zdeněk H.",
"Currently, dialog window is visible only when an input is present. 
I would like to be able to see dialog window also without the need of having an input. 

Thanks.",
When loading in a table to snowflake workspace and you have messy data that you need with trial and error get into a workspace I have to each time select the same workspace over and over again. I would like it to remember which workplace to load the data to. I also would like the error message to include which column it failed to import. Now it just says a that a boolean column could load data '' and you have to guess which it is.,
"Hi, I would appreciate SSH tunnel for FTP/SFTP data source.
Michal",
Debugging SQL transformations is quite slow because I have to everytime manually replace all variables in code. It will be great to have some option to see/copy code with already replaced values.,
"I’ve already settled for KBC to be using green on so many places, even though it’s clashing with success indication. But these red arrows seem like there is some issue with the configuration (failed job, wrong set up…). I really don’t like it. Red says: “There is something wrong.”
Letting user know that this is turned on is good though. I believe that some more subtle icon in the top right of the box would be better. Then there could be more indicators specific for the box (like specific config row setup, once it’s doable).",
"template use from provate repository on github or Bitbucket which will be setup in project manifest for the use. 
the use will be only from the CLI.
usecase: internal templates which we or users don’t want to be visible to everyone
keboola interbal activity center - premium feature
some clients can create their own templates in the future.",
When setting up an extractor that has several options for example google analytics it would be good to give the user an option to select all and not set up each metric individually.  This applies to extracting from a database.  Extract all tables rather than clicking and choosing each one individually,
"Hi. I am adding couple of users to my project and when I add a one user, flash message pops up, which is ok, but it overlaps the CTA button, so I have to wait till the flash message disappears or I have to kill it manually to add another user. It is a bit annoying
https://prnt.sc/XtNQVgoYBgtV ",
as per Ticket: How can I just auto-renew my minutes… – Keboola.com – Zendesk,
"We are a few steps of having shopify integration & template in a perfect shape allowing users to integrate shopify “with one click”. To get there we’ll need to list our app in shopify marketplace which meand we need to develop npm app and integrate it with the template.

Build app which will help users to download data to keboola easily
ideal timing for January + February and run marketing campaign inside Shopify marketplace",
"Hi team,
as per slack conversation in our shared slack room Im creating a feedback for Keboola integration with Bitbucket. As per now it is only possible to intergrate with GitGub without writing own deployment pipeline.
Thanks a lot
Adrian",
"We display 2hrs/2months ago etc - pls display the particular datetime, too - it's helpful when debugging something.",
"Hi KBC,
Is it possible to set the sorting in the recycle bin (in restore mode) according to the deletion date? It is currently sorted by component.
thank you very much
Best regards
Filip",
"UI says ""deployed"" but deployment job is still running. If you try to use the model that says ""deployed"" then you'll get an error.",
"Allow code editors to autosuggest possible fields/field names for parameters (was it ""incremental"" or ""incremental_output"" or ""incrementalOutput""?
in the case where they are interacting with the platform through code definition?
Generic Extractor
CLI/Keboola as Code definitions
etc.
so the editors can propose valid params etc.",
I can create workspace by selecting a table on Bucket level of the Storage. Couple of times I already looked for (means needed) an option to create workspace from the level of a single table. Might be needed by many? Not sure how many users actually create WS from the Storage though. ,
"Keboola presents itself as a powerful tool for data governance. Unfortunately, it lacks several key functionalities to make this claim. One of them is a better data catalog. The current data catalog allows you to insert a data object into a project if it shares it with me. But customers are calling for more advanced features like data discovery. They want to be able to explore all available data objects (even if they don't have permission to use them) and they want to be able to ask for their eventual sharing with the built-in tools in Keboola.",
"Sometimes the client has request for A/D user authention for MSSQL access.  If this is a simple fix, would love that feature.",
This will inevitably lead to exponential growth of the table but we can warn user rigth after he makes this mistake instead of letting him find out after his job fails on out of memory or timeout or whatever this invalid configuration causes.,
"I think it would be great if it would be possible to filter data samples in the Storage application by NULL. I would like to check rows with null value in a column. However, I cannot do it directly in the Storage application, and I need to load the storage into a workspace to do that.",
"Based on our experience with data platforms we would like to extend Storage API about adding foreign key. It is very valuable from architecture and reference integrity PoV. 
https://keboola.docs.apiary.io/#reference/tables/create-table-definition/create-new-table-definition 
Pls provide us timeline, what time we should awaiting the upgrade of storage API.
Kind Regards,
Tomas Kratochvil",
We only display “hour ago” in job log. Particular time would be handy as well. ,
snowflake://KEBOOLA_WORKSPACE_7852031:XXXXXXXXXX@keboola.west-europe.azure/KEBOOLA_4197?role=KEBOOLA_WORKSPACE_7852031&warehouse=KEBOOLA_PROD,
"We have a brand new customer who is interested in taking the data from extractors and automatically making all of the columns/table names uppercase in storage, so that quotes are no longer needed in subsequent transformations. Fisa mentioned that this may be possible to do with some modifications of the “Create Manifest” processor. 
Any idea what sort of effort this would take? Is it a quick fix, or would we be waiting for a few sprints? 

thanks!",
The link leads to general transformation but should lead to Packeges under py trans https://help.keboola.com/transformations/python-plain/#packages ,
"The current possibilities of scheduling an orchestration are pretty nice for let's say you wanna to execute something daily or hourly. For more special needs, it would be pretty convenient I could make use of usual cron syntax like */5 */4 * * * 
Thanks for taking it into consideration.",
It would be great if a folder could contain a folder so that organization of many transforms can be further grouped,
"Super useful in the tables with large number of columns, such as:
shoptet - Storage / l0-salesforce / Project__c (keboola.com)
thanks!",
"In some occasions, we would like to re-run the job without the need of manual actions. F.E. - data are not yet in the source or the extractor randomly timeouted…, try again later, if they are successfully downloaded, dont try agan",
"It happens sometimes (recently here https://keboola.zendesk.com/agent/tickets/23719) that users are confused by the consumption in PAYG. Their jobs would show it took X minutes to run it but as they turn on parallel jobs within it actually consumes X+Y minutes. 
And then they are confused as they are not warned when turning on the parallelism (in component or flow) and the job UI doesn’t diplay it properly.
It’d be good to display smthng like 10 mins (15 mins total)? 
I heard that new billing page is under constructin so maybe it’ll fix that.",
"Hi, can you please add a link to the job when it's terminated? Without such detail, I need to manually search for the job in the job history. Thanks",
"It would be great to see in the job logs who actually terminated the job. Right now it only says ""Job 839199148 terminated"". 
Suggestion:
""Job 839199148 terminated by Matus Pavliscak"".
Thanks,
Matus",
"The data world is increasingly turning towards machine learning and artificial intelligence. This world works with large volumes of structured data, but it is not stored in a database.
If Keboola is to be at the center of the data world in organizations, it should learn to work with structured data stored in files. Keboola should understand basic data formats such as Parquet.
Files should be able to use input/output mapping, share via data catalog, and share with third parties.",
"It is sometimes useful to provide rich descriptions of a project. It used to be visible on the main page and now it's not.
On the screenshot is project where are added a project description. I thought it is some UI glitch so I clicked PROJEC DESCRIPTION button and realized that the decription is saved and hidden under this button. I suggest it's visible on the dashboard in its own tile like it used to.
For me it's like not having the description there at all. It would be probably even less confusing.",
"super cheap for us to do, but might ease the pain of local setup.
Might be handy for quick & dirty edits without needing to set it up locally (and brew, terminal, git, etc.)
Also, some json editor would be great!",
"== double down on user generated metadata over configuration of the data pipelines - increase stickiness of the platform

Story:
common user cannot finish some config - it keeps failing. He mentions his colleague from other department “somewhere in the UI” with a comment, which will bring them to the config to help out.
Users could also use the platform to have a meaningful discussion about the configs, transformations, datasets, etc. => add user generated metadata and context. 
This would increase a stickiness of our product and also allow to be a one stop shop where business and tech people meet (and each bring their value)…
Check out how data catalog/discovery tools allow users to collaborate - but this would extend that to the data pipeline logic as well.
Obviously tags would be useful here… for example “for review” “prod” “experiment” etc. for storage “gold” “silver” “bronze” etc. ",
"Hello,
getting just “Failed to connect!” when setting up MySQL connector is not very useful for debugging.
It would be great, if it showed something like this:
Trying to connect from IP XXX.XX.XXX.XX (so I can easily verify that this specific one is allowed in firewall)
Trying to establish SSH connection…. OK/fail (to verify that the SSH Tunnel is configured properly)
Trying to connect to database XX on host XY using username ZZ… OK/failed (to verify, that the DB is configured properly)",
"Hi, I was about to use environment variables in my Python script. I noticed there's ""Variables"" feature, but Keboola documentation lacks any information about how this work in Python transformations as it is described only for SQL (Snowflake). I spent almost 1 hour to get it work.
1) Why there are no working examples in documentation for Python transformations?
2) Snowflake example is misleading, variable name has ""space"" characters around which doesn't work in Python (it must be variable, not {{ variable }})
3) Why the variables aren't loaded as environment variables, so I can use them with os.getenv function? Please try to follow standards and best practices, loading variables into code using your own syntax is clumsy.",
" 
 
 Product Fruits 
 
You've got a new feedback via Product Fruits! 
When configuring the Identifier of Date dimension it doesn't inform about any rules... However if I enter ""My identifier"" I'll get error: Identifier has to start with a letter. Allowed characters: lowercase and uppercase letters, numbers, underscore ""_"" and dot ""."" It'd be nice to mention this somehow on the dialog where you edit it.... I'd share a screenshot, however (and that's 2nd feedback :D) the submit feedback window doesn't work when any editing dialog window is opened in the platform It wont allow you to type... 
By michal.hruska@keboola.com at 02/08/2022 13:02:35 
View the feedback
To reply to your user, you can reply to this e-mail.| 
 
 
© 2020 Product Fruits. All rights reserved.Product Fruits 
 
 ",
"It would be helpful to sort rows in the data preview table. The endpoint supports it.
Thanks",
Some components require to be marked as “safe for run in branch” before being executed in Dev branch - but the UI nor the documentation mentions a reason why or some explanation of what it actually means. Could there be some info icon saying why that’s needed? ,
"Coming emails using ""Orchestration"" instead of new ""Flow"" - in the subject and the text of the email.",
"Hi, 
let me explain you our use case. We have several data layers (extraction - original data, L1, L2, DM a writers) and the logic behind is that L1 uses data from extraction, L2 uses data from extraction and L1, DM uses data from extraction, L1 and L2 and writers uses data from all layers.
So we need to be sure that firstly we run extraction, secondly L1, etc..
But we don’t need to recalculate everything every day. Usually we update the data once a day but for some cases, it is okay for us once a week or once a month.
We would like to have one big orchestration which ensures that the order will be correct and phases are triggered after the previous phase is done. To do that we would need to have orchestration which is triggered every day and for some tasks (transformations, writers) we could exclude them from the run if they don’t met condition like it is 1st day of the week or 2nd day of the month, etc..
Do you plan, something like that? It would be really helpful for us. If you need some additional info, let me know.
Best regards,
Jan",
"As long we generate sources with full schema, I think we can have a neat init version of storage support in CLI - by this way a schema changes will be captured.",
"Hi this is for me the most annoying feature (bug for me) of the Flow view -> The modal that displays the config detail. Clicking escape returns you to the flow. Even if you are in row, code block, or any other part. This is inconsistent with the behaviour in normal view / other place. Escape just brings you one level up.
Also try to to go to config detail from flow - display versions and click the X button to get back to the config.
Even on the 1 level it returns you to the whole whole. you completely loose the context and it leaves user confused.",
"I HATE IT A LOTI don’t understand how anyone can prefer it to “army” 13:05 format. Maybe people in US, but they still use gallons and feet and weight people in stones. 
The “Mar 3” is also awful, but I hate it bit less than AM/PM. ISO is best, but even “3rd March” would be better. ",
"ATM there's no way to create a variable for components with Keboola CLI (without using the UI or API call). Furthermore variables can be created only for for transformations in the UI.
It would be sufficient to simply have a ""variable"" option in the CLI for “kbc local create” and “kbc remote create”.
See ticket #22008 for more context.",
"How do i see only the content of the folder?
The folder is not clickable - like a folder elsewhere
I cannot create a transformation directly within a folder as a result ^^
I cannot drag & drop transformation to a folder
I have to manually click on the transformation checkmark and wait for icons to move the trans in a folder.
Some of the icons are grayed so my head thinks those actions are disabled - but its just a poor icon choice.
Fisa",
"dates (on, before, after, between, etc.)
string using ""*"" and/or full regex?",
"When the user creates a manage token, the UI is different than when creating a storage token. It took me some time to realize where to find the manage token. Couldn't this use the same pop up as for storage tokens?",
"Hey gents, when will be “Graph“ fully working across all components?
Example
https://connection.eu-central-1.keboola.com/admin/projects/401/storage/in.c-keboola-ex-google-bigquery-v2-458841703/transactions_croatia#usage
vs
https://connection.eu-central-1.keboola.com/admin/projects/401/storage/in.c-keboola-ex-google-bigquery-v2-458841703/transactions_croatia#graph
Kinda SAD after all those years.
M",
"I would like to be able to create my mlflow models inside a transformation so that I can test and update them automatically.

So would this customer: https://keboola.zendesk.com/agent/tickets/28470

What this requires is adding AWS S3 bucket credentials / ABS connection string to the runtime environment for transformations (added by docker_bundle)",
"""The no-code transformation ""IS NOT BLANK"" does not actually check if blank but ""IS NOT NULL"" I think. So for one it would be nice to have actually an ""IS NOT BLANK"" option and also the option to see the SQL behind the No-Code transformation and optimize it if something is missing. So that the No-Code is not an either or, but a quicker way to get started with a transformation if that makes sense.""
(eQuota - Mark Bitter being the only user for now)",
"Customers cry often to get back that data lineage graph for tables in storage. This time , feedback is coming from Košík. 
btw.It would be a great super motivational feature for Queue v 2, that would make clients wanting to migrate.:)",
"As I work with Flows more and use it in demos as well I realized that I am missing information on what is happening in the flow in terms of data movement. E.g. icon for Snowflake Transformation and Writer looks the same.
I think it would be great to visually highlight the type of operation that happens. E.g. on first sight I could tell it downloads something from external source , transforms or pushes out. This would be a huge improvement in readability and visual feedback from the flows.
See attached example which shows typical E-T-L but one must click through the detail of the nodes/configs to be able to tell so:",
"I think it is confusing for users because this https://help.keboola.com/components/extractors/storage/google-drive/ is not a Google Drive extractor (allowing to download arbitrary files) but a Google Sheet extractor (the documentation itself says: This extractor loads sheets from Google Drive Sheets and stores them as tables in a bucket in your current project. )
In component factory we have a Google Drive extractor in pipeline because it is quite limiting. We have OneDrive but not it’s counter part.

here’s one recent customer/prospect related thread https://keboolaglobal.slack.com/archives/CGG5B6XLL/p1675076035758949",
This was given as feedback from a demo today with a current PAYG user. ,
"Seznam: David Kroupa
Nove transformace: Chybí mu hodně (i pro debug), že nemůže pouštět jen části skriptů. Aby treba mohl pustit 3 code bloky. Aspon ve workspacu.",
"Database connections can sporadically get refused. On those cases, a simple automatic retry could solve the problem of a broken pipeline without any manual intervention, specially outside working hours.",
"See attached recording. 
I want to edit the Hour so I select it and hit “delete” which deletes the field.
Then I try to edit minutes (enter something like 10:30) but when I type anything the fields disappears and UI changes.",
"As agreed, every time I hear somebody crying about data lineage graph , I will submit a feedback ticket. Btw, I am not asking them. They just say it spontaneously. 
This time Rossum - Josef Pithart. He says that “Flow” can be nice for small project and it is useless for Complex and big projects. That we should have done Data Lineage Graph instead of this. It was super useful to get oriented in complex projects with a lot of dependencies. ",
"I have run a google analytics extractor. The run seemingly ended ok, but did not add or change anything in the storage table and there was no warning. 
see https://connection.north-europe.azure.keboola.com/admin/projects/5967/queue/15003578 
It would be helpful, if there was something like ""WARNING: No data retrieved""",
"example scenario:
first code block has 3 rows
second code block has 2 rows
the code fails on the second row of the second block
an error message will yield that there is an issue at row number 5. 
However, row numbering starts at 1 for each code block and it might be an issue to figure out where the problem occured
suggestions:
either continuously number rows in code blocks (start second block with row 4) or return the error message with the row number that relates to a particular block",
"Why? It used to be clickable as a whole. 

Also it says “Success Job”. That’s not English. Should be “Successful job”. Also “Duration 28 sec” seems excessive, just 28s or 0:49h would be enough IMHO. ",
https://keboola.zendesk.com/agent/tickets/24781,
Please add preview so setting datatypes is easier...,
"this is too detailed, imho:
Job 14470431 import to OpenLineage API - end
Job 14470447 import to OpenLineage API - start
would suffice",
"Hi, is it possible to start supporting triggers in CLI? I need it for templates purposes. 
Thank you. Monika


https://keboolaglobal.slack.com/archives/C02C3GD6Q64/p1665998625079709",
"Our enterprise customers do have an issues with your approach to UPPER_CASE and case sensitivity in general. Whereas I know we are working on fixing this on a table level/column level, I have realized the biggest issue is in the DB extractors - and we behave like this for no technical reason:
Do not honor PK from the source tables - we force users to set PK even though the table has it set (?) This could be suggested.
We lowercase the table name in the destination by default even though the table is UPPER_CASEs in the DB? (replicated)
Some of our enterprise opportunities do have a serious issues with this 
I just spoke with a user who is trying to migrate from Oracle (all UPPER_) to Snowflake and we would force them to click all over to change the default behaviour. Matillion (our frequent competitor) does not behave like this.

Furthermore, (and this is potentially more complicated) one prospect is wondering how they would load 300 tables from Oracle through our extractor without configuring 300 rows manually. Our answer was to automate this via API and our internal tooling, though he has the point…

https://keboolaglobal.slack.com/archives/C02PGBVTF54/p1679607154201589

Replicated in the video >>",
"Current sync action based dropdowns will reveal values and labels after clicking the reload button. Once a user saves their choice, the label disappear and only the underlying value is visible, which is confusing.

Reload has to be clicked again to reveal the label:

Consider saving the labels with the Save button to at least keep the selected values.",
"Hi, I have a feedback from client (not a first one actually) that it would be nice to have a possibility to explore data right in the platform. 
See an example where is a suggestion for another tab where the whole data are visible and also there would be a chance to explore data with sql (for example show me all contracts which are <1000K). Basically to have the workspace directly in the platform. Currently, I need to spin up the workspace to I am able to explore data.
Thanks a lot for considering it,
Roman",
"Hi, Juan,  
thank you for your answer. If I need to sort in ascending order, I can just type ""order by"". ""Asc"" is the default ""order by"" setting. So this doesn't solve my problem.  
Regards, 
Iva 
Iva Štohanzlová 
BI Analyst 
www.skippay.cz 
Skip Pay |U Garáží 1611/1, 170 00 Praha 7
+420 702 243 345 
 <https://skippay.cz/?utm_source=signature&utm_medium=email>  
From: Juan Pablo <jira@keboola.atlassian.net> 
Sent: Thursday, July 20, 2023 12:32 PM
To: Iva Štohanzlová <iva.stohanzlova@skippay.cz>
Subject: https://keboola.atlassian.net/browse/SUPPORT-1344#icft=SUPPORT-1344 Order of rows in output   
—-—-—-—    
Reply above this line.   
Juan Pablo commented: 
Hi Iva, 
Thanks for submitting your ticket. I think you are just missing how you want to order by month. For example: “DESC” or “ASC”. You can check it here: https://docs.snowflake.com/en/sql-reference/constructs/order-by 
Let me know if it helped ! 
Regards, 
Juan   
View request • Turn off this request's notifications 
This is shared with iva.stohanzlova@mallpay.cz. 
Powered by Jira Service Management 
Keboola.com • Blog • Documentation • Help & Support 
Dělnická 27, 170 00 Prague 7, Czech Republic    
Sent on July 20, 2023 12:32:03 PM CEST",
"Hello,
is there a way to know for which project the invoice is?
We pay this for our client (just to make their lives easier) and we'll probably soon need to be able to tell which invoice belongs to which project.
Thanks a lot. 




 
Roman AppeltauerData Engineer 
| 
 
+420 720 758 983<tel:+420720758983>

 roman@signals.cz 
www.signals.cz
 
On 17. 7. 2023 22:34 +0200, Keboola Czech s.r.o. <invoice+statements+acct_1Gwr8rLrNShX2jIG@stripe.com>, wrote:

Your receipt from Keboola Czech s.r.o. #2399-7193 ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  

Keboola Czech s.r.o. 



Receipt from Keboola Czech s.r.o. 
$40.66 
Paid July 17, 2023 



 
Download invoice <https://59.email.stripe.com/CL0/https:%2F%2Fpay.stripe.com%2Finvoice%2Facct_1Gwr8rLrNShX2jIG%2Flive_YWNjdF8xR3dyOHJMck5TaFgyaklHLF9PSFhDVkYwVzZlaDhvR3Z5azAwMkRkbGN0bGNNakRMLDgwMTY2ODkx02006v5FxaR8%2Fpdf%3Fs=em/1/0101018965903a6d-1c26c853-742d-42a5-a1cf-0235b04c3223-000000/ID61Bvy5y0_4CuXZmRPpcZruhHVurIGIlqSnHZWON4M=309> 

Download receipt <https://59.email.stripe.com/CL0/https:%2F%2Fdashboard.stripe.com%2Freceipts%2Finvoices%2FCAcQARoXChVhY2N0XzFHd3I4ckxyTlNoWDJqSUco68vWpQYyBmxdIK6GyTovFiOYPpAJQl-OiQkbjET5gnx61zfKArslJcBrJOoTjcPhVOG2AeaDI1JZOr2SyBA%2Fpdf%3Fs=em/1/0101018965903a6d-1c26c853-742d-42a5-a1cf-0235b04c3223-000000/omIffrVggTE7cs2xVCeyeIwPdQo-zJELaV293XYoJqw=309> 

Receipt number 

2399-7193 

Invoice number 

4-1769 

Payment method 

8195
 




Receipt #2399-7193 


Credit 

Qty 4 
$33.60 
$8.40 each 

Subtotal 
$33.60 






Total excluding tax 
$33.60 

VAT (21%) 
$7.06 






Total 
$40.66 






Amount paid 
$40.66 






Questions? Visit our support site at https://support.keboola.com/hc/en-us/requests/new, contact us at support@keboola.com, or call us at +420 608 499 077. 



Powered by  
<https://59.email.stripe.com/CL0/https:%2F%2Fstripe.com/1/0101018965903a6d-1c26c853-742d-42a5-a1cf-0235b04c3223-000000/ugIYwyg3EnPpuSVYh4qPYtQPACv8rGjF1pLRs9nJrGk=309> : Learn more about Stripe Invoicing ",
"I have a very simple problem but it is unclear how to solve it based on your documentation here: https://developers.keboola.com/extend/generic-extractor/tutorial/basic/
I need to authorize my API calls using headers in the form `{""Authorization"": API_KEY, ""accept"": ""application/json""}`. Please help.",
"Hello! I'm trying to create a Tableau TDE writer to a Tableau server, and I keep getting a 403 permission error with my login after I run it (which I've tested and I know is correct).
Do y'all have any context you can help me out with? Do I potentially need to whitelist the Keboola IP addresses on our Tableau server?
And if so, does it need to be all the 75 IP addresses listed here?
https://help.keboola.com/components/ip-addresses/ 
Job 995802819
Thanks in advance!
Michael Ryan (mikeryan@pond5.com)",
"Hi guys,
Can you please check this error for me - I do not understand where the problem is.
https://connection.keboola.com/admin/projects/9231/transformations-v2/keboola.snowflake-transformation/981139859",
"Hi, is there any extractor for Graphql API?",
Not sure why the trigger is failing. I'm using a v2 orchestrator.,
"Hello, 
My company uses Sage Intacct's ERP system and currently we are using a custom built process to sync our ERP data to our database. A Sage Intacct Component would be a huge help to our business. 
Thanks, Cason CTO",
"Hi guys,
I would like to know if there is any function I can use to change parameters in query I sent. I have one case where the API endpoint returns current month by default but I need data from the previous month as well since there are some changes during current month.. The problem is that the endpoint doesn't operate with parameter date but it has two parameters: month and year. Is there any function or other way how to achieve this? 
Best regards,
Aleš ",
"Hi, when using custom component the max runtime is 1h. Is it possible to extend it? Temporarily or permanently?
Thanks",
"I would like to terminate job and the request does not responses. It tooks 20 minutes and the terminating process is still in processeing. By this way I would like to ask you for help 
Job number: 596762636",
"Hi,
could you pls add organization and customers into Datasets? We need it to for internal reporting purposes.
Thanks
Dave",
"During Job (for example 997586900) we are receiving error:
Error connecting to DB: SQLSTATE[HYT00]: [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired.
message: Error connecting to DB: SQLSTATE[HYT00]: [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired
context
exceptionId: docker-780ed3093b8f4c4fccf451bcaffec449
config: 975920430
component: keboola.ex-db-mssql
mode: run
Connection to the server is working.  
We also contacted server provider and they recommended us to contact Keboola support as connection and server is working properly on their side.",
"Hello,
I would like to ask you, if is possible send a special monthly pricing for our Generali Česká insurance company? We use your classic billing options, but i think that you have a some special pricing possibilities, is it true? Please could you send me a information on (petr.vodicka@generaliceska.cz) and for my manager on (ales.kyr@generaliceska.cz).
Thank you and have a nice day.
Petr Vodička",
"Hello, I am struggling with singing up into snowflake workspace. When I fill in the username and password on the snowflake sign in page and click on Sign in, nothing happens and the page leaves blank. 
I have deleter cookies, try also other browser and nothing helped.
Could you please help? 
Barbora Nemravova",
"Hi is there any way to execute an incremental transformation as full load? eg. using a parameter to turn off inceremental load? 
If you still do not have it - would it be possible to tell the product team to add a parameter on the transformation so I can manually override increments? this is needed when someone makes a mistake. and needs to reload all the tables. ",
"Hello, 
I am about to investigate Datahub application implementation in our project (following this guide https://help.keboola.com/templates/datahub/), but can not find it in applications list of components section. Could you give me hands on that please?
All the best 
Tereza",
"Hello, I have a little bit of issue to connect Shoptet extractor. I have noticed that you need cvs format in the link, however when I use link with csv in it it tells me that the link is incorrect and I dont know what am I doing wrong. Could you help me with that please? Thanks a lot ",
"Hi,
I am getting a Application error in my Snowflake transformation despite all tables updating correctly. What is causing this and how can I fix it? This is very important to get a new Sales Lead Source Orchestration out.
Thank you!
https://connection.keboola.com/admin/projects/976/jobs/999110803?q=",
"Hello, I would like to know how to connect keboola to the reports API of Shopify : https://shopify.dev/docs/api/admin-rest/2023-04/resources/report#post-reports
? 
As for now I think there is only transactional data available, no data about traffic, add to cart etc.. ",
"hey, is there a way to restore a development branch that i have deleted? ",
"Hello,
I would like to see how much minutes i spend in keboola project each day. Could you send me minutes balance development from the beginning of the project history? - https://connection.north-europe.azure.keboola.com/admin/projects/16054
Regards,
Van Hieu Vu",
It is not really clear from this,
"I am trying to find the datahub destination component listed here https://help.keboola.com/templates/datahub/
It does not appear to be in the developer portal.
Thanks.",
"Hi guys how is it possible that we start a new project with -25 minutes? Can you please top it up? 
Best regards,
Aleš",
"Hello, i wouid like to get some support to setup Mongo DB Atlas component.",
"Hi, I would like to use the xml to csv component, would you help me to set it up?",
I am setting up a configuration that uses google drive/sheets as a data source and every month a new column will be added. When I try to add a column to the sheet that column is in Keboola storage but gets ignored in my snowflake data destination step. Is there a better way to set up this scenario?,
Test ,
"Hi guys, any idea what the problem is? ",
"Hi,
I would like to ask for enabling the new Activity center template for our 30 - {DM} Activity center project.
Thanks.
Matej",
"Hello, when I extract the data through the instagram API the like count of a post does not match the real data displayed in the post ? 
was there any change to the api ? 
https://connection.north-europe.azure.keboola.com/admin/projects/13275/table-preview/in.c-keboola-ex-instagram-36541160.media?context=%2Fadmin%2Fprojects%2F13275%2Ftransformations-v2%2Fkeboola.snowflake-transformation%2F37320888
For example for post : 
select 
*
from KEBOOLA_13275.WORKSPACE_48363557.""instagram_media""
where ""id"" = 18370654477008217",
primary key,
"wrong filter, and also need a Union ",
wrong count(),
"Hi there, 
It seems like the Heureka.sk extractor doesn't return the data from the platform. Jusn the empty values. Would you please tell me if there is any issue from your side. It seems like we have just tried everyhing. 
Best regards,
Krystof",
"Hello team,
I am trying to write a generic extractor for not fully RESTful API.
The endpoint in question requires POST method and has pagination rules which would fit Offset method.
Need to pass maxResults (i.e. limit parameter) and startAt (i.e. offset parameter).
Last thing, I need to pass non empty string as a query string for the POST method, which must be passed in the body of the request. 
What seems to be happening from watching logs in the component is that when I set the endpoint method to be POST, it passes the pagination rules in the body of the request instead of in the url parameters as would be with GET.
Could you please confirm that this is the case and potentially help me figure out a way to pass the query string in body and the pagination in url? I cannot figure it out from the docs online. 
All the best,
Matyas Jirat",
"Hey guys, this is a second ticket i am submitting with a request to have this component activated. Thank you.
pavel",
"Good afternoon! It looks like the Shopify writer API version that we are using has been deprecated, and this is something we are not able to update on our own in Keboola. Could you please help update this or allow us to choose the API version ourselves? Thanks!",
"Hello, 
Is it possible to restore deleted table in storage by time travel feature? 
If Yes, how? 
Regards, 
Filip ",
"Hi, 
Did you find, what may be the issue?
Best regards, 
Krystof 
From: Juan Pablo <jira@keboola.atlassian.net>
Date:  Monday, 7 August 2023 16:47
To:  Krystof Kasper <krystof.kasper@publicisgroupe.cz>
Subject:  SUPPORT-1594 Heureka.sk, empty data values   
—-—-—-—    
Reply above this line.   
Juan Pablo commented: 
Hello Krystof, 
Thanks for submitting your ticket. I’ll check it and come back to you as soon as possible. 
Regards, 
Juan   
View request • Turn off this request's notifications 
This is shared with Kryštof Kasper. 
Powered by Jira Service Management 
Keboola.com • Blog • Documentation • Help & Support 
Dělnická 27, 170 00 Prague 7, Czech Republic    
Sent on August 7, 2023 4:47:08 PM CEST      
------------------------------------------------------------------------ 
Disclaimer The information in this email and any attachments may contain proprietary and confidential information that is intended for the addressee(s) only. If you are not the intended recipient, you are hereby notified that any disclosure, copying, distribution, retention or use of the contents of this information is prohibited. When addressed to our clients or vendors, any information contained in this e-mail or any attachments is subject to the terms and conditions in any governing contract. If you have received this e-mail in error, please immediately contact the sender and delete the e-mail.",
"Good day, can you advise me where the error is in this simple call? Everything works through google explore api
Thnks",
"I had an entire flow set up under a trial account just yesterday, and now it says I do not have an account at all. 
 
From: support@keboola.com <support@keboola.com>
Sent: Tuesday, August 8, 2023 2:24 PM
To: Aaron Bell <aaron.bell@evereve.com>
Subject: [Keboola Connection] Password Changed 
 

<https://connection.keboola.com>
You've successfully changed your password.
If you did not make this change and believe your Keboola Connection account has been compromised, please contact our support. 
Contact Support
Questions? Email us at support@keboola.com

<https://connection.keboola.com>
 Keboola.comDělnická 27, 170 00 Prague 7, Czech Republic ",
"I have the dbt docs generate step enabled, and I've run the transformation repeatedly, but I cannot access the dbt Project documentation despite all that.",
"Hi, how long should be the gap between changes in Storage (downloading a new table) and the moment when it appears in read-only workspace?  I can not reach such new table for 15 min already.  How can I force the refresh? (the refresh button in snowflake workspace does nothing in this case)? ",
"Please, with a lot of new features, users get lost what is enabled and where. 
To be specific, Radek Hájek from Kofola would appreciate some overview what feature is enabled and where. 

He really loved the table I showed him in Looker. This is exactly what he would need. I checked with matejkys and it is not part of telemetry extractor and he thinks it should be part of UI. Therefore I am submitting this suggestion.   https://keboola.looker.com/explore/keboola_internal_reporting/kbc_usage_metric_values?qid=eFZ0pmR2yo8e9FzgznJRTq&origin_space=94&toggle=vis",
"Hey guys, 
I am trying to play around with Full Friction data. When trying to make it accessible in another project of mine, I cannot find a way to share this. ...sure, I am losing the logic here, but i cannot find a way to share this, how could another novice user do. If I am not allowed to share from Storage, i should be able to do it via Data CAtalog? But the table cannot be found there..... 
Is there anything we can do in here to make it a bit more intuitive?
Cheers
Pavel",
"Hey guys thanks for adding the feature AI configuration descriptions. It is great one, however it crashes a lot. It gives just Error - Bad request. But once tweaked out it it will save valuable time as well as will improve the documentation.",
"Hello,
I create branches for testing some changes(adding new columns to 2 tables). When a branch is created, I change the first transformation(add a new column to create a table ), and after that, I run the transformation. In Storage, I can see a new branch table with a new column. 
When I need to add this new column to my second transformation, I open this transformation, add a new input table - the same table but from a new branch, and when I open details... I can't see this new column(Keboola took details from an old table, not a branch.)
So in general, I can't test this change in the branch. 
Thank you for your fix.",
"When there is CLI activity we don't see Storage on project Dashboard anymore. But we do want to see the Storage rather than CLI. We'd expect we should be able to setup what and how we want to see on the ""Dashboard"".",
Could transformation folders have description field as well?,
"Hey, 
it would be nice to be able to filter for NULL values in the Data Sample view of table now that Native Types are allowed and we can have actuall NULL values in the columns.",
"I would recommend not offer native datatypes to any of our customers and especially prospects. 
Vast majority of components running in our platform work with changing schema in a way that it is forever expanding - for instance CDCs. This is necesssary because failing job each time a new column appears sucks. Our components remember schema so they do not fail on deleted columns either.
All this functionality is useless when Native Types are enabled because it doesn't allow for changing schema. 
I suggest fixing this asap because users are now able to turn native types on. And if you create new PAYG projects with them a shitstorm is coming your way.",
"Hi,
Would you like to acquire the Attendee Email list of “Web SummitExpo 2023”
Titles: Founders and CEOs of Technology companies, Fast-growing start-ups, Investors, Policymakers, Managing Partners, CMO, Marketing and events Executives, Deputy directors, Full-Stack Developers, Representatives, Contributors, and More!
Could you please let me know your thoughts?
I'll get back with precise counts and pricing details for your consideration.
Best Regards,
Marlene Baker| Senior Event Manager",
"Hi, Keboola support,
Please do not delete this project. Our client wants to continue.
Thanks",
"Hello Keboola, 
I have a problem, with AdForm (MasterData) Job 602991169. Would you be so kind and look at it? It doesn't seems to be extracting all the data. 
Best regards,
Krystof",
"Cannot add next users for watch ""Support Requests"". Please see picture in attachment. I tested name, login, e-mail. Dont work it.",
"Hi team,
we need a File input mapping to be enabled for S3 data destination component. MiQ is currently evaluating our platform and main usecase is extraction of data from sources and storing it in S3. They use their S3 as a backend of their project but they need to bring it to different buckets and also folders. So using an S3 writer would be an acceptable workaround. 
Michal",
"Hello guys, 
I found some small bug. Bucket have ""shared"" tage when I share it using Data Catalog. This SHARED tag is missing when I search for the bucket. 
Also It would be nice to search all shared buckets. Thanks. 
Kind Regards,
Matúš Frick",
"Assignment URL (mysql-kds-ca.chyq0ekobopj.us-east-1.rds.amazonaws.com) given under the ""Best Practices""  is not loading, kindly recheck",
"Hi, i have problem with assignment to Common components and processos.
I set all credentials, job write success, but no data was store. Can you look on tjis problem? You can find my settings in https://connection.north-europe.azure.keboola.com/admin/projects/15189/components/keboola.ex-aws-s3/43073398/rows/43194585.
Thanks for your help",
"Hello, 
my project expired. Could you please create a new one for me? I think I had increased free computational time per every month there. 
Thank you. 
Regards, 
Lukas Kouril",
"Hi, please implement alphabetical sorting of tasks in each group in flow, now they are in the order of when they were added and it becomes hard to navigate quickly.",
"Hey folks, here's the latest feedback on Data Apps from Michal Pich:
PAIN POINTS: 
connection to a private rep is missing
when adding a code directly into the UI, I cannot use multiple files (one only is allowed)
missing config for login credentials - “i do not want to put passwords into the repo, so it would be awesome if i could put them in config in KBC directly""
Output Mapping - missing we do this via API, if we want to write any table. But it is not a good pattern.
possibility to turn it on for a limited time (like for 30 mins, as defined by a user)
Authentication - now the App is publicly available. I would need to make it accessible based on authentication. How should i do it?
....hope this info helps.
Cheers
Pavel",
"HI 
I have a GA4 extractor that I used to pull data out of GA4.
When I compare the data extracted to the data i see in GA (using https://ga-dev-tools.google/ga4/query-explorer/) I see a discrepancy between the data coming in with keboola and the data in GA
Can you please help me understand this?
I attached screenshots from both tools in a zip below. 
Thanks ",
"Hello I have problem with connecting to snowflake workspace. I fill up the credentials, then the page loads for a while and then it stays blank. I tried different browser, and have deleted cookies. Could you please help?",
"Juan,
Does this get a new ticket created?
Fingers crossed. 
Thanks,
Alex 
‑‑
ALEX BERNSTEIN
he/him/his
ASCEND
Founder, Chief Executive Officer
W: 412.745.2141
‑ C: 412.638.0708
On Aug 15, 2023 at 6:51:53 AM, Keboola EU Support (Keboola.com) <support@keboola.zendesk.com> wrote:
 
##- Please type your reply above this line -##Your request (28544) has been solved. To reopen this request, reply to this email.
You can go to the ticket directly: https://keboola.zendesk.com/hc/requests/28544 
 
Keboola EU Support (Keboola.com) Aug 15, 2023, 12:51 GMT+2 Hello Alex, Would you mind to re-submit your ticket again please? We migrated our Support platform and would be great to follow up on this ticket. Thanks,Juan
 
Alex Bernstein Jun 22, 2023, 02:16 GMT+2 Evening, Juan - any updates on this effort? We are still in negative credit territory and all of our jobs have failed since 6/16. Thanks!Alex
‑‑
ALEX BERNSTEIN
he/him/his
ASCEND
Founder, Director of Finance + Business Development
W: 412.745.2141
‑ C: 412.638.0708
On Jun 16, 2023 at 11:02:22 AM, Alex Bernstein <alex@ascendclimbing.com> wrote:
That’s awesome and generous - thanks, Juan! Much appreciated.  
 
 
Alex Bernstein Jun 16, 2023, 17:02 GMT+2 That’s awesome and generous - thanks, Juan! Much appreciated. 
‑‑
ALEX BERNSTEIN
he/him/his
ASCEND
Founder, Director of Finance + Business Development
W: 412.745.2141
‑ C: 412.638.0708
On Jun 16, 2023 at 11:01:32 AM, Keboola EU Support <support@keboola.com> wrote:  
 
Keboola EU Support (Keboola.com) Jun 16, 2023, 17:01 GMT+2 Hello Alex, Thanks for submitting your ticket. The issue was actually on snowflake side but we are refunding the creds for the failed/prolongued jobs. I'll do it during the rest of the day if that's ok for you. Regards,Juan
 
Alex Bernstein Jun 16, 2023, 15:17 GMT+2 Hey team! Looks like there was some sort of platform issue that blew through thousands of our minutes last night. Would you be able to get this sorted out for us and our minutes back on our account? Thank you!------------------Submitted from: https://connection.north-europe.azure.keboola.com/admin/projects/1634/dashboard
] This email is a service from Keboola.com. Delivered by Zendesk.   [42KWM9-JLR2J] 
 ",
"Does this work??
  
Martin Fiser|Field CTO - North America| Vancouver
  
 
<https://bit.ly/empower_fiser>
 
<https://www.linkedin.com/in/fisermartin/>
<https://twitter.com/keboola?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor>
<https://www.youtube.com/channel/UCUDR3znPbK9xvOHYIGBBSIA/videos>  
  
Book a Meeting:  
15- 30- 45- 60   
 ",
"Greetings,
I had a starter project in Keboola that got deleted because I didn’t access it for a while, but my team is ready to start looking into building it again. It will be an expansion of what I had started before, is there a way to get it back or do I need to start a new project?
Thanks,
Zachary Landry
Sent from Mail for Windows",
"Hello,
Please delete my account. 
Thank you",
"Is there an easy way to view which components / flows are consuming the most credits?
Also, we're no longer using the Keboola hosted Snowflake cluster, can you delete it? I don't see a way to do that in the UI.",
"Hello, 
I have signed up to Keboola Data Academy but did not receive invitation to KBC project. 
Could you please send me the invitation? 
Best regards, 
Patrik Barna 
Česká spořitelna 
Data Specialist, squad Personalization Platform 
Budějovická 1518/13  
140 00 Praha 4 
M +420703101762 
pbarna@csas.czInviti 
www.csas.cz ",
"I keep getting ""Invalid credentials or permissions."" with my AWS S3 destination. Ive followed all the instructions, even gave the user more permissions than i would have liked. Would be nice if I could only give it access to a path in a bucket",
"Hi,
has there been any chnage for Shoptet Permalink extractor component recently? 
if yes I suggest to roll it back and assess whether the enforcement of using the predefined URLs is necessary. 
thank you
Example
https://connection.eu-central-1.keboola.com/admin/projects/3431/components/kds-team.ex-shoptet-permalink/410500975
Failed job
https://connection.eu-central-1.keboola.com/admin/projects/3431/queue/606832148",
"I needed a project for a small thing, so I have created PAYG and then in the UI, I just had to create a flow. There was no option to close it or exit… I had to do a dummy flow to do something meaningful for me…omg",
"Hi, currently the Freshdesk extractor is loading only limited amount of data due to rate limiting issues and we'd like this problem to be solved (discussed with Timothy Kuhn before). We're currently using the 'Estate' plan on Freshdesk (https://developers.freshdesk.com/api/#ratelimit))",
"Hey guys, here`s a feedback on a possible simplification of sharing options for the buckets/tables in KBC as being discussed with @Tomas Netrval:
Goal: To make the sharing options a bit more intuitive and visible, as currently it takes ages to find how to share it into another project….
Here are the ideas for improvement:
1. The option to share a bucket to be available also within the Bucket view (see a screenshot attached). 
…add the Share Bucket option into the three dots drop-down. Once the user clicks on it open the modal with a pre-filled name of the bucket selected for sharing.
2. The option to share a table from the Storage view (again see a screenshot attached). 
The user would see the option to share among other options such as “create alias”, “create snapshot”, etc…
Once they click on it they would see the modal where again the given table is already selected.
Hope this makes sense. Please let me know if any questions. I would be more than happy to clarify more.
Thanks
Pavel",
"Hello,
I am using GoogleAds extractor to download data regarding impressions, clicks and spend of campaigns. For one account, namely customerId 8421626618, one campaign type, campaignAdvertisingChannelType DISCOVERY, is not downloaded. All other campaign types download normally without any problem. Can you think of where the problem could be and why this type of campaign is missing?
(Configuration 411230167 )
Thank you
Kateřina Švubová",
How can I change org name? ,
"Hello I'd like to ask whether is possible to send me an expert list. I haven't time to finish setup of our project.
I need to create connection to Facebook Ads and Google Ads and pull certain date from those system. I have basic setup and I need somebody how is expert for that and support me. Thank u in advance.
BR Dusan",
I’m developer,
"Dear Keboola’s team,
Is Keboola currently hiring Software Developer position? We have a pool of talented individuals with diverse skills, and leveraging their expertise can save time, effort, and cost associated with scouting for specialists in each of the areas.
My name is Andrew Komissarov – I'm the Chief Innovation Officer at Andersen, a software development company. Some time ago, I emailed your colleague, Petr Simecek, to discuss possible IT cooperation between our company and Keboola, and then I was redirected to this email address. 
Thinking of what Keboola is up to at the moment, we’d be happy to involve our experts into your projects’ development — it may be a real boost! 
If this is of interest to you, may I know your availability for a brief call this week?
I'm looking forward to hearing from you.
Regards,
Andrew Komissarov 
<https://www.linkedin.com/in/andrew-komissarov/> 
Chief Innovation Officer 
<https://clutch.co/it-services/staff-augmentation/leaders-matrix> 
<https://andersenlab.com?utm_source=email&utm_medium=L1> 
Click here if you don't want me to follow-up.     ",
"The component  https://connection.eu-central-1.keboola.com/admin/projects/3822/components/carvago.ex-kb/583970806 suddenly stopped working. Now it hangs in status ""Running component carvago.ex-kb (row 1 of 1)"" and nothing happens. Could you please take a look what could cause a problem? ",
"https://prnt.sc/AIZLpHLf6l79
giving sev2 because it would be easy to fix but it has very bad UX",
"Thank you for the report.
We're currently in the middle of the process of setting up DMARC for our domain - that's why DMARC is in ""report only"" mode.
Vladimir Kriska
On Tue, Aug 29, 2023 at 12:53 PM Mark Lee <mark@cybarriersolutions.com> wrote:
Hi Team,
We have found your website vulnerable to this vulnerability.
Vulnerability Type: Mail Server Misconfiguration Leads To Hijack Your Mail Server Due To BAD DMARC Implementation/Cross-Site Scripting Attacks/ Malicious Payloads Injection/Victim PC Hijack
Severity: Critical
Description:
DMARC Quarantine/Reject policy is not enabled on keboola.com so due to this attackers will be able to send an email using your domain keboola.com to your users.
Due to this vulnerability, Attackers will be able to use any email of your application and send email to any one of your users,
Proof of concept:
Impact:
Attackers will be able to send an email using your email 
 support@keboola.com
and other official emails of your company/staff members which will result in your business and reputation loss by sending unwanted emails to your users/clients as a result of exploiting this vulnerability.
Attackers will be able to take over any user account by sending your user's malicious links using your company/official emails which will result in a complete account takeover of any user as they will follow any instructions given by your official company emails. And more they will be able to do anything with your users by communicating with them by using your company/emails. 
Attackers will be able to execute malicious scripts on the victim's PC by sending them malicious URLs and payloads directly via email as the clients will see that these are from authenticated sources of your company email which will result in malicious payloads/URLs being injected into your users PC. 
Attackers will also be able to attach malicious files using your email and once the files get downloaded to the victim PC resulting in a victim PC hijack.
Cross-Site Scripting Attacks
Account Takeover
Transactions Performed
Money Scam
Actions Performed by Attackers
Malicious Code Execution
Victim PC Hijack
Malicious Files Get Downloaded To The Victim PC
Attack Scenario:
An assailant would send an email saying ""with any unwanted information"" (or any malicious files attached with the email and instruct them to download it) using your company emails like 
 support@keboola.com
and any email which is found to be linked with your domain. Clicking on the connection takes him to a site where certain JavaScript is executed which takes his keboola.com information. The outcomes will be more perilous. POC is likewise joined. You can likewise observe that I can utilize your space name email and will be able to send the mail to any of the clients of keboola.com
This is just an example of what attackers will be able to do using your company emails. This issue has a larger impact as attackers will be able to spoof your users by sending unwanted emails and users will do the actions accordingly provided by the attackers. Attackers will be able to spam your whole application and your users. They will be able to take whatever they want from your keboola.com users. Their credentials, their credit cards, and much more.
Attackers will also be able to perform transactions with your clients/users by communicating with them using your official company emails as we have shown in the proof of concept.
Mitigation:
You should review the policies to be applied for the DMARC mail server configuration. As I see your DMARC is misconfigured. Just Add DMARC Quarantine/Reject policy in your DMARC configuration and so attackers will not be able to exploit this vulnerability on your application.
Thank you
Sincerely,
Mark lee 
  <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality11&> 
Sender notified by  
 Mailtrack 
08/29/23, 03:44:23 AM 

 
 ",
"Avocado: https://connection.north-europe.azure.keboola.com/admin/projects/13902/transformations-v2/keboola.snowflake-transformation/412363
Netflix: https://connection.north-europe.azure.keboola.com/admin/projects/13902/transformations-v2/keboola.snowflake-transformation/412405
Premier League: https://connection.north-europe.azure.keboola.com/admin/projects/13902/transformations-v2/keboola.snowflake-transformation/412496
For Avocadopocalypse the hardest part was understanding the case in general. No clues there for what could be wrong, just that the numbers are too low. So the assumption was that I have to divide by a smaller number which I tried and the number after that was the same as the correct result from the assignment.
For Netflix the unclear part was for country/category, there can be multiple items concatenated in each so I was not sure if I am supposed to handle that or not. But after going through the queries I decided not to complicate things with splitting rows into duplicates with only one country and one category each.
This assignment was easier than Best Practices for me because it was more clear in general and also I am a bit more familiar with Keboola now.",
"After adding some other users to my project, when they attempt to join, Keboola prompts them for a password. These users signed up using google authentication so their accounts didn't have a password set up. We resolved this by adding a password to the accounts. I am not sure if this is intended behavior, but the others could not authenticate to join my project using google authentication.",
"Hello,
Please check my completed task.
https://connection.north-europe.azure.keboola.com/admin/projects/14991/transformations-v2/keboola.snowflake-transformation/412496",
"Avocado - https://connection.north-europe.azure.keboola.com/admin/projects/16144/transformations-v2/keboola.snowflake-transformation/412363
Netflix - https://connection.north-europe.azure.keboola.com/admin/projects/16144/transformations-v2/keboola.snowflake-transformation/412405
Premier League - https://connection.north-europe.azure.keboola.com/admin/projects/16144/transformations-v2/keboola.snowflake-transformation/412496
the primary key could have been dropped and it would work, but there was no way to distinguish the team that scored those goals, which was the primary goal of the data analysis",
"Hi,
in one extractor job (https://connection.eu-central-1.keboola.com/admin/projects/49/jobs/612010400) there is this error:
Internal Error
Something is broken. Our developers were notified about this error. Feel free to contact support to request more information.
Can you help me. What can I do for fix it.
Thanks
Regards, 
Lenka Hanková",
I got an application error when pulling data.  Is there more detail what the error could be about?,
"Hi
The client accidentally deleted their KBC orchestration, which they use daily. I can see it in the trash, but I don't have the ""restore"" icon. Can you help me or provide guidance? It's an orchestration named McPen, deleted this morning.",
"Hi.
The client accidentally deleted their KBC orchestration, which they use daily. I can see it in the trash, but I don't have the ""restore"" icon. Can you help me or provide guidance? It's an orchestration named McPen, deleted this morning.",
"Hello, 
How can I get Data Engineer Certificate? 
Or what file should I submit? 
Data Engineer Certificate - Keboola Data Academy 
Thanks, 
Martin Dojčar",
"Hello I have just began receiving failures in all of my configurations.
This is the error:
Cannot import data from Storage API: Application error. Please contact our support support@keboola.com with exception id (kbc-us-east-1-connection-5fcbf6f4d3bbfd74a1f0fb1fdcce597c) attached",
"Our salesforce syncs are failing to import to a table because of an issue with AWS_I_AM, which appears to be on the Keboola side.
Salesforce: Event 7398481198
Job ""1011046608"" ended with user error: ""Failed to process output mapping: Failed to load table ""in.c-kds-team-ex-salesforce-v2-915068449.Yellow_Flag__c"": Load error: An exception occurred while executing a query: Failure using stage area in [AWS_IAM] after multiple attempts. Cause: [Error getting subscoped AwsAccessCreds]. This is usually due to a temporary failure condition with the cloud provider. Try ag""",
"We are getting the following error when we try querying the data via the Storage - Data Sample or from the Snowflake schemas, we can't load data to workspaces, etc.
This is affecting the whole project and is critical to get it working ASAP.
Error:
SQL Error [91002] [22000]: Failure using stage area in [AWS_IAM] after multiple attempts. Cause: [Error getting subscoped AwsAccessCreds]. This is usually due to a temporary failure condition with the cloud provider. Try again.",
"I am receiving the error below when I am running the transformation
SQL error: Failure using stage area in [AWS_IAM] after multiple attempts. Cause: [Error getting subscoped AwsAccessCreds]. This is usually due to a temporary failure condition with the cloud provider. 
Any idea what this indicates?
Leo",
"Hello, 
we are running into trouble with our multicountry data, which is stored in MongoDB. If I'm correct, the current mapping does not allow adding custom columns (e.g. a string defining the country/timezone) as a SQL would, so we need to have separate transformations in order to get data into one table with country identifier at the end. I know that MongoDB projection allows this, but for some reason the query in the MongoDB extractor does not support projection. Would it be possible to add projection to this component?
Thank you in advance, 
Eva-Marie Lacinová",
"here's a different job error:
https://connection.keboola.com/admin/projects/9508/queue/1011070717
^^ that is what you get when you go into a workspace and try and execute a query",
"Hi, I need to run dbt run, but I need to run it up to a subfolder in c-coates-arbys called dbt. Is there any solution to solve this currently?",
"Hi, what is the best way to run query to Google Analytics like this: 
{""dimensions"":[{""name"":""contentGroup""}],""metrics"":[{""name"":""sessions""}],""dateRanges"":[{""startDate"":""2023-08-01"",""endDate"":""2023-08-01""}],""dimensionFilter"":{""filter"":{""stringFilter"":{""matchType"":""BEGINS_WITH"",""value"":""Sběratelství""},""fieldName"":""contentGroup""}},""orderBys"":[{""dimension"":{""orderType"":""ALPHANUMERIC"",""dimensionName"":""contentGroup""}}],""metricAggregations"":[""TOTAL""]}
Where I will be able to use ""metricAggregations"":[""TOTAL""]}
Because it's fiferenet then SUM() of all rows.
I try python transformation, but I have problem with permisions. Is there any way how to modify this extractor? So I can download data in different form? 
Like for example Use Dimension Filter, without using Dimension in section Demensions.
Supermetrics can do this but we are not able to do that in Keboola :/
THanks for any help.",
"Native type functionality does not always work correctly / has limitations:
(not a blocker but would be great to have it ) Creating new table by Native types fuctionality in extractor with advanced query does not work - new table is created as text (advanced query is used pretty much always)
advanced query se dela protoze je v pozadi velka tabulka ktera by se normalnim importem nenaimportovala (Oracle, Hive, Onedrive, atd.), Rockify ridi tu flow pomoci variables ktery vklada do query aby se naimportovala pouze cast tabulky
L0 projekty nejdriv zalozi tabulku pres API ktera se plni extraktory pres advance query (techto je vice)
L1,L2 projekty zakladaji novou tabulku pomoci extraktoru pres advance query (techto je vice)
When creating new table by native types in oracle extraktor I noticed following errors: 
All columns in db are created as nullable except PK, athough in Keboola UI it reflects nullability, it does not match db
Text lenght is not reflected in db, it is default athough in Keboola UI it shows lenght of text, it does not match db
Numbers with decimal point are screwed because number fields are created in default NUMBER(38,0) and all after decimal point is lost
when importing data its converted from DATETIME => DATE and datetime values lose time fraction HH:MM:SS
Update statement in incremental merge does not have condition validating value changes. It always updates records that already exists in target tables and this has higher cost ad run time. Confirmed both by Waldek Kot and proved by tests. Also timestamp changes every day.
souvisi s povolenim null hodnot? aktualne s native types se nedelaji komparacni query
pokud nekdo pocita s timestampem tak my mu to prepiseme
Cheers
Petr V  ",
"I confirm!
On Thu, Sep 7, 2023 at 3:46 PM Zora Jelínková <jira@keboola.atlassian.net> wrote:
—-—-—-— Reply above this line. 
Zora Jelínková commented: 
Hi Matuš, 
thank you for the ticket. I am adding Pavlína as a participant of this ticket and please, can both of you reply once again that both of you agree? 
Thank you 
Zora 
Zora Jelínková updated a comment: 
Hi Matuš, 
thank you for the ticket. I am adding Pavlína as a participant of this ticket and please, can both of you reply once again to this ticket that both of you agree? 
Thank you 
Zora   
View request• Turn off this request's notificationsThis is shared with matus.pavliscak@productboard.comand Pavlina Fronkova. 
Powered by Jira Service Management 
Keboola.com• Blog• Documentation• Help & SupportDělnická 27, 170 00 Prague 7, Czech RepublicSent on September 7, 2023 3:46:49 PM CEST   ",
"Hi, would it be possible to renew my account.
Thanks in advance
Best regards.",
"Hi, can you please check if everything is ok with orchestrations? We have lot of long running orchestrations now across projects. It seems that jobs itself are running ok, but it takes lot of time for orchestrations to start and complete. ",
"Hi, can you please help me with getting access to our Keboola org?
We are dealing with a potential security incident and need that access for review.
Thanks! m.",
"Hi, could you give me an advise what is wrong?
I read documentation and it looks like I need to skip 5 lines and after that create manifest. When I run configuration it seems it was success but there is not output...
https://connection.north-europe.azure.keboola.com/admin/projects/11007/components/keboola.ex-aws-s3/51296265/rows/51298248",
"Hi, we have problem with ssh tunneling to our server.. which mean that we are unable to load data. Could you please tell me which IP adress Keboola uses? So we can debug the problem (which is probably) on our server? Thx!
Petr",
"Hi,
I ́m struggling with facebook pages extractor. After authorization I don't see the pages I selected in the authorization. I tried multiple accounts, some pages I can see and some I can not. 
Michal",
"Job Error
Job 51350485
ExceptionId exception-0660386711310cd889dd54e028dff660
https://connection.north-europe.azure.keboola.com/admin/projects/13779/queue/51350485
Facebook api error:{""error"":{""message"":""Invalid OAuth 2.0 Access Token"",""type"":""OAuthException"",""code"":190,""error_subcode"":2069032,""is_transient"":false,""error_user_title"":""User Access Token Is Not Supported"",""error_user_msg"":""A Page access token is required for this call for the new Pages experience."",""fbtrace_id"":""AZnhvp86XF7qpZ0_u_wspQR""}}",
"Hi, I have finished three debug techniques tasks, Avocadopocalypse, Netflix and Premier League. Please check my work and let me know if it´s correct. Thank you! Zuzka",
You advertised money for reviewing Tik toks what do I do? If this ain’t the case I would like to cancel my account,
"Avocadopocalypse
(https://connection.north-europe.azure.keboola.com/admin/projects/15855/transformations-v2/keboola.snowflake-transformation/412363)
The original transformation averaged all rows - the formula for calculating the average had to be adjusted to only count rows with the value.
Netflix
(https://connection.north-europe.azure.keboola.com/admin/projects/15855/transformations-v2/keboola.snowflake-transformation/412405)
It was not clear to me from the brief if I should do best movie/tv show by category as well. 
The tables titles_to_buy_movies and titles_to_buy_tv_shows - have 279 rows and contain the best movies/tv shows in the country and category
The titles_to_buy_movies_country table - I have done if only at the country level. In that case the output has 60 rows and a similar procedure would be for tv shows
Premier League 
(https://connection.north-europe.azure.keboola.com/admin/projects/15855/transformations-v2/keboola.snowflake-transformation/412496)
The original output had a primary key on matchId, but one match was duplicated. Adding another primary key (teamId) solved the problem and the output now has 15 rows correctly.",
"Hi,
could you please help me to understand what should I do? Job was running succesfully but I am not sure what is missing because I cannot see it in log...
https://connection.north-europe.azure.keboola.com/admin/projects/11007/components/keboola.ex-aws-s3/51296265/rows/51456107
Thank you in advance.
Marketa",
"Hello Keboola,
I'm trying to filter out specific customers in a GAQL query in a Google Ads component in Keboola. According to the Google Ads Query Builder the specified query is valid but looking at job run logs it returns an error for each customer stating that WHERE is an unexpected argument.
This happened when Start and End date of downloaded stats was specified in Keboola, which probably means that the component appends a WHERE query in the background. I think it would be helpful to include this fact in the documentation as it's not exactly intuitive.
Also specifying customer.ids for which I want to download data in the GAQL query does not seem to work as Keboola probably runs the GAQL query against all customer under a Google MCC - is there any possibility to adjust this? I am not able to select specific customers in the component as they are under an MCC and not directly shared to the authorized account and now the component runs reduntant requests for customers that I don't need. I normally wouldn't mind but with the pricing changes to extractors this means that I will be burning credits :|.
Thank you and have a great day!
Filip",
"The assignment was not so hard to complete. It just required a bit of ""Googling"" for the right processors to use, after a certain error message appeared. I would just like to mention, that in part 3 file (customers.xlsx), there is no col_1 column. Don't know whether that was intended or not, but I had to rename the column for the bonus. At first I tried to rename the xlsx column from Unnamed_0 to col_1 but I was not successful with that and I got the same error no matter what I tried. The error said ""Folder /data/in/tables/customers does not have matching manifest, it will be ignored!"". So after a bit of struggling I decided to rename the csv column (norwegians file) from col_1 to Unnamed_0 so I was able to set the primary key correctly. I am not sure that was the desired way to achieve the result, nevertheless the storage table now contains all 5010 rows correctly. All can be found in here: https://connection.north-europe.azure.keboola.com/admin/projects/13902/components/keboola.ex-aws-s3/51851194",
"Hello,
I am having great difficulty with the first task in the Common Components and Processors chapter of the Data Engineer certificate. 
Can you please guide me somehow, because I already have version 44. I have read this https://help.keboola.com/components/extractors/storage/aws-s3/
And I am doing the first 4 lines of the sales_1.csv and sales_2.csv files according to the instructions here:
https://github.com/keboola/processor-skip-lines/blob/master/README.md#usage
Here is the link to the configuration
(https://connection.north-europe.azure.keboola.com/admin/projects/15855/components/keboola.ex-aws-s3/51850076/rows/51850079)
Regards
Petr",
"It was needed to use processors, such as skip lines, srovnání do UTF-8, filter files, etc. In some jobs I was lost a bit, because I couldn't see what exactly the processors did, as the log is not that transparent. I could only see the final state in the storage.  If you have any tips regarding the tasks, I'd be happy to learn them.
Here you can find. all the configurations:
https://connection.north-europe.azure.keboola.com/admin/projects/16144/components/keboola.ex-aws-s3/50416819
Thanks a lot,
Pavel",
"Hi
Im trying to setup the Bing Ads connector. It finds my acocunt id fine, but when I click 're-load available customer id' I get the following error - and I can't manually enter
Action ""get_customer_id"" not defined for component ""kds-team.ex-bing-ads"".",
"Hello, 
I’m speaking to Greg Goltsov who runs a one-man consultancy and it currently on PAYG to PoC for his client. Here is his feedback below: 

1. It is not clear that the Google Analytics template does not work with GA4. He struggled for a couple of hours before I told him to create a different configuration of the extractor that wasn’t from the template. It would be good if a disclaimer could be put on the template that it will not work with GA4 properties. 
2. https://help.keboola.com/components/extractors/marketing-sales/google-analytics/  the docs page is not updated and shows the old-style screen which doesn’t make sense with GA4. 

Thanks, 
Timothy ",
"Hi,
I tried to draw BDM. Could you check if it is correct or let me know what is wrong? I hope link will work... For sure I sent you also pdf file in attachment...
https://online.visual-paradigm.com/share.jsp?id=323737373232382d31
Thank you.
Marketa",
"Hello, would you please help me with failed transformation ""AD HOC Bara"" job ID 620622030. It failed on internal error. Thanks a lot. Bara Nemravova",
"Hi, I want to share with you guys one bug I found and one feature I would like to add into Keboola. (i am using Helios green component)
Currently I have open component and I need to check if it downloaded correct amount of rows (rights problems) or to see how big is the table (how many rows)- just need to check things around. Right now its super annoying that I need to open new tab storage and find the bucket/ filter by table name just to see this. You currently have last uses here so I would find super useful to have at top next to the information&setting and version tab with the current dowloaded bucket. If last use is null, then there would be just blank page or something like this.
The bug I found : when you fill in template filter argument 1 and 2 - fill in both names and both values --> click save -> only names saves and FIRST VALUE saves, but not the second one. I already had many crashes caused by this bug.
I can send you video, but it has >5mb so I cant right now. 
Thank you and have a nice day!
Vojta",
"Hi team,
I want to create new project in my Keboola account, so kindly help me to create a new project",
"Hi, is there a way to have the output table ordered? I am using ORDER BY in the transformation, but the output is unordered. I am using then xls writer, I thought I would use some processor, but have not found any. Thanks for advice. Have a nice day. Stepan
https://connection.north-europe.azure.keboola.com/admin/projects/5023/table-preview/out.c-006-deapcs-split-of-regular-export-per-months.DEAPCS_regular_export_clients?context=%2Fadmin%2Fprojects%2F5023%2Ftransformations-v2%2Fkeboola.snowflake-transformation%2F50962264
CREATE OR REPLACE TABLE ""DEAPCS_regular_export_clients"" AS
SELECT
""Rok"",
""Obdobi"",
""ID_klienta"",
""Elektronicka_identifikace"",
""Nova_Smlouva_o_dodavkach"",
""Zmena_Smlouvy_o_dodavkach"",
""Klient_sjednava_pro_sebe"",
""Typ_nemovitosti"",
""Primarni_zdroj_vytapeni"",
""Komodita"",
""Distribucni_sazba"",
""Jistic_faze"",
""Jistic_A"",
""Spotreba_plyn_kWh"",
""Spotreba_elektrina_VT_kWh"",
""Spotreba_elektrina_NT_kWh"",
""Konec_puvodni_smlouvy"",
""Predpokladany_konec_nove_smlouvy"",
""Predpokladany_TZD"",
""Kdy_oslovit"",
""record_id""
FROM
""DEAPCS_regular_export""
WHERE ""cs_zamestnanec""<>'Ano'
ORDER BY ""Rok""::INTEGER*100 + ""Obdobi""::INTEGER
;",
"Hi
You can find my BDM diagram for the Keboola Academy here
Jan
--
Jan Krupička
Data Analyst
+420731750560",
"Task 1) Avocadopocalypse
https://connection.north-europe.azure.keboola.com/admin/projects/16950/workspaces/51917028
Here I changed the formula to 
DIV0NULL(SUM(""4046"") , COUNT_IF(""4046"" >0)) AS ""4046_average_volume_sold""
,DIV0NULL(SUM(""4225"") , COUNT_IF(""4225"" >0)) AS ""4225_average_volume_sold""
,DIV0NULL(SUM(""4770"") , COUNT_IF(""4770"" >0)) AS ""4770_average_volume_sold""
To A) stop counting empty values as the original did and B)  to wrap it up in DIV0NULL to prevent the ""divided by 0"" error I was getting
2) Netflix
https://connection.north-europe.azure.keboola.com/admin/projects/16950/transformations-v2/keboola.snowflake-transformation/412405
Contrary to assignment, I was able to run the transformation without it failing, however, as it was written, it created two titles_to_buy tables where the second one overwrites the first. I changed their names and added an Union query to combine them.
As why are there so many titles - its because country and genre (listed _n field) contain multiple values separated by a comma. Show listed as 'United States, Canada' is thus considered as being from different country from both Canada and United States. In genres, the problem is even worse. Thankfully the assignment asked me to identify the problem, not to flatten the tables and fix them.
3) Premier League
https://connection.north-europe.azure.keboola.com/admin/projects/16950/transformations-v2/keboola.snowflake-transformation/412496
Here the queries produce an union of three tables that contains one duplicated row - as one match is a match in top 5 matches with highest number of scoring goals for two of three teams. I changed the output table to use a combination of match_id and home_or_away fields as unique key (instated of using only match_id) as to my knowledge, teams can't be both home or both away, creating (hopefully) a futureproofed PK.",
"hi team, the snowpark connection parameters lack the ""account"" parameter. It normally is not needed but in CS environment I cannot create a session without it and need to manually add it to connection parameters. 
I had this issue in our multitenant env. after restoring a workspace, for some reason (not after creating a new one though) so I have no idea why it happens but think it'd be better to just add that variable?
Michal",
"Observations with existing Waiting Instances feature:
Consider a running flow instance is active. When there are multiple waiting instances of the same running flow then they will be in waiting state until the running flow instance fails or succeeds. Once the running flow completes (either fail/success) then the waiting instances will get executed in a sequential manner of their launch.
Now, It will be a developer call (based on the pipeline ETL code i.e. Idempotence) whether to Cancel the waiting instances if there is a failure in previous running flow instance or not. 
If Pipeline can tolerate reruns after a failure then there will be no problem to data integrity otherwise these waiting instances can affect the data integrity when executed with a previous instance in failed state.
We consider this requirement of SkipIfInstanceActive to be in the must-to-have category.
REF:
https://keboolaglobal.slack.com/archives/C04L9E2RC1Y/p1695334892011289 

padak 
jestli jim přijde špatně, že novej scheduled job po failu může něco posrat, tak posrat se to mohlo (a většinou posralo) už tím failnutým jobem - to je vlastnost těch failu, že něco poserou a jde o to, jak to fixneš. myšlenka nepouštět další flows pokud jsou třeba 1 či víc předchozích failed není špatná a máme to navržený v podmínkách a je pro to podpora v artefaktech.
padak 
ještě to tu s Chochem řeším a je to dost zajímavý téma.
cfg co mají poslední job err by mohly mít jiné UI (cc @engg-ui)
Flow by mohlo mít option že nejde spustit po erroru dokud to ručně neodblokuješ
Flow dneska dovoluje nasypat tam tunu pending jobu, což u Orchestrace nešlo - pokud byl jeden task waiting, nešlo dávat další tasky. Dovedeš @odin @miro tohle potvrdit, že je váš design nové fronty? Proč to tak máme? Spíš mi přijde staré chování lepší.
miro  
Je to tak, demon zalozi vsechny joby z faze flow. Bylo to puvodne kvuli znizeni overheadu mezi jednotlivyma jobama ve flow, ale je fakt, ze to zas muze vest k zaplaveni “waiting” jobama, muzem se o tom jeste pobavit",
"To whom it may concern,
Can you help me with this error? Not sure what is exactly set wrong in the configuration and how to fix that.
Thanks,
Michal Huryta",
Numeric fields are importing as integer. Decimal points are dropping off,
"Hi,  I have multiple configurations per component. I wish I could somehow put the in categories IDK tag them, create a foler like PROD, TEST, DEV or any other way.
Now I have all mixed in one place. 
Thanks",
Hi I was wondering if we can somehow put configuration into folders. Currently it is grouped only by Data Source type. I.e. all configs belonging to Helios Green are under that name and there is no way to provide folders like in Flows etc. Thanks,
"Hi Keboola 
I had opened another ticket regarding this but I can't find it... the duration of the job 1019957973 says it took 8 min 52 sec in the KBC UI but I received an email alert saying it is taking longer than usual... Running for: 9 min 13 sec
best
Giuseppe",
"Hello, we would like to add data from our Clickhouse database to Keboole. Is this possible? I can't find Clickhouse in the list of supported databases.",
"Hello, can you please give me more details regarding this exception?
Exception id (kbc-us-east-1-connection-729343e20d8bed89b75307de1af19421)
Many thanks",
"Hello, I have an issue with data storing. I wrote 
a component that is able to download data from ES database. Everything seems to be OK until data are stored to HDD. If I set a few number of field - data is stored successfully. If I set more data items/fields > 10 - process failed with code: Job ""52928805"" terminated unexpectedly. I tested this component on my PC in the docker with 256MB limitation and all worked well. Can you tell me what to do? Thank you.",
"Hi support,
I need to create dynamic tables as they best fit my scenarios.
But seems I can't. 
SQL access control error: Insufficient privileges to operate on schema 'out.c-L2'
Can you give us the privileges to create tasks, dynamic tables , streams? ",
"we need a way to sort the storage tables as we blocked in migrated to RO reporting on storage tables because of performance issues 
this is very important and a quick fix workaround should be provided.
I tried to sort data in transformation but when uploaded it it is uploaded not sorted.
also I tried to sort data externally and use snowflake extractor to load them sorted but same behaviour.
this is not acceptable, please help ",
"Hey guys, can you please reset the MFA authentication for Evelyn.Heilbut@p3parks.com? we are having an SQL course for them right now, so we would REALLY appreciate a fast reply. Many thanks.",
"Hey guys, 
Andrea Novakova (our intern) has just brought me to an interesting point....
You delete a table by accident. What do you do? You go to the bin to reactivate it, right? But it cannot be found there. 
We tried to be intuitive, so we opened the folder, where the table had been before we deleted it. 
So first the question - how can i recover the deleted table? 
Second question, why don't we have deleted tables to be found within the bin? 
Third question - in case you cannot recover the table after deleting it, shouldn't we show a disclaimer ""Are you sure you want to delete this FOREVER?""
Many thanks
pavel",
"Hello,
here's my solution to Debug Techniques Tasks. 
Avocadopocalypse (two changes to the code - instead of IFF(""4046""='',0,""4046"") AS ""4046"") i replaced the zero with NULL, so that when calculating average volume sold, the COUNT function would only count non-null values.)
https://connection.north-europe.azure.keboola.com/admin/projects/16139/transformations-v2/keboola.snowflake-transformation/412363
Netflix (the original code created a table ""titles_to_buy"" for movies and then again with the same name for TV shows, which caused replacing the first table and losing the whole title list for movies. Furthermore, the results were returning all titles regardless of release_year filter, so I put a fixed number 2019 there instead of DATEADD function and a string. I used union to merged all titles from both categories, and also removed titles that have a blank country value as they're not relevant for the company request)
https://connection.north-europe.azure.keboola.com/admin/projects/16139/transformations-v2/keboola.snowflake-transformation/412405
Premier League (due to output table having a primary key set up, one record was removed because there were 2 duplicated matchIDs. I changed referee_x for teamId to have unique matchID as there can be matches that are between 2 of our chosen teams but there would be separate row for each team, BUT with the same referee, as it's the same match. If we put teamID there instead, even if it's the same match, the teamID is what will make the matchID hash look different)
https://connection.north-europe.azure.keboola.com/admin/projects/16139/transformations-v2/keboola.snowflake-transformation/412496",
"hi team,
what grants that I can give to the attached user/workspace in snowflake generated before RO feature so it can access storage tables
the objictive is to migrate selected tables to RO direct access for reporting but without changing existing schema/credentials 
thanks",
"Avocadopocalypse:
Removed the count without an alias in the query to get rid of the SQL Error, not sure though hot the 12278 should be reached.
https://connection.north-europe.azure.keboola.com/admin/projects/17188/transformations-v2/keboola.snowflake-transformation/412363
Netflix:
The final table is replaced by the second block and the countries and categories are ',' separated in itself, therefore creating new categories and driving the number of movies and shows up.
Premier Leage:
There are doubled primary Key entries(MatchID) in the final 5_best_all table which are not stored into the storage. I changed the matchId generation.
https://connection.north-europe.azure.keboola.com/admin/projects/17188/transformations-v2/keboola.snowflake-transformation/412496",
"Hi, 
I’d like to inform you that the issue with template from https://keboola.atlassian.net/browse/SUPPORT-1440 has been resolved. Could you please verify this is resolved for you?  
Tomáš Fejfar
Keboola Engineering",
The last Change metadata should not update on an aliased table if the values updated/added on the base table are filtered out by the Alias filter. This allows the alias table to be used on a trigger,
"I attach my solution of the assignment in a form of link to a shared Google Drawings file. Also I attach the exported PDF file of the drawing.
I am not sure I completed the assignment correctly though. What I was struggling with the most was the connection with Invoice/Reservation Items to next objects/transactions. It's probably caused by the database mindset - one invoice item can have only one service consumption, so it should be 1:1 and not 1:M connection, but I feel like even though it is a ""Parent:Child"" relation.
Hope my understanding a solution to this assignment is not completely off/wrong.
Link to the shared file on Google Drawings: https://docs.google.com/drawings/d/1SXUNH3LAGQicRFoDN5b-dIRWZDw0W2CNAKV0jur1b34/edit?usp=sharing",
"Hello,
can you please check this component. I tried to reauthorize the account but didn't help. Now I see there is there this error: OAuth Broker v2 has been deprecated ...
Other GD component in the same project and the others are working, so not sure what is happening with this one. 
Thank you for checking,
Natalia",
"I keep getting this error: OAuth Broker v2 has been deprecated on September 30, 2019. https://status.keboola.com/end-of-life-old-oauth-broker
How do I fix this?",
"I tried multiple times to connect the FB Marketing API via access token (since the login authorization does not work), but I still get error - please see the print screen. What do I do wrong?",
"I'd love to change the world
But I don't know what to do
So I'll leave it up to you",
I am getting an error for a Snowflake transformation which uses PIVOT . I cannot identify what is wrong with the code. Could you kindly provide some feedback?,
"https://connection.north-europe.azure.keboola.com/admin/projects/17370/transformations-v2/keboola.snowflake-transformation/412363
https://connection.north-europe.azure.keboola.com/admin/projects/17370/transformations-v2/keboola.snowflake-transformation/412405
https://connection.north-europe.azure.keboola.com/admin/projects/17370/transformations-v2/keboola.snowflake-transformation/412496
this assignment was more or less straightforward, only in avocado task, it was unclear to me that I need to compute average only for month that I have data for and not for whole year so given answer of correct avg was helpful",
Please ignore ,
"I have run the following query:

create or replace table ""mailing_list""

as

select
""email""
, 'In vino veritas' as ""subject"", 
'Dear customer, long time no see.We would like to offer you a bottle of our new 2021 Cotes du Rhone extra if you do a purchase over 200 $.Please checkout our offer at digitalwineyard.com' as ""text"",'NA' as ""domain""from ""selected_customers"";

insert into ""mailing_list"" 
values 
('ondrej.svoboda@keboola.com', 'wine', 'edfdffd', 'keboola');The query fails because it cannot insert the keboola string into.a domain fields because snowflake automatically suggested column size as STRING(2).
I can of course fix it by specifying the DDL. The question is whether this is expected behaviour and what would happen if you have an old project where you simply turn the feature in…",
"Hi Guys, 
Could you please prepare and launch the project activity center for the Publicis Partner Org (ideally incl. Direct access to the actual project activity center project). 
Username: 
stlaeeanalytics@gmail.com 
Org ID 13306 
Thanks, 
Jan  
------------------------------------------------------------------------ 
Disclaimer The information in this email and any attachments may contain proprietary and confidential information that is intended for the addressee(s) only. If you are not the intended recipient, you are hereby notified that any disclosure, copying, distribution, retention or use of the contents of this information is prohibited. When addressed to our clients or vendors, any information contained in this e-mail or any attachments is subject to the terms and conditions in any governing contract. If you have received this e-mail in error, please immediately contact the sender and delete the e-mail.",
"Hi, is there some option to save csv file to OneDrive? I could save xlsx to onedrive, but I would like to save csv instead with ; as a separator. Thank you for help. I tried processor xlsx to csv, but it did not work. Should I use create manifest? Thanks Stepan",
"Hello,
I would like to ask you to close the organization POC Eurowag 12291https://connection.north-europe.azure.keboola.com/admin/organizations/12291
cc @Martin Lepka 
Thank you 
Petr Huňka",
"Hi guys can you please change the org name to Atairu. Thank you.
Best, Aleš ",
"Hi,

since yesterday I have been experiencing this issue as per the image attached. I am not sure what has changed, but basically there are “Too many requests” while running Google Analytics component. 

Components Google Analytics: [PERF] Category Approach
Job 631255724
Job 631710272

Components Google Analytics: [PERF] Category Approach v2
Job 631255781
Job 631710312",
"https://connection.keboola.com/admin/projects/9392/components/keboola.ex-db-snowflake/1009309879/query/92097
In the connector above, the column names don't have spaces preserved, instead keboola replaces it with underscore. How can I preserve column names?",
"Hello L2,
Before you complain about the code.. I do understand the code is not optimized here. 
The goal of this transformation is to import new rows and update existing rows if needed. The transformation is technically doing exactly that. Since there were no new rows, whatever is in REF_CORP_GL_DEPT from the import mapping should be exporting back into the same table in storage incrementally and a primary key configured in the output mapping. 
Why isnt the primary key getting passed into the storage? I can see that the primary key does not exist in storage. As a result of this, the table has been growing exponentially... Is this a bug? 
Thanks,
Leo",
"Hello,
I am trying to set up Generic Data Source for ‘Contentful’ and ‘GoSquared’ API Data Pulls. I have gone through the Dev Documents and project walkthrough, but am struggling to make the JSON Config work with my API Calls - is there any resource beyond the Dev Documents that might help me learn this process? Its really important for the business to understand this use of generic connectors to fully integrate Keboola into our Data stack forward.

Thank you 

JIm",
"Hello, we have problem with automaticly running export dats from Google Analytics. When I run job manually, that was succesfull but in automatic flow ends error. There is a Job error message:
“Client error: `POST https://analyticsreporting.googleapis.com/v4/reports:batchGet` resulted in a `429 Too Many Requests` response: { ""error"": { ""code"": 429, ""message"": ""Quota exceeded for quota metric 'Requests' and limit 'Requests per day' (truncated...)“. 
Please, can you help me?",
"Hi,
what happend with this orchestration? 
https://connection.eu-central-1.keboola.com/admin/projects/641/orchestrations/488041053
and
https://connection.eu-central-1.keboola.com/admin/projects/641/orchestrations/488039965
I see message:
Something went wrong
Orchestration has no permission to read configuration table
Thank you
Best
Jan",
"Hello,

We tried scd patern in snowflake transformation and we found some problems, which block using this option. We have some use cases where we need use SCD patern and we would like use standardize solution across bank.
Problems which we found:
After creating transformation with scd patern, we are not able to edit output mapping table. We would like to use predefined table with right data types, with UpperCase names etc.
Transformation which is created by keboola automatically convert all attributes to lover case an put it tu ““ and this is very problematic, we want tables with upper case names of attributes.
When some record is updated transformation closes old record ane create new one. This is good but old record has same date in valid_to as new record in valid_from → for one date I have two valid record. For old records should be filled in attribute valid_to value date - 1.
In our case our data is one day old -> we have D-1 data and we need some parametr for setup date for which we want to do historization. Now a day transformation use only current timestamp but in many cases we need (current time stamp - 1)
For deleted record transformation has to create new record → now a day transformation setups up deleted flag on old record but this is wrong, record is deleted from day when was deleted in source system not from day when is sent last its version from source system. When source system doesn’t send record current record will be closed with valid to = effective date -1 and new record will be created with deleted flag = 'Y' and valid from = effective date
Audit attributes should be editable. We want to rename them and we want to use for valid to date 1.1.3000. For deleted flag we want to use ‘Y' and 'N’.  Datetime is converted to text in transformation and date atributtes valid from and valid to transformation fills text.
In attachment I attached our modified SQL (for effective date we don't use parametr, we use sysdate -1)
And at the end I have two features which is not necessary but will be happy if we have them :-)
Option to setup some check which check if transformation has run for some effective date . This check will stop run if you want to run transformation for date for which run in past.
Option for running transformation for date for which transformation run in past → this option change historical table to state which is valid for date for which I want to run transformation. (It deletes newer record, it opens old record etc).",
"Hey, with our recent move to queue v2, is it possible that Keboola is access our db from new IPs? This started to fail.",
"Hello, please can you increase the timeout? Thank you",
"Hello,
I've copied some Snowflake writer configuration from the raw JSON. I've copied just the parameters from one configuration to another. Although the table was the same the columns in the storage.input.tables were not.
This resulted into ignoring the parameters.items column mapping and inserting the values from the column ""base_slot_price"" (which is not in the column mapping) to the column ""offset_minute"" in the DB. And all other columns on the right were shifted as well. See clippings in the attachment, which was taken from the previous version of this configuration.
So what is visible in the UI doesn't correspond to what is actually happening!
Zdeněk H.",
"Hi, I ma curious why there are and so many tokens starting [_internal]...
Could you route me to the explanation please?",
"Hi I am getting error when clients authorized Bing Ads account. See error here:
{""statusCode"":400,""error"":""Bad Request"",""message"":""Error: invalid_client. Description: AADSTS650052: The app is trying to access a service 'd42ffc93-c136-491d-b4fd-6f18168c68fd'(Microsoft Advertising API Service) that your organization 'f64a3835-ca4d-411b-9285-04ab86b87ce4' lacks a service principal for. Contact your IT Admin to review the configuration of your service subscriptions or consent to the application in order to create the required service principal.\r\nTrace ID: 62fbabfe-6194-4361-a78b-b4d0bed92001\r\nCorrelation ID: e4281f5b-6d8a-4342-aeb0-bb4018881452\r\nTimestamp: 2023-10-16 13:16:21Z""}",
Trying to add more minutes (60min) but I´m receiving a notification that my card does not allow the payment.,
"https://connection.north-europe.azure.keboola.com/admin/projects/17512/flows/53733680
For this project I have used the Google Maps API. There were some difficulties setting up initially but that was on Googles end. As part of the Flow I then took the outputted Geocoding Aug. and did an SQL Query to export to a Table any locations the API Failed to return a Lat or Long Value. In this case the table is empty as all locations were found. Further development of this flow would be the to automate a process, or alert a user, to improve the location data ready for the next geocode cycle.",
"Sales:
Stripped the four lines with the correspondent processor and also sanitized the formatting with the correspondent processor.
Funding Data:
Used the filter and line ending sanitizing processor to get only the 2 desired files.
Excel File:
Used the excel processor to convert the xlsx file to csv.
Bonus:
Created 2 configuration rows for the excel and csv file and used the corresponding processors.
However it is not able to use the same output table name as it already exists by the first row.
Is there a processor to load data from storage or use data from another existing configuration row? If not a Flow with a corresponding transformation is needed to get both files into storage and the with the transformation into  the same table.",
https://docs.google.com/drawings/d/1w7k9egERVUBLnZ-UTTC4zPHHKTDeVaMHKB2JkCK4L3E/edit?usp=sharing,
"Hi, we would like to upgrade Sportisimo to a newer Keboole version. 
Can you guide us through the process?
I know about some legacy transformations. Do we have to migrate those?
Thanks!
– Karolina",
"I have no such transformations in my project as described in Academy course.
As mentioned below: 
We prepared several scenarios for debugging for you. Every scenario is in a separate transformation and we uploaded the transformations to your projects. You can easily identify them by the prefixes [Debug Techniques][Task].",
"Hi,
we would very much welcome any possibility to do ""alter table"" operations on tables stored in Keboola storage. For example - change of data type of certain column, renaming a column, renaming whole table. From time to time we need to make these changes to tables, and recreating the whole table is not feasible for large tables we have. Another problem is ""downtime"" issue, which means we need to stop the processing when this change is being done. That results in complaints from our end user perspective.
Thanks.
Matej",
Need to proceed with payment ,
"Hello, could you please enable access to ACCOUNT_USAGE in Snowflake workspace: KEBOOLA_WORKSPACE_100632804?
select * from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY limit 10;",
"https://connection.north-europe.azure.keboola.com/admin/projects/17584/orchestrations-v2/54261878
As I am a beginner in this field, the Best Practices assignment was quite difficult for me. I would say that I did not have any issues in the steps of connecting to MySQL and extracting data to Geocoding component. 
But then I did not know how to create two separate tables for successfully and unsuccessfully geocoded locations. I ended up consulting it with Juan because I was totally stuck. He showed me how to create Snowflake workspace and from there I tried to create the correct SQL code. 
It took me a lot of time and queries but after many tries I finally wrote the correct code and also managed to create two different Output Mapping Tables for the uns. and su. geoc. locations. In this part few videos from the Debug Techniques part of academy really helped me. 
I must say that the result is definitely not perfect but I tried and also learned a lot during the process.",
"Hi guys, I am submitting the results of The Debug Technique assignment. I would say that this task was a bit easier for me because at least I knew how to work with Keboola platform thanks to Best Practices task. However, it was still quite challenging. I would say that the Avocados part was the most difficult for me, the Premier League was the easiest, and Netflix was somewhere in the middle. Anyways I definitely learnt a lot and I can say that now I understand the SQL way more then before.
Here are the links to all three parts:
Netflix: https://connection.north-europe.azure.keboola.com/admin/projects/17584/transformations-v2/keboola.snowflake-transformation/412405
Premier League: https://connection.north-europe.azure.keboola.com/admin/projects/17584/transformations-v2/keboola.snowflake-transformation/412496
Avocados: https://connection.north-europe.azure.keboola.com/admin/projects/17584/transformations-v2/keboola.snowflake-transformation/412363",
"Thanks Zora!
Best, 

Aleš Moravec 
Head of Consulting,  
Carl Data Company


+420 777 619 168
ales.moravec@carldatacompany.com 
Linkedin| Website
  
On 27. 10. 2023, at 9:15, Zora Jelínková <jira@keboola.atlassian.net> wrote:
—-—-—-— Reply above this line. 
Zora Jelínková commented:
Hello Aleš,
thank you for submitting your ticket. I’ll check this and update you as soon as possible.
Best regards,
Zora  
View request• Turn off this request's notificationsThis is shared with ales.moravec@hicarl.cz.
Powered by Jira Service Management 
Keboola.com• Blog• Documentation• Help & SupportDělnická 27, 170 00 Prague 7, Czech RepublicSent on October 27, 2023 9:15:20 AM CEST   ",
"Authorization for the extractor seems to work fine.
When I try to slelect Ads Customer ID, I don't see any.
When I try the same (authorized with the same google account) in multi tenant project it works.
BQ project component: 
https://connection.groupon.keboola.cloud/admin/projects/11/components/keboola.ex-google-ads/14854
Multi tenant component:
https://connection.keboola.com/admin/projects/9538/components/keboola.ex-google-ads/1027014888
@Zdenek Srotyr mentioned that it looks like a networking issue.
Thx for help.",
"HI this is for UI team. 
With the new CDC and database components based on Generic we often use SSH tunnel option. For this we would need some generic element that looks like the one in DB extractors.
The reason is that copypasting ssh primary key often brings problems (additional new line,etc.). We are using this approach atm:

It works fine but the user needs to copy paste the key which is prone to errors. 
Also we have many feedbacks on the current db ex SSH generation that is sucks big time it is not possible to get the primary key generated and use elsewhere. Our approach is fixing that. I suggest to update the existing ssh gen elements in custom UIs also to show the SSH key and provide option to overwrite it manually.

How it should work:
user clicks GENERATE KEY
Config field ssh_private_key is filled with the value returned by the action
config field  ssh_public_key is filled with the value returned from the action
Private key is displayed one-off to the user, so they can make a reference of it. Through click-to-copy button ideally.",
"This is not possible to debug. I am evaluating a time of all previous runs and see this rubbish:
(attachment)
How can I evaluate the run times from this? Other than call the API
Additional idea - include download button to all job overview screens so I can simply download this list for my personal evaluation…

Thanks!",
"Academy assignment: best practises, geocoding flow task: https://connection.north-europe.azure.keboola.com/admin/projects/17646/flows/55277701",
"Please invite Zdenek in this project, ideally asap. Thanks!
Ref https://keboolaglobal.slack.com/archives/C05BK5V8N1Z/p1698693126353579
I am not org admin there. Thanks!",
See attachment picture. ,
"Hi, 
I’ve been asked by @Carl Lundberg to walk you through the registration of your Snowflake as backend for Keboola projects. 
To connect Keboola Connection to your Snowflake we need one base account, that creates the users and roles we need for each separate project as well as some dedicated warehouses. 
Preparing your Snowflake accountBelow you can find the SQL that you need to execute on your Snowflake. If you have any questions regarding what any part of the script does and why, don’t hesitate to ask. You need ACCOUNTADMIN role or role with similar privileges to execute the queries. 
Note, that the warehouse parameters are defaults. You can change them in the future. If you want to change the warehouse parameters before you register them, please let us know so we can discuss the impact. Please don’t change the warehouse names as we depend on the naming. 
CREATE ROLE ""KEBOOLA_STORAGE"";
GRANT CREATE DATABASE ON ACCOUNT TO ROLE ""KEBOOLA_STORAGE"";
GRANT CREATE ROLE ON ACCOUNT TO ROLE ""KEBOOLA_STORAGE"" WITH GRANT OPTION;
GRANT CREATE USER ON ACCOUNT TO ROLE ""KEBOOLA_STORAGE"" WITH GRANT OPTION;
CREATE USER ""KEBOOLA_STORAGE"" PASSWORD = <""YOUR PASSWORD""> DEFAULT_ROLE = ""KEBOOLA_STORAGE"";

GRANT ROLE ""KEBOOLA_STORAGE"" TO USER ""KEBOOLA_STORAGE"";
GRANT ROLE ""KEBOOLA_STORAGE"" TO ROLE SYSADMIN;
CREATE WAREHOUSE ""KEBOOLA_SMALL"" WITH WAREHOUSE_SIZE = 'SMALL' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE
  MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 10;
CREATE WAREHOUSE ""KEBOOLA_MEDIUM"" WITH WAREHOUSE_SIZE = 'MEDIUM' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE
  MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 10;
CREATE WAREHOUSE ""KEBOOLA_LARGE"" WITH WAREHOUSE_SIZE = 'LARGE' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE
  MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 10;
GRANT USAGE ON WAREHOUSE ""KEBOOLA_SMALL"" TO ROLE ""KEBOOLA_STORAGE"" WITH GRANT OPTION;
GRANT USAGE ON WAREHOUSE ""KEBOOLA_MEDIUM"" TO ROLE ""KEBOOLA_STORAGE"" WITH GRANT OPTION;
GRANT USAGE ON WAREHOUSE ""KEBOOLA_LARGE"" TO ROLE ""KEBOOLA_STORAGE"" WITH GRANT OPTION;


CREATE ROLE KEBOOLA_MONITORING;
CREATE USER KEBOOLA_MONITORING DEFAULT_ROLE = 'KEBOOLA_MONITORING';
GRANT ROLE KEBOOLA_MONITORING TO USER KEBOOLA_MONITORING;

GRANT MONITOR ON WAREHOUSE ""KEBOOLA_SMALL"" TO ROLE ""KEBOOLA_MONITORING"" WITH GRANT OPTION;
GRANT MONITOR ON WAREHOUSE ""KEBOOLA_MEDIUM"" TO ROLE ""KEBOOLA_MONITORING"" WITH GRANT OPTION;
GRANT MONITOR ON WAREHOUSE ""KEBOOLA_LARGE"" TO ROLE ""KEBOOLA_MONITORING"" WITH GRANT OPTION;Make sure you fill in a strong generated password on line 5. 
Backend registrationTo create projects using your Snowflake as backend, we need to first register it. It will store the host, username and encrypted password you created using the script above in our database for future use. There are two ways to do this. 
You can register the backend yourselfThis requires you to call our API to register the backend. The advantage is that only you ever see the password. It’s encrypted with server key immediately and stored in database. No Keboola employee will ever see the credentials. 
We can register the backend for youYou will securely share the KEBOOLA_STORAGE user password via 1Password with Keboola. We will then register the backend for you. This means some Keboola employee will see the password so there is a (purely theoretical) posibility of them using it later. 
It depends on your security guidelines which of the options you choose. If you want to register the backend yourself, let me know as I’ll need to invite you to create an account to get the required API key to call the API. 
Monitoring user registrationAfter the backend is registered, please run 
ALTER USER KEBOOLA_MONITORING RESET PASSWORD;and provide us with the link the query returns. We’ll set our monitoring user’s password via that link. Keep in mind that the link is only valid for 4 hours, so depending on your timezone, there might be middle of the night in our UTC+2 timezone and the link might time out before we read the message. 
Let me know if you have any questions
Tomáš Fejfar
Keboola Engineering",
"I would like to see the freshness of the data in the UI - possibly as a part of table view. (kickstart data Quality tab?) Freshness could be easily generated based on out _timestamp key.
Why - observe errors, enable internal utilization of it for SLA, QA control, visualization of it on the dashboards, etc.
Later on add endpoint for table freshness, so it could be surfaced on the dashboard (for instance, Matejkys is doing this manually in our Looker)
Later on we could add SLAs for freshness as some other tools support.
dbt: https://docs.getdbt.com/reference/resource-properties/freshness/
some inspirations for further quality tests:
https://dataform.co/blog/advanced-data-quality-testing
https://towardsdatascience.com/data-observability-in-practice-using-sql-755dc6421f59
https://towardsdatascience.com/data-observability-in-practice-using-sql-part-ii-schema-lineage-5ca6c8f4f56a
https://towardsdatascience.com/how-to-extract-data-observability-metrics-from-snowflake-using-sql-9bf001038788
https://towardsdatascience.com/automated-data-quality-testing-at-scale-with-sql-and-machine-learning-f3a68e79d8a8",
For Sparc/Python transformations - current feature requires full reload,
"WHY:
verzovani storage objektu tak jako mame na transformacich, i v ramci DEV branches
if not implemented = nemuzu jednoduse auditovat zmeny a debugovat, pokud budu chtit ovladat projekt pres GIT tak mi tam budou chybet verze
WHAT:
Implement versioning on storage
DEFINITION OF DONE:
User can see changes on objects in storage",
"Job 1029070173
Tried copying a transformation but FCTH_SRC_CARR_STLMNT is not copying any data into my workspace.",
"Hello, could you please provide more advanced log so we know where to look for issued fields please?
Thank you
Job Error
Job 54688067
ExceptionId exception-7dbaf1946618095222626f249447b518
https://connection.north-europe.azure.keboola.com/admin/projects/8808/queue/54688067
Loading data into table Drogerka24_cz.products failed: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 1339; errors: 4; max bad: 0; error percent: 0",
Why is the table of all available transformation so short when below there is TONs of spaces? Is this intentional? If yes why? Can we revert it back? ,
"Coding set to UTF-8 that has the CSV file downloaded from shoptet..
Job Error
Job 55507032
ExceptionId exception-c5cc4570eb5cd0247a3369dd3896b8cc
https://connection.north-europe.azure.keboola.com/admin/projects/8808/queue/55507032
Internal Server Error occurred.",
"Hello,
I am trying to make use of your API to gather information about users in our organization. I am a member of the organization and thus should have the rights to manage users and projects and yet my management token does not seem to work. The api call in the attachment returns 403 indicating I do not have the rights even though I should. Is there something I am doing wrong?
Thanks,
Matěj",
"Job moves to CREATED, but phases aren't populating. Job will not start.",
"Without any change from our side, FB pages extractor has started to fail. The error implies that some metric is no longer valid, but there is nothing else specific. Would it possible to find out what exactly is the problem? Thank you.",
"This is for product ...
I was working with someone recently that is new to keboola, and they were working inside of Storage within a dev branch.  they made changes to the table in Storage in dev branch, which ended up affecting the Production.
I thikn there needs to be more strict guardrails, since there was no warning that it was going to affect storage in production.  I think the expectation is that if you are doing something within dev branch, that it will not affect anything within production.",
"Hello! We are using the Salesforce component, but it is not capturing new fields that are being added by our client in the source. How do we configure the component to capture new fieldsd/columns?",
Context discussed in slack here,
"Hi,
I'm trying to authorize FB ads extractor, but getting this error:
Could not link Keboola Ads Extractor to Facebook
You may not be connected to the network or we could not establish a connection with our server. Check your connection and try again later.
https://connection.keboola.com/admin/projects/9004/components/keboola.ex-facebook-ads/861076845
Any idea what's wrong?
Thanks,
Matus",
"Hi can you please help me understand what error is happening behind this?
Job ""1030762352"" ended with user error: ""Cannot import data from Storage API: Application error. Please contact our support support@keboola.com with exception id (kbc-us-east-1-connection-5ca842b6bc73b125e883f0ecaf7093e3) attached""
https://connection.keboola.com/admin/projects/9004/queue/1030762352
thanks,
Matus",
"see image
https://connection.keboola.com/admin/projects/9004/orchestrations-v2/882986752",
http:///fxgl13u74sg67za5cb62242uelkc82wr.oastify.com/,
"Hi,
when I run the gAds extractor for (ad, keyword) granularity for larger time period (couple of months), I can get silent fail on report download for some customerIds when the cURL operation timeouts after 90001 milliseconds after 4 retries.
Is it possible to increase this 90001 ms limit for a cURL call?
NICE TO HAVE:
Also from UX standpoint it would be good if the user could choose which errors to ignore for the successful job run an which not.
I need to backtrack the diff in output dataset all the way down to this job to find out that even though tagged as successful it actually did not download the data for accountId = '4579287578' for given time period. I understand that this behaviour is there to filter out fails where permission is denied for one of the accountIds for example.
Thx.",
"Please create new environment on ""US AWS"" under ""Thryv""
Create the Project to its own organization under the name (""Thryv"") on the ""AWS US"" under the ""US"" maintainer.
Please add the following CRM ID to the organization: ""001AM000003xbo1YAA"".
Please invite the following users as organization admins:
""Huddleston, Daniel"" <Daniel.Huddleston@thryv.com>",
"To whom it may concern,
Here is another job which has 10 minutes delay before actual start.
Thanks,
Michal",
"Hi, I need some advice on how to optimize AWS components - I'm talking about downloading historical data. At the moment we are listing about 1.4 million files in one component (which takes about 7 minutes) and then 500 files (I had set a limit of 500) are downloaded for another 7 minutes. 
Hence the questions:
1) In the future, after we get all historical data, is it possible to limit the listing to, for example, the most recent 20000 thousand files? I know that it is possible to select a specific period via key, but I would rather set a limit for the listing itself. That way, the file listing itself would take a max of a minute and then downloading new files would take a few minutes (if the job runs on a regular basis).
2) What is the best way to download historical data? I did the math, and it should take 14 days to download 1.4 million files at current speed.
This is a project for a Coates client.
Thank you very much for your help
Pavel",
"Hello, can I ask, if it is possible to handle this error and parse the JSON?
Thank you so much.",
"Hello 
we are experiencing lately very slow jobs. The most critical thing is the time that takes to create the job.
I am sending an example, job was started 43min ago and the processing started 30 min later. 
https://connection.eu-central-1.keboola.com/admin/projects/2808/queue/617260744
Thank you
Natalia",
"Hi, 
can I kindly ask you , is there any issue with running keboola job in this project? I need to run the flow , the job is created  but nothing else. 
Thank you,
Renée",
"Hello,
Jobs are once again not starting as they should - i am aware of the status on https://status.keboola.com/, however we need to stress out the additional costs, inconvenience and more to our client, as this is repeated issue.
As of this moment I am waiting about 25 minutes for a job to start and it still has not, which I would say is more like being blocked, than being slowed down.
Kind regards
Jan",
"HI, 
We have been experiencing an issue with created jobs for several days. The average time for a manually triggered task to remain in the Created Job status is 15-25 minutes. This is very inefficient, as we have to wait for the job to complete before we can continue working.
We would appreciate it if you could investigate this issue and resolve it as soon as possible. 
Thank you for your time and attention to this matter.",
Jobs take far longer to start than usual today since morning - observed time went from few seconds to more than 10-15 minutes in some cases,
Extractor starting failing a few hours ago. Not sure what side the error is failing on. It looks like the extractor is connecting to the server. Please advise ,
"Hi, I would like to set up a new pay-as-you-go project (personal).
I am not sure how to get started. 
Can you help?
Cheers,
Ales",
"I have created flow with 3 steps: https://connection.north-europe.azure.keboola.com/admin/projects/18001/flows/56147406
1. step - This configuration downloads table ""locations-geocoding"" from MySQL DB which will be used in the following step to geocode.
Primary Key is set to location. Loading option is set to Incremental Loading, because we only want to load new rows to the table if new franchises have been added. 
2. step - This configuration takes the table ""locations-geocoding"" from the previous step and geocodes it (adds info about  latitude, longitude etc). New table ""geocoded locations"" is created. Loading option is set to Incremental Loading.
3. step - This transformation takes the ""locations-geocoding"" from the previous step, imports only needed columns - query, latitude, longitude and creates 2 new tables - first with succesfully geocoded loactions and second with location which were not geocoded succesfully. Primary Key is set to location. Loading option is set to Incremental Loading.
I am not sure what is meant by ""You should make sure your solution is robust, so if the pipeline fails one day, it won’t break down and it will work as expected after that without the need to manually change anything."" - I assume that the flow shoud continue even if the locations are not goecoded succesfully. Therefore I have marked up step 2 and 3 with ""continue on failure"".",
Sent from my iPhone,
Gift card API service,
"Having steps in flow is really nice feature, but it would be much more practical if veery step in the flow would have IN / OUT tables. This way we could debug flows at much faster pace.",
"Could you please create for us working python REST API example that would take table ID and would list all transformations and workspaces that take this table as IN and as OUT? There is no such function in keboola right now and we need to keep our tables and transformations under control, without constant manual checks.",
"I think you support only csv tables for IN / OUT. Perhaps you know that parquet can be several times efficient for disk used and also for loading saving times. Streaming in parquet is also supported. Overall I use parquet everywhere I can, because there is no reason not to. Using parquet would almost certainly speed up loading times and upload times from transformations and from workspaces. I see that snowflake supports parquet import and export, so I guess you only didn't implement this format into your backend and frontend. For IN it doesn't matter, because the backend should be able to recognize the format (and not only from the file extension  ). For OUT there could be a way of selecting the format for table to be exported.",
"In the python workspace we would like to request newest patch versions of python3.11 and python3.12. Yes, we are well aware that not all packages are working with python3.12, but we would use it anyway for the most transformations. Please if you add it, make it also available for our requested enhanced python workspace (other ticket).",
"As discussed on the first meeting, we would like to request custom docker container for python workspace. That means everything that is inside python workspace + following list of commonly used utilities:
htop, jq, parallel, pv, zsh, ncdu, nmap, tmux, screen, hyperfine, btop.
Except btop you can find all of then on apt and probably on dnf and yum. Btop is only available on snap, of course you can build all of them from source, but this is not our request. We would like to get reasonably up to date versions of them, I understand that packages in apt are few versions behind. Also, please do not remove the current workspace, but rather make another one available with requested setting. If you add there zsh, please make is as default shell. ",
"Maybe I missed the feature, but as of now I am not avare that keboola can store secrets as global vars. Having such feature as it is on GitHub or on GitLab would be nice to have. Now all secrets are stored in workspaces and in transformations. ",
"It would be awesome if we could move around transformations in the structure without the use of move button. I understand it is simpler to have it this way, but I don't think it is that difficult to create movable elements in web UI.",
"When preparing transformation or setting up workspace it is slowing us down when we need to manually set inputs or outputs. For example for selecting desired columns. In the end we are not doing it very often, because it is taking some time. It would be better to have operations such as copy, move between and within transformations/workspaces and option of bulk edit of intputs/outputs.",
the running (graph) status of the transformation running inside flow is lost when editing,
"Having flow is really nice feature, but it would be much more practical if the whole flow would have IN / OUT tables. Plus seeing intermediate tables would help.",
Right now it is hard to determine from the graph where the week ends and where it starts on Project consumption page. I would appreciate if you could include vertical dashed lines or solid lines ideally gay and with alpha=50% or something similar. This way one can easily navigate in project consumption history.,
"Dear Keboola,
after uploading a table, I'd expect the Table Schema to be at least partially inferred. When working with a lot of tables that are being created daily, setting the Schema manually becomes quite time-consuming.
Is there an option to do so? Is this a feature you're working on?
The most basic need for Schema becomes evident once one wants to simply sort a numeric column. (which I can't if the column is not defined as INTEGER)",
"Relate to my previous request.
When changing the schema of the output of a transformation, it is extremely annoying to having to delete the output tables (most likely by going into the bucket...). Deleting should be possible directly from the transformation",
Following  https://keboola.atlassian.net/browse/SUPPORT-2070 I am creating separated “Product request and feedback” regarding the outputs of BigQuery and OneDrive data sources to a specific bucket.,
"Ticket was created after discussion with Tom Netrval.
It is expected that the recovery of data lineage information will be more likely when more of these tickets exist.",
"WHY:
pro tabulky plnene z vice zdroju. Rano si naplnim dataset a odpoledne si chci aktualizovat jeden sloupec/field. Ma L1 a L2 vetsina transformaci. Napr. kdyz jdu nastavit output mapping a dam o 2 sloupecky min tak by to nemelo vadit. Moznost nastavit pro nektere sloupce mandatory ktere budou naplneny null
Partial transfornation for some partition - denni stahovani transakci kde chci vymazat jen cast tabulky",
"V navaznosti na https://keboola.atlassian.net/browse/SUPPORT-1961  Table Definition API endpoint nedovoluje dodatecne metadata, ktere podporuje Snowflake specificky comments.
e.g.
select table_schema,
       table_name,
       column_name,
       comment
from KEBOOLA_226.INFORMATION_SCHEMA.COLUMNS
where table_name = 'YEVGENIY_TEST_COLUMN_COMMENTS'To je funkcionalita, kterou by chteli vyuzivat v CSas. Prosim Yevgeniy, aby doplnil kontext pro jejich usecase.
Nase aplikace, ktera vytvari table definice z predpisu umoznuje zadavat “komenty', ale je to standardne mysleno jako Metadata Storage → tj zobrazi se uzivateli ve Storage, samotna fyzicka tabulka ve Snowflake je na sobe nema.",
"Hello, 
could we please create more projects so we would have project per client?
Thank you",
"Hi,
We need help building a component to their Pervasive Database.  It is version 13.3.  Client has the ODBC driver to use to connect.",
"There should be a way how to display average per last N days (7, 14 ,21, 30 days or something similar) on windows that are of bigger size than those smoothing windows. The point is that the consumption oscillates and it is hard to read the long term trends between weeks.",
"Hi, this is another broken Google extractor. 
I will try to explain what is wrong and how trapped is every user using it.
Scenario: I want to download stats from my Google Ads. Fist I want to download last year then I want to download every day last 30 days.
1. Downloading last year works perfect. 
2 Then I go to table and set Primary key for every attribute except metrics.
3. Second run I download last 30 days
Here comes the problem it's something between full load and increment, but none of these so I am traped. 
Because it duplicates rows and delete my PKs from table. 
Can you somehow fix this, please? 
Thanks a lot 
Michal
PS: on example you see how it ignores PKs and duplicates rows according how my metric changes during the day.
My solution? Fix the increment or let me rewrite table each run, I will do increment on my own in tranformation, but this behaviour is trap. Thanks",
"Hello Keboola,
this is a follow-up to https://keboola.atlassian.net/browse/SUPPORT-2235#icft=SUPPORT-2235 where I was advised to send this as a feature request.
Currently the Google Ads Data Source lets you pick from a dropdown of Accounts for which the component rows should extract data. However due to the hierarchy of accounts in Google Ads you are only able to pick from directly shared Accounts for the authorized account.
In our case we have a few directly shared Customers and then we have huge MCC's containing hundreds of Customer Accounts and I'm not able to ""drill-down"" and choose a specific Customer Account under an MCC.
This is generally fine because Google Ads Query Language used in the component rows allows me to filter out 1-x specific customers in the query. But! Keboola at the moment sends the query against all customer accounts in the MCC, rather than send the request only against the customer accounts specified in the query. This means that in case of big MCCs the job is running for example for 8 minutes unnecesarily when it could run for just a few seconds - so we're basically burning credits for nothing. 
Sharing the customer accounts directly to the authorized account is not possible as only 20 google ads entities can be shared to an email address and it's not feasible to do this for multiple use-cases where we want data for only a few customer accounts.
What I think would be nice would be to either:
a) make the component aware of any customerIds specified in the query
or b) include a text-input where I would be able to specify customerIds (akin to Adform DSP reports component or Sklik)
That's all 
Have a great day
Filip
I understand this",
"Hello. 
I would like to submit my finished assignment for the Data Engineer Certificate, Business Data Model part. Flow link: https://docs.google.com/drawings/d/1yU6rONxUZd6jhN3FmiiNHwwLqh6AlwV-mPozbKMrSAk/edit?usp=sharing 
Thanks",
"Hello. I would like to submit my assignment for the Data Engineer Certificate: Components and Processor
Link to the component: 
https://connection.north-europe.azure.keboola.com/admin/projects/18025/components/keboola.ex-aws-s3/57310317
Thanks",
"Hi Team,
At present, while we're able to access insightful metrics such as impressions and engagements via the Share Statistics table at the organization level, I've observed that it lacks the essential Post URN. This missing link prevents us from seamlessly associating these statistics with their respective LinkedIn posts.
Upon reviewing the schema documentation of the LinkedIn Share Statistics API, it's evident that the Post URN is pivotal for aligning these statistics with individual posts.  Could we include the Post URN alongside the Organization URN in the extracted share statistics table?",
"https://connection.north-europe.azure.keboola.com/admin/projects/17512/queue/57516701 Hi please can you help me - I need to create a configuration with a 'Bearer Token Authentication:' but I keep getting this Unkown authorization type message. Is it possible to do this? every variation of the below fails: 
""authentication"": {
""type"": ""Bearer"",
""headers"": {
""Authorization"": ""Bearer your_access_token"",
""Content-Type"": ""application/json""
}
}
Thank you",
We have some cases when column names are longer than 64 characters. Keboola has restriction for column name for 64 characters. Snowflake has 255 characters(according to documentation). We need extend Keboola restriction to more characters. ,
This might be duplicate - sorry if it is… This is coming from P3 - they would like to see the folder of their transformations (and in future also of the configurations in general),
"Hi all,
Keboola is not working. Different web browser or different pc or different account does not help.
Please make it work asap.
Thank you,
Radan",
"Hey guys, I was checking KBC telemetry and I can't find if there is a way how to join to the JOB RUN ID a name of the configuration row? 
Meaning that for RUN ID 641016723.641017749.641017997.641018206.641018753.641020709 I would like to see that it was configuration row ""dedicated-engine"", not just ""prod-us-part"".
What I am trying to achieve is that I want to agggregate running time by kbc_component_configuration_row to figure out what is running for the longest time",
https://keboola.atlassian.net/browse/SUPPORT-2927,
After login I am waiting for reaction of web page!,
"I don't see Redshift destination component in Components tab. I saw this component in documentation, but I can't create this component. Do you actually have this component?",
"Hi,
I have a question about the free credit (for the following month). We´ve run out of credit the first month - does it mean the free 60 minute credit won´t be received the next month until we top up? 
Thanks,",
"Hey.
I'm pushing changes to TR in my project (example version #36):
```matejkys@Matejkys-MacBook-Pro l1-5_kbc-telemetry % kbc push
Loaded env file "".env.local"".
Plan for ""push"" operation:
B main | changed: metadata
C main/transformation/keboola.snowflake-transformation/kbc-telemetry-cloud-keboola-csas | changed: configuration, transformation
Push done.```
As you can see, both config and transformation are changed. In this particular case, I'm adding some SQL script and adding couple output mappings.
But if you look at the version diff, you can see only changes to the script and not to the mapping (config).
Matějkys",
It would be really helpful to have also regex based filtering in string columns within data preview. Current functionality in info seems pretty much limited. ,
"We already have an example where the python even using polars was that slow that it was better to create rust program, compile it and use the binary within keboola.
How does it work? The transformation takes the binary that is stored in files. The binary has to be copied, because otherwise there is an issue with reading it (some issue with permissions on filesystem). Then it is made executable and after that I execute it with `subprocess.Popen()`. This works, but basically it is redundant to execute it from python. I suggest creating bash transformation that will be with bash script executing commands.",
"I see you are using Debian 10 image on workspaces. Do you think it would be possible in the long run to add the ability to install packages via apt within the particular workspace? I know we are discussing creating custom workspace with those packages, but I see it as a workaround. Installing from apt will probably require superuser role. I am not sure how big of an issue would be granting superuser to the jupyter user. Since it is sandboxed, I think the risk is small if not none. ",
"It would be awesome and really practical if one could display in canvas automatically generated oriented graph of all tables. All the tables could be connected by transformations or by workspaces. Connection between table A and B is in case if transformation / workspace has in IN tables table A and in OUT tables table B. I made other ticket for API example for basically obtaining everything needed for this functionality. Of course if keboola could support it natively it would be a huge help. I believe not only for my team, but teams in general on keboola.",
"Daniel Herman:
the workspace will timeout after the tables are loaded, but I am not actively saving jupyter notebooks. I think the timeout should be bounded to CPU load rather than notebooks activity.",
"Hi, I have Postgres connector and it starts to fail: Error connecting to DB: SQLSTATE[08006] [7] FATAL: unsupported startup parameter: options https://connection.eu-central-1.keboola.com/admin/projects/25/jobs/653769645
Please, can you help me how to fix it? We didn't make any changes in our database, thank you.",
"Hi, I'd like to try out snowpark transformation, but I haven't been able to find any example/snippet/docs anywhere. Can you provide me with something to get started? Thanks!",
"Do you have a Qualtrics connector or documentation on how to create one? 
I saw Qualtrics mentioned on this document (pg. 16) but I don't see anything in the New Component module. https://help.keboola.com/kbc-intro.pdf",
"Hi, why there are such long waiting times for jobs to run?
Regards, A",
"Snowpark transformation is based on our generic keboola.python-snowpark-transformation which lacks support for custom PIP repositories and so it can’t use CSAS repositories. This implementation was done only for custom keboola.csas-python-transformation-v2. With workspaces, there is no special Python workspace image for CSAS, so the support for custom PIP repos is already part of generic Python workspace.
It should not be to hard to port the custom PIP repo feature to generic keboola.python-transformation-v2 but it’s definitely for a feature request.",
"Hi.
I've got stucked storage jobs in one of our telemetry projects. And that (I assume) blocks telemetry data update for several hours (that's why I chose sev 1).
All waiting jobs are related to this table: https://connection.keboola.com/admin/projects/7874/storage/out.c-kbc_internal/kbc_trigger#overview
Thanks a lot for your help,
Matějkys",
"Hi,
I'd like to ask for enabling Generative AI Keboola component (kds-team.app-generative-ai) - it has already been added to the list of SLSP components. Here is especially important that it should be set up in ""no-internet"" mode.
For this to work correctly, I would like to ask for creating new private endpoint to our Azure Open AI service - it's added in the list of private endpoints with #14.
Thirdly, after this will be set up, I'd like to ask for enabling all the ""native"" Keboola AI functionalities - description generation, impact analysis etc.
Thanks.
Matej",
Could you consider making component for seznam.cz imap? We would like to send emails via seznam.cz.,
This project is non-sox at this moment. Could you please convert to SOX.,
"This feature is missing and it would be desired. The constant loading and timeout of workspaces is annoying and it slows our work. I suggest following timeouts 1 hour, 2 hours, 4 hours, 8 hours, 12 hours, 24 hours. ",
"Hi, thanks to the amazing work and help of @Jakub Kotek we have now ability to create dynamically loaded elements, validation buttons using sync actions and also beautiful multi-select elements using just the JSON Schema generic UI. Jakub managed to do in a day most of what we were requesting for past 3 years. And it made a huge difference in UX. Just compare esnerda.ex-bingads and kds-team.ex-bing-ads 
I would like to ask if we can regularly add similar features. I know you mentioned it before on the product call that extending the generic UI is the way to go.

I suggest the next step is allowing us to create writers with nice UI, That means we need an element similar to existing db writers that would have preloaded columns from the input table. We cannot do it via sync action loaded lists because the input columns are not listed in the configuration unless explicitly selected. The only way is to hack it via forward token.
I imagine it would work like this:
In the Deloper portal we would have option to select special type of table input mapping that accepts only 1 table. This would populate the dropdown select of input table when creating a row, just like the Snowflake writer does now for instance
The configuration would be created with a hidden array element that contains list of columns → this will allow us to use it for any element as enumSource . That list of columns should be refreshed like it is currently in db writers, but it’s change should not cause configuration change (forcing user to save). 
Btw in general I think we should come up with some JSON schema element group that is excluded from configuration. Otherwise it can also mess up templates / validation. With the introduction of sync action buttons we now have such elements, that are not part of the configuration. We also quite often create helper elements to allow for some UI magic.
We would be able to define Array object (format table) that would contain the column in a row (as on the screen). This would be in two modes: collapsed (user ads new mapping row using + button as normal array); expanded (just like current writers, see screenshot)

We will be happy to provide more info over coffee / call.

Thank you!",
"Hi,
we encountered strange situation, where packages specified in Python transformation cant be installed due to some error. When I install same package via ""pip install user-agents"" in a workspace, there it is working without problems.
To me, it looks like our Nexus repo is not configured on a transformation level, only on workspace level.
Thanks.
Matej",
"Hello,
can you please help us add this workspace WORKSPACE_647001654 to DWHM in this project. 
Thank you,
Natalia ",
"Hi Kritiga
I am going to resubmit the assignment again over the holidays - I see and understand what Juan was saying. Please feel free to close down this Support Request.
Thank you
Jim
James Morrison
Junior Developer
Mob: 07879 147947 
Linkedin: www.linkedin.com/in/jamesruairidhmorrison 
Github: https://github.com/jamesrmorrison  
On 18 Dec 2023, at 08:43, Kritiga Ravishankar <jira@keboola.atlassian.net> wrote:
—-—-—-— Reply above this line. 
Kritiga Ravishankar commented:
Hi James,
I am could you confirm if my colleague’s recommendation worked for you?
If not, please let me know.
Best,
Kritiga  
View request• Turn off this request's notificationsThis is shared with James Morrison.
Powered by Jira Service Management 
Keboola.com• Blog• Documentation• Help & SupportDělnická 27, 170 00 Prague 7, Czech RepublicSent on December 18, 2023 9:43:46 AM CET   ",
"Hi I've been trying to get this generic component to work, but it keeps failing with a ""'config' section is required in the configuration"" message. If I encrypt the token it fails on incorrect cypher. Can you help me to get this working? Thank you",
"Hello, I am reaching out to initiate the process of enabling an text processing AI component in our Keboola environment for Česká spořitelna. This AI component is already in use in the public version of Keboola, and we aim to integrate for internal use at Česká spořitelna.
Request Details:
Security and Compliance: As part of this process, we will collaborate with our security team to ensure all security and compliance standards are met.
Collaboration: This task will be coordinated by Ondra Franc with Vojtěch Tůma. The aim is to move forward with the process
Thank you! 
Ondra",
"Hi, can’t pursue this deal due to missing Oracle CDC feature. Just for visibility and to add to backlog. This is the link: 
https://www.linkedin.com/company/sundaram-finance/
We are using Oracle 19c. For reporting purpose, we are using PowerBi with MySQL.
Both (Oracle and MySQL )servers are on premises.For data ingestion, from Source Database ( Oracle ) to Destination (MySQL) Database we are using talend tool.We are expecting near to real time data. So we are checking CDC.
Near real time data will move from Oracle to MySQL.
We provide Standalone server (DR server) for reading log files.
 which CDC method you using (Log Based/ Trigger based) ?
If log based, is possible to read redo / Archival log files from Stand alone server ( DR Server)?",
"Hi,  your component returns a column name over 64 bites long so the thing crashes. Please fix. see error 
https://connection.eu-central-1.keboola.com/admin/projects/731/queue/617491187",
"Dobry den, Davide.
Dostal jsem, preposlal jsem na Pavla.
Snad to konecne vyjde
Diky
Marek
Odesláno z iPhonu
8. 11. 2023 v 10:32, support@keboola.com:
﻿ 
 

<https://connection.eu-central-1.keboola.com>
A project access token was shared with you.
The admin david.kroupa@firma.seznam.czsent you an access token to the project Seznam_SF.
The sender left you the following message: token ...
The link will expire in 24 hours. 
Retrieve Token
Questions? Email us at support@keboola.com

<https://connection.eu-central-1.keboola.com>
Keboola.com
Dělnická 27, 170 00 Prague 7, Czech Republic 
 ",
"Hello,
we have a failing transfer from Keboola into Big query. The Job error mentions "" Please look into the errors[] collection for more details.""  But where can I find this collection - is this part of Keboola or part of Big query interface?
Specifics:
Job 57218541
ExceptionId exception-6c27b7f0f8a3e5056a83c1d68ffbad52
https://connection.north-europe.azure.keboola.com/admin/projects/16075/queue/57218541",
"I did a phone restore, which gave me access to Google Authenticator. Now I can't login to Keboola. How can I set up Google Authenticator MFA once again?",
"When changing the schema of an output of the transformation, I need to delete the old table. Usually I click the table link in the ""Table Output Mapping"" section of the transformation.
Then I get to a page with table details where I click the bucket name in the top-left corner which takes me to the bucket where I can finally delete the tables.
BUT! Sometimes the bucket page is different (see the screenshot, too bad that I cannot upload more of them) and I cannot delete the tables. I have to find the bucket in the Storage and then it works. This is really annoying.",
"Dear sir or madam, 
Please add alexandre.lima@buildingminds.com to this support portal. Thanks!
best
Lothar",
"used python transformation with the geopy library, worked like a charm",
now everything should be ok,
https://docs.google.com/drawings/d/1aBNUibTgw-HUWLXnx5RMMX8icmdZMGndTxXWZtOh_Cg/edit?usp=sharing,
"that one was hard, but after understanding how the compiler go on order using the code and finding also the developers portal got it",
could you please check this error for me?,
snowflake documentation was the thing to check,
"kritiga.ravishankar@keboola.com(she asked me to tag her)
So this is the example I was talking about, we have 3 funding files beeing 1 with the .xlsx, and i would only want the csv ones, but when I try using the funding*.csv it does not work, is there a way to make this specific cenario work?",
"Hello, is there some limitations for number/size of variables used in transformation? I tried to replace the columns name by variable in union transformation and if I run it for only one table type (vehicle), then it is working, but if I add other tables to join transformation and I keep the same structure for sql and variables, then it is finishing with error. It doesn't make a sense to me. You can see the previous versions within a versioning. Can you please give me some explanation, how to do it right? ",
"WHY: Keboola does not log every statement executed in the DB Transformations. So DB admin who is trying to debug the stucked Synapse job he doesnt know from where the job came from (config_id, project_id)
Keboola should log every step in detail, including every statement/command (selects, source system, target DB, every other command)
Let the users to see these detail in current job logs as third level of info on click. For MVP, it can be in some simple way (e.g. in the database, in some files, ...)
Lets users to write custom info to LOG and retrieve it later => textova informace mezi znaky
WHAT:Instead of improving job logs adding more information we will give CSAS DB admin access to Telemetry project where we extract all job logs we send to Activity Center so admin can use this data and search the problematic config using “user_id” used for that job
DEFINITION OF DONE:Anyone who has access to telemetry project can find details related to Synapse DB user (KBC project, Job ID).",
"WHY: I want select files in FILE INPUT MAPPING with using filters, I cannot fully use dev branches and merge change to prod
userstory: I want to eliminate files where tag is not=processed
WHAT: enable query in File IM Development branch
DEFINITION OF DONE:
user can use query in dev branches
ETA: H1/2023",
"If # is written at the beginning of the line, only the first word is highlighted as commented. The rest of the text is unchanged.
Its only cosmetics, but should be fixed.",
"In case of DB error in Python transfromation, it is required to contact Keboola support to get the error message (in Python Log, there is a message “Application error. Please contact our support support@keboola.com with exception id …”. It would be more convenient to see the error message directly in the log. Thank you.",
"Hi Keboola,
I use event triggers on Orchestractions/Flows.
But sometimes it is quite confusing whether partial event trigger is pending or already triggered. There is no indication in UI. In case of multiple event triggers it can be kind of hell to sync it properly.
Please, consider adding some sort of colored indicator into UI to indicate Event Trigger current status (for each trigger). Maybe some kind of traffic light would help.

PS: Additionally, It would be supercool to have UI handle to change the Event listener status (pending, triggered).
Thx.",
"In my project, I am solving a problem where I need to wait 10 minutes after a data write to perform an additional data write. The reason is just a requirement of the target system not to write data at one moment.
I am currently solving the situation through a Python transformation with a small backend where I trigger the wait for 10 minutes. However, I find it awkward that I am occupying resources for 10 minutes when I am not actually doing anything. It would be great to have a building block in Flows that would execute the timer without starting any component.
PS: I know that delaying the start of Flows can be achieved through programmatic changes to the Flows scheduler, but I find it a tricky solution. A building block in Flows would be easier to use.",
"We might often need to update column's definition - data type, lenght, default value, nullable (true, false).
It's not possible now, not even for native types tables. That in some cases forces us to create a new table and copy data from existing one - that's very expensive when talking about billions of records.
This is a change that is standard on DB level, just altering a column so I think it just needs to be developed and there is no conceptual issue with that??
I'm setting this as critical because it actually slows down process of migration to Snowflake which has a financial impact to us as CSAS organization.",
"Hello - 
Would I be able to regain access to my Keboola Academy Project? It got closed after many months of inactivity but I’d like to reopen it so I can complete the Data Engineer certificate.
Please let me know. 
Thank you!
Best,
Christian Steinert",
"Problem Statement:
When retrieving job run data for Keboola SOX projects, the Keboola Queue API’s /jobs endpoint returns (among other things), a field named previousJobId, which links a re-run of a job/flow to its previously failed instance. However, this is not available when querying for non-SOX project job data. This causes issues when we are trying to store/retrieve job data related to reruns for non-SOX project jobs.
Acceptance Criteria:
1) Add the previousJobId field to the Keboola Queue API’s /jobs response for non-SOX Keboola project jobs. 
2) Add documentation to the Keboola Queue API /jobs endpoint page.",
"The information contained in this message may be proprietary, confidential or trade secret and may be legally privileged. The message is intended solely for the addressee(s). If you are not the intended recipient, you are hereby notified that any use, dissemination, disclosure or reproduction is strictly prohibited and may be a violation of law. If you are not the intended recipient, please contact the sender by return e-mail and destroy all copies of the original message.  
Les renseignements contenus dans ce message peuvent être de propriété exclusive, de nature privilégiée, confidentiels ou relever du secret commercial. Ce message est strictement réservé à l’usage de son ou ses destinataires. Si vous n’êtes pas le destinataire prévu, vous êtes, par la présente, informé que toute utilisation, distribution, divulgation ou reproduction est strictement interdite et peut constituer une infraction à la loi. Si vous n’êtes pas le destinataire prévu, veuillez communiquer avec l’expéditeur par courriel et détruire tous les exemplaires du message original.
Hey Team, 
I'm currently going through the Data Engineer certification courses and was told to reach out for this portion:
How would I go about accessing these projects?
 
--Kevin Strickland Business Data AnalystFirehouse Subs Headquarters12735 Gran Bay Parkway, Suite 150, Jacksonville, FL 32258
kevin.strickland@firehousesubs.com
CONFIDENTIALITY NOTICE: The information and attachments contained in this electronic communication are confidential and intended only for the use of the intended recipients. If you are not an intended recipient, you are hereby notified that any review, use, dissemination, distribution or copying of this communication is strictly prohibited. If you have received this communication in error, please notify us immediately of the error by return e-mail and please permanently remove any copies of this message from your system and do not retain any copies, whether in electronic or physical form or otherwise. 
PLEASE NOTE THAT I DO NOT CONSENT TO THE ELECTRONIC/DIGITAL RECORDING OF ANY COMMUNICATIONS, INCLUDING MEETINGS, VIDEO CONFERENCES AND TELEPHONE CALLS, AND MY PARTICIPATION IN ANY MEETING, VIDEO CONFERENCE OR CALL SHOULD NOT BE CONSTRUED AS CONSENT (IMPLIED OR OTHERWISE) TO RECORDING THE COMMUNICATION.",
"In the current state, DELETE ROWS configured within OUTPUT MAPPING uses only the default S-cluster and not the WH size configured at the trans level.
This causes unscalable low performance of delete operation on large tables. This behavior is also unintuitive because the user expects all operations within a transformation to use the same cluster.
Please consider to respect the specified WH size (and not just the default size) when performing DELETE ROWS.
Thx,
Vaclav",
yes,
"please, restore deleted project https://connection.north-europe.azure.keboola.com/admin/projects/17057",
"Keboola to SF Data Destination transformation 
is running extreme slow...
is there any specific reason behind it....
2 jobs are running extreme slowly....even failed at first run, so reran in upsert mode ",
"Hi, please see response from partner. Can we please scope this today?",
"Hi, I ave tried to make another JOB with new configuration and its make error. How It is possible to download data from Shoptet without error in column exchange rate?",
"Hi there,
I love this component  But, it can happen that the contact has f.e. property EMAIL with primary email and additional email. More than 1 email.. The extraction will get me only the primary email. But its a bit crucial to get also additional email for me. Is it possible?  
thanks a lot
Marketa",
"Problem Statement:
When retrieving job run data for Keboola SOX projects, the Keboola Queue API’s /jobs endpoint returns (among other things), a field named previousJobId, which links a re-run of a job/flow to its previously failed instance. However, this is not available when querying for non-SOX project job data. This causes issues when we are trying to store/retrieve job data related to reruns for non-SOX project jobs.
Acceptance Criteria:
1) Add the previousJobId field to the Keboola Queue API’s /jobs response for non-SOX Keboola project jobs. 
2) Add documentation to the Keboola Queue API /jobs endpoint page.",
I am trying to create a new connection.  I am getting Login Timeout expired.  if there anymore details to see where its exactly going wrong at?,
"Hello, I would like to ask why the transformation wasn't finished...the computation ended around 23:20 but then the transformation continued doing nothing until I terminated it after 11 hours. Could you, please, tell me where is the problem? Since similar transformation ended few days ago nornally. Thanks a lot, Klara",
"Hi,  
I am from Drew Marine. We have been using Keboola/Snowflake for last 2 years.   
Our ERP system JDEdwards was running on Linux/Oracle environment. Recently it moved into Windows/Oracle environment with a different version of Oracle. We are unable to connect to the new JDE from Keboola. Is there anything to be changed or reset from your side ? Please advise. 
I have sent e-mails to Maria Buchenko and Timothy Kuhn, but nobody replied. We are kind of stuck in refreshing our Snowflake database. So your quick attention will be highly appreciated.  
– This email may contain confidential and privileged material for the sole use of the intended recipient. Any review or distribution by others is strictly prohibited. If you are not the intended recipient please contact the sender and delete all copies. --
Thank you, 
Kamal Guha 
Sr. IT Director - Database and Application 
Drew Marine 
100 South Jefferson Road, Whippany, NJ-07981, USA 
O : 973-526-5728 |  kguha@drew-marine.com ",
prolong deletion date till 30.09.2024,
"Hi, here's my BDM for the assignment, it can be accessed on this link as well: https://drive.google.com/file/d/1oLtDmCIyqE7sxASokIYy-YaF6Mfa1Tkb/view?usp=sharing
I'm sure it will need alot of fixing to do but I couldn't figure out where exactly, as anytime I tried to make more relationship connections I was facing the issue with more paths from a child to a parent, so I will be glad for feedback.
Thank you.
Best regards,
Mai Phuong Bui",
"Hi there, we recently switched from our own component to ""kds-team.app-dbt-cloud-job-trigger"" and we are missing one feature.
Would it be possible to print a link to the specific run it triggers, for example:
""Run triggered: https://cloud.getdbt.com/deploy/43491/projects/69424/runs/235927378/""
Thanks,
Matus",
We cannot access the snowflake instance using the username and password,
"Hi,
I would like to ask if you can correct invoice number 4-4088.
We need to have the VAT in CZK
Thank you
Best regards
Lucie Minářová
Finance Coordinator
Smartsupp.com",
"We would like to use the generic extractor to get data from Uber API - OAuth 2.  Via Uber developer portal, access token can be generated, but is valid only 30 days and after that new token has to be generated. Is there any way to parametrize access token, so it can be dynamically automatically changed every 30 days? Or eventually is there a way to generate new token with every call?",
"Hey folks, 
If i am not mistaken we have already discussed this somewhere on the internal Slack, so for transparency and tracking, let me register this Feature Request to enable generating an API token for Developer and Reviewer in SOX projects so they can start working with it via CLI. 
Many engineers at Groupon are used to work with their tools via command line and their IDEs so this would enable them to work according to the routine they are used to. 
Thank you
Pavel",
"I need to understand what is causing these errors: We constantly are recieving Internal Server Error Occurred.
Component keboola.orchestrator job 1077855902 ended in error: Component keboola.ex-db-snowflake job 1077878882 ended in error: Component keboola.ex-db-snowflake job 1077879016 ended in error: Internal Server Error occurred.",
"Hi,  
I’m looking for the best blog to post one of my client’s articles. During my search, I came across your blog. Well, I found that the posts on your blogs are quite edifying and admirable to read. So, I’d like to suggest your blog to my clients. My articles and links will be General not related to  Casino,Loan,Adult and Gambling. So let me your price in $ without adding casino prices.     
Now, I’m reaching out to you to ask if you are interested in publishing guest post articles on your blog without sponsored tags. If so, please devote some time to tell me your thoughts in your busy schedule.
I hope our post will definitely bring you valuable information to your avid readers.
I’m eagerly waiting to hear from you soon.
Thank you,  
Regards",
"Hello, I know it is a tiny thing, but it is still a bit confusing. 
When I create a workspace in our stack, the config id in url and credentials have different ID than the worskspace dataset. 
In the combination with the visibility of all other workspaces, this makes it quite massy and i need to keep looking to the job parameters to get both ids right 
Nevertheless the difference seems to be constant config + 4 = workspace id. 
Still I agreed with Pavel Synek to file it here",
On Dec 2nd 2023 we started having timeout issues with the SQL Server - Trapeze EAM - PROD connector.  Nothing has changed on our end and would like to know the reason behind the timeout issues we are getting.,
"Hi support, I sent a ticket on accident by hitting backspace (#4376). You can delete it.
We have set up a sort of test config with a test folder for you to run in case you can debug it and see something we are not being able to see_
https://connection.keboola.com/admin/projects/9508/components/kds-team.wr-sftp/1071104397
You can trigger the component and send files into the folder set up there. This configuration is always failing. 
We have already monitored the IPs as I mentioned in the ticket #4369, but we also changed the MaxStartups of the load balancer for the SSH in the server side to support a higher traffic and it didn't really help either. (For extra info, we set it to 50:30:150)
The banner_timeout from the ticket #4302 has not helped so we are just not sure where the SSH rejection is coming from.
Thanks",
"Hi,
I have a Pipedrive to Snowflake integration connected to Looker studio.
I am missing some parameters from Pipedrive:
activity type (to do/done)
meeting notes
deal source
contact status (active, no active)
Can you please help?",
"Hello, 
I would like to ask for your help.
For the following payments made via card, I would need to identify the project ID to which the individual payments are linked. 
Nákup: KEBOOLA.COM, ICO 28502787 / VAT CZ28502787, DELNICKA 27,, 030167, CZE, dne 5.11.2023, částka 10.16 USD
Nákup: KEBOOLA.COM, ICO 28502787 / VAT CZ28502787, DELNICKA 27,, 030167, CZE, dne 19.11.2023, částka 10.16 USD
Nákup: KEBOOLA.COM, ICO 28502787 / VAT CZ28502787, DELNICKA 27,, 030167, CZE, dne 10.12.2023, částka 10.16 USD
Nákup: KEBOOLA.COM, ICO 28502787 / VAT CZ28502787, DELNICKA 27,, 030167, CZE, dne 22.12.2023, částka 10.16 USD
Nákup: KEBOOLA.COM, ICO 28502787 / VAT CZ28502787, DELNICKA 27,, 030167, CZE, dne 14.1.2024, částka 10.16 USD
It´s Bill to
Data Driven Marketing s.r.o.
třída Karla IV. 502
50002 Hradec Králové
Czech Republic
billing@marketing.bi
CZ VAT CZ07555750
Thank you for the informations
--
Tomáš Kubín  
Head of marketing  
Marketing BI  
Pobřežní 249/46, 186 00 Karlín  
+420 732 173 721 | tomas.kubin@marketing.bi",
"Could you please explain me what does this error mean, and how can we prevent this in future?",
Could you please share the root cause of this error and let us know how to fix it?,
Could you kindly help us resolve Internal Server Error issue from the Airtable extractor as this is impacting our pipelines and downstream processes.,
"Hey CF team, 
here's an enhancement request for the BigQuery Writer. When setting up the Google BigQuery Dataset value, the pre-defined grey ""help"" within the text box says ""my-dataset"". BUT, if you use the break symbol, you get an error (see attached), which enforces for alfanumeric symbols with underscore only. So maybe a slight. adjustment of the help would be helpful for the new users....
Thank you. 
Pavel",
"Hello,
Some time ago we discussed ways how to use Keboola for running python scripts where some credentials are neeeded to be input. The recommended way was to use the ""kds-team.app-custom-python"" (which is, I believe, created by Keboola's team) but it seems the python libraries version in the container there are not much updated so it's quite outdated and well usable. Do you have any other recommendations?
Kind regards
Jan",
"Hi,
atm, the Flows are designed as each Phase running sequentially and Task in given Phase in parallel. I would like if it would be possible to set for tasks in phase to run in sequence as well (instead of parallel).
The usecase is following: we use phases to group same stage transformations etc together, so we can keep a clean execution DAG. But for big flows, that means that we have tens of tasks in single phase. Now when the execution of phase starts, all the tasks in the phase starts at the same time, which causes unnecessary waits for snowflake warehouse availability, which causes credits burn just for waiting. Currently, this can be fixed by increasing warehouse size (which we don't want to do, because we do not really need the results faster), or splitting the big phases into more smaller phases (which we also don't really want to do, because it will just make everything less visible - different phases meaning the same stage of computation).
Would it be possible to add the option to run Tasks in Phases in sequence? 
Best regards,
Matej ",
"Hi, would it be possible to get notified about warning in dbt transformation test? Currently we only get notified about errors, since it shuts the transformation, but we would like to get notified about warnings as well.
--------------------------------
Follow-up to ticket: https://keboola.atlassian.net/browse/SUPPORT-2986#icft=SUPPORT-2986
Hi, would it be possible to get notified about warning in dbt transformation test? Currently we only get notified about errors, since it shuts the transformation, but we would like to get notified about warnings as well.",
"On https://connection.europe-west3.gcp.keboola.com/admin/projects/70/components/kds-team.ex-dv360/38939
when trying to authorize access by clicking on sign-in by google , a blank page with error is opened:
https://oauth.europe-west3.gcp.keboola.com/authorize/kds-team.ex-dv360
{""error"":""Consumer \""kds-team.ex-dv360\"" not found."",""code"":0,""exceptionId"":""exception-5d48df573ba0805f2d0e884885913ab7"",""status"":""error"",""context"":[]}",
"Hi, 
could you check that you can see tables in the storage? Anytime I open the storage there are no tables (only buckets).. When I open output of any config, the table appears suddenly.
Thanks.",
"Hi Team,
Is there a way to introduce a DATE filter on the Snowflake writer to reduce the data loaded into our BI Workspace? We are currently using Data Filter to select only list of customers loaded, but want to introduce something like date < '2023-01-01'. I know I can preprocess the data in the transformation, but want to avoid it.",
CertificateError(&quot,
"Hi, thanks for you help with Hubspot lately.
I have following problem. 
Now we miss data about list of companies, it's something it's not possible to handle with your component. We can download only list of contacts, not list of companies.
Can you please fix this too?
Many thanks Michal",
"Flow: 
Please move our keboola storage from AWS to Azure as discussed with carl.lundberg@keboola.com",
"Hey, I still think it would be awesome if you could add new feature to the component to handle Error 104.
Matus",
"Hi Team,
I have triggered a job in dev branch and terminated it in 1 hr 36 mins. But the same job was running in BigQuery for more than 5 hours before it got terminated. 
Please help with this.
Thanks,
Ranjith",
"The googleapiclient api has the functionality to extract the Campaign Manager 360 metadata: Accounts, Advertisers, Campaign, Landing Pages, Creative etc
Context:
The attributes are available in reports but, the main issue with this functionality is that it’s metric dependent and therefore will not provide a full metadata picture.
Another issue is there are some missing dimensions specific various levels of the metadata.
The use case is our requirement to be able monitor and analysis the quality of the setup within the platform (i.e. landing pages validation, Ad setup validation) at the same time as monitoring performance.",
"Hello,
do you see longer version of this error? I changed few part of query, but it's hard to debug it, since im not sure what is incorrect here.",
"Dear Keboola Support,
Unfortunately, my project has been deleted... Is there any way to restore it? I use it sometimes for Czechitas Data Academy. Or is there another possibility? 
Thank you.
Yours Faithfully,
Veronika Cizek Martonova
---------- Forwarded message ---------
Od:  Dustin from Keboola <dustin@keboola.com>
Date: st 17. 1. 2024 v 13:15
Subject: Your Project Will Be Deleted
To: <martonovave@gmail.com>


 

 



Your project will be deleted 
Hi there, 
I noticed that there hasn't been any activity in your project  Veronika C-M for the past three weeks. As a result, the project will be deleted in 7 days. To keep your project active, log in and do some action! 
If you're encountering any difficulties with your project, or if you have questions, we're here to help. Don't hesitate to reach out. 
  
 
For any inquiries or concerns about Keboola, feel free to contact us or schedule a call with our expert via Calendly.

Go to your project
Have a good one!
Dustin

 
Keboola.comDělnická 27, 170 00 Prague 7, Czech Republic
unsubscribe",
"Hello,
I’ve tried to connect data from BigQuery to Keboola but for some reason it has never worked. 
I’ve replaced the original source in the SQL Query in one of your templates (Google Analytics Comparison) and added my location  in my BigQuery instead. Now, it looks like this:
FROM
`ga4-bigquery-vsechny-weby.analytics_312102871.events_20240201`
But when I run it it always says that the table or URI was not found. 
Do you have any idea where might be the mistake? You can find a screenshot with the actual BigQuery location in the attachment, that might help.  
I know, it's quite a complicated question, but maybe you can help me.",
"Hi Keebola team,
Any update here?
Thanks,
Eliana",
"Hi,
could you please do an MFA reset for my colleague - dmitrii.sidorenko@actumdigital.com
This way I am verifying his identity.
Thank you,
Pavel",
"Dear KBC,
i would like to ask you on 2 quetions.
1) currently, the JIRA component, what precise API calls are used to get the object ""issues"" ?
2) some of the filelds retrived in ""issues"" object can be in JIRA (we started to use that in 11/2023) linked to another atlassian product ""assets"", causing that we dont retrieve in ""issues"" directly the field value, but just some weird info that could be used (via Atlassian asset API) to get the info from their atlassian Asset module - would it be something that you can guyes extend your connector with, that in case a client (like us) are using the JIRA component with combination of asset atalassian product, that all the fields (including the custom json fiels) which are pointing to assett DB - you would actually retrieve the proper value?",
"Hi, I have difficulties to setup Azure DevOps generic extractor. Could somebody from tech team help eg. David Esner? Thanks Stepan",
"In workspace sandbox i was able to import nltk, however in transformation it returns error",
"Currently, in case of any failure the flow fails and needs to be manually retried every time. This is fine for any steps that do not expect repeated failures, however for any steps that have dependency on external systems/networks having configurable automatic retries will be much helpful.
We need a way to
1. Define/configure how many times a specific component within a flow can be retried
2. Define retry interval between successive retries
3. Send warning email notifications in case a component is being automatically retried",
"Hello, we would like to configure another row for the same object in our Salesforce extractor. Unfortunately, we can't setup different output table.
For example we would like to output data from order_test configuration into different table - not Order that is used as output for Order config https://connection.keboola.com/admin/projects/9004/components/kds-team.ex-salesforce-v2/830012788/rows/1085624615
Is there any hidden config we could set in /raw endpoint or any processor we could use for this purpose, please?
I checked for example Rename File and Create Manifest processors but it looks like none of them is capable of changing table name in /out folder or setup different table name.
Thank you!",
"Hey guys,
Would it be possible to get response_upload.json() after sending data somewhere using POST method? Like it'd be saved somewhere in storage or something, so we are able to work with that output.
Best,
Kate",
"Is it possible in Keboola to pull the changes from production for a transformation without overwriting your own changes? 
For example 2 developers working on the same transformation on different branches. One pushes their changes to production. The second branch is now missing the updates in production but does have the changes already done by the second developer. 
Right now I can not see a way to resolve the conflict except to manually add to the dev branch the changes that are in production or else to overwrite the dev branch by ""pull from production"" and then manually add back in whatever changes were being worked on. This is fine if both are working on small changes but if not then it is quite manual.",
"Hi, 
could you help with the error while fetching the list of tables? 
We were checking the credentials and also whitelisting IP. The connection seems to be working in Data destination connector. https://connection.north-europe.azure.keboola.com/admin/projects/19182/components/keboola.wr-db-mssql-v2/65011546/credentials",
"I frequently filter on NULL values but Keboola by default always converts NULLs to empty strings. We can toggle this int he input mapping but I have to toggle this everytime in each transformation and workspace. It's quite time consuming and annoying especially when I am working with NULLs that shouldn't be empty strings. This also causes issues with data type casting, for example when casting to number or dates or timestamps, empty strings prevent me from setting that columns correct data type unless i specifically use IFF(column ='', NULL, column).
Please have a project wide toggle to leave NULLs as NULL",
"I'm converting the ticket to see if it would be possible to modify keboola.wr-google-sheets the way that it would be possible to use the same table as a input mapping multiple times with different filters and different sheets as destinations but within one configuration. 
Original request from customer:
Hello, 
I found a situation that I wanted to create a component google sheets (data destination type) with one calculated table, but multiple data export using data filter to distinguish what will be imported into each sheet. 
Therefore I created desired component, set up the first configuration and wanted to add another one in the same component based on the same data source, but when I click ""new sheet"" and want to add input table it doesn't offer me that table again. 
Is that a wanted behaviour or not? 
I intended to have only one calculated table with all the data and one component in which I will have more ""jobs"" from the same table but distinguished by filter and different sheets in the desired google sheet document. 
If it is not possible, I will have to a) calculated different tables or b) set up more components for each sheet separately. 
Thank you for the answer. ",
"Hi,
I would like ask you. If is it possible to add filter in Storage jobs.
Thx a lot.
Jana",
Dynamic variables in SOX projects → dynamic variable in the SFTP component - keboola.ex-ftp,
"dbt cannot create the tables because this is native datatypes and we impose NOT NULL on PKs. The issue is there is no way dbt can prep this kind of table.
I had to create that table manually before and either with manual upload = non native table or via API call to get native table. ",
In Components listing I want to be able to see the most recently changed configurations ,
"Hi,
I would like to ask for small change of behavior for dbt remote transformations. We would like to utilize target parameter in dbt run (since we use it in multiple environments), but Keboola doesn't allow us for using any target - it is forced to have static value by Keboola. It's beed discussed with Vojta Tuma.
Thanks.
Matej",
"Hello to Keboola, 
i have question about our timecredits in Agatin Svet project. We dont make any changes in our project and our timecredit increase from 1st feb (flow run about 30min) to 14th feb (flow run over 40min). Can you explain it please? Thank you a have a nice day. Jakub",
"looking to the code and the error message in the log it seems the component failing to find the configuration we set 
https://bitbucket.org/kds_consulting_team/kds-team.ex-azure-blob-storage-v2/src/master/src/azure_oauth.py",
"From https://keboola.atlassian.net/jira/servicedesk/projects/SUPPORT/queues/custom/149/SUPPORT-4132 
It would be great to add parameter to set timezone for the whole transformation including OUTPUT MAPPING.
Because now, setting timezone via “alter session set timezone“ works in the transformation itself.
But not in the Output Mapping part (writing to Storage). e.g. if you have a view with a current_timestamp() function, it will be evaluated  in the OM - ignoring the session timezone settings.
That’s very confusing. 
Adding an option would solve this problem",
"Hello, 
I need help with creating component of PinterestADS. I have done all the steps, chose all the columns which were loaded and deleted some of them, because of the error job which said that some of the columns cant be there.
Now it says to me, that in CSV file there are some duplicities, but I checked all the columns and there is none duplicity column. So I dont really know, what should I do now.
Please can you help me and maybe give me some advice?
Best regards, 
David",
It's currently only available in SOX projects. Vault allows you to set variable across transformations. Transformation variables are not replacement. ,
Add date and timezone to the list of variables https://developers.keboola.com/extend/common-interface/environment/#environment-variables to allow date-aware pipelines.,
"Similarly as the amazing UI feature ""EDIT ALL SCRIPTS"" it would be great if the CLI could represent all code blocks and snippets within transformation e.g. Python in one file. transformation_code.py.
WHY:
The blocks are only functional as a whole and executed in sequence. One block/snippet may be only definition of imports, another may be a single function. This is very useful to structure the code and quickly orient in the project but it is not practically usable when you want to work locally and edit the project directly in you favourite IDE (The files need to be merged.)
I heard this requirement in several discussions with customers. Also Michal Hruska brought this topic recently from some of the enterprise client.
In my opinion the CLI should enable this option as it is already available in the UI and proved to be very useful.
Usecase: 
Client wants to develop Python / SQL code locally in their favourite IDE. They could simply open the file and run it straight away. Especially with eventual ability to connect to Jupyterlab remotely or by exporting a workspace content into a temp local workspace file (we could have support in our library for that, and already partially have).
Until then the CLI representation is useful only for templates and nice feeling that I have project versioned in git rather than for actual code-first development purposes.
How:
Same way as the UI does it. Also Jupyter solved ipynb <-> py conversions using similar logic via comments https://code.visualstudio.com/docs/python/jupyter-support-py#:~:text=When%20you%20open%20a%20notebook,Python%20file%20(.py).",
"It would be great to add extractors in the usage preview of the table. We have many tables, that are extracted in different places and we need to have know that if we changed the script on one part. It should be reflected in the other part. ",
"We miss an option to setup different hourly schedule for weekends and weekdays. We would like to have only one flow that would run for example each 4 hours on weekdays but only once on weekends.
Currently, I have to create different flow with different schedule and trigger original the original one.
Simple weekly schedule with subform for each day or something like Airflow's Timetable would be great to have in KBC ",
"When I am looking at data in storage, I would love to be able to create a workspace that has this table input in one click.
Simple filters are great, but if that's not enough, I'd love to be able to quickly create a workspace from here.",
Needs to be re-written as standard component where you provide your own databricks.,
option to define timestamp format to kds-team.wr-sftp in the filename,
"When we join a project as L3 support we have an expiration attached (default 2 days).

It would be great if I could set this value when inviting a user to my project.  Sometimes I just need some consultant for a couple of weeks for example.",
"When looking at storage you can very clearly see which buckets you are sharing with another project thanks to the shared label.  It would be nice if you could see which buckets are being shared with you.  Maybe something like a label with the name of the project that is sharing the bucket.
Additionally when utilizing descriptions for buckets it would be nice if I could add a description to the shared bucket",
Currently you could filter jobs by user.  It would be nice if there was an option to filter all jobs not created by a user (Scheduled Runs),
"Hello, I would like to try this application to be able send emails from Keboola. . I have not found any documentation so I dont have a clue how to set it up. Could you help. Thanks
Stepan
https://connection.hci.keboola.cloud/admin/projects/20/components/kds-team.app-email-smtp-sender/5407904",
"We would like to send emails from our own domain novatv.cz to other emails within the same domain. Could you consider adding component for outlook, tak would enable that to us?",
"Hey folks, 
GRPN is a bit confused (and angry) they are forced to merge and pull from production even configurations they have not touched in the branch. They expect some minor simplification in the SOX workflow in this area. 
Here is more information: https://groupondev.atlassian.net/browse/GDP-1221
Thank you.
Pavel",
Any plans on supporting MongoDB 7.0 for as data source? It was release end of summer 2023.,
possibility of Data destination component Airtable,
"Job for Data Destination as SF get failed .
It inserted some records and
rejected some records .
So can I get the list of rejected records in a report in Keboola after the job runs.",
"Hey,
this seems like bug - in transformation when you click on any table name and go to Data Sample, the table of data there is cut short and using the whole screen length, looks a bit weird having big empty space at the bottom.",
"We allow users to activate ""Require Multi-Factor Authentication"" at the organization level. However, we don't support it for maintainers and superadmins, or for the whole stack. Therefore, we must ensure Keboola employees follow security best practices through retrospective compliance checks > https://keboola.atlassian.net/wiki/spaces/ENGG/pages/3160309781/Security++MFA+and+long-lived+tokens+checker+for+superadnims",
"Hello, can I please ask if there is any connector/ processor for process data from Twitter? All connectors I found so far are disabled. 
Thanks ",
"Our use case involved getting data from Sources (Salesforce, Hubspot, GSheets etc) and saving to Redshift, and we also want to send cleansed data back to Hubspot. 
Does your tool support this? If so, is this available in the Free Tier?",
"Hello,
I’d like to ask you, if you plan add social media “X” as a data source in near future.",
"Hey boys and girls, 
let me follow up as agreed with Tomis and Pepa Pokorný on the point of versioning in Dev branches. Fixing this will allow for reverts in case they need to get back. 
The problem is that once you create a Dev branch, the configurations do not list any previous version (there always is Version 1: Copied from Dev branch). 
If we start listing the previous versions of the config, you could easily revert back to any of the previous versions and merge into Dev branch. 
Hope this makes sense.
Thanks
Pavel",
"Hi there, 
with reference to the ticket https://keboola.atlassian.net/browse/SUPPORT-3961, let me raise this Feature Request to enable setting up 0 approvers required for the SOX approval process. Currently, this is not possible, but GRPN would need it to simplify the process. Let me also confirm that from the perspective of their audit it is acceptable. 
Thank you
Pavel",
"Hey guys, 
we have a request from the GRPN people. They are asking for the possibility to use a CLI (generate token) for the Developer and Reviewer roles. The feature is being requested to allow for CI/CD. They want to use CLI mainly for tracking the differences in the code, according to my understanding.  
Is this something we can deliver?
...AFAIK, internally in Keboola there is a lack of a consensus on what functionality there is in CLI in BQ+SOX environment. So maybe just testing this first might help. Please let me know... 
Thank you
Pavel",
"Hey guys, 
another issue reported by Andy Werner and GRPN people. The issue is following:
""As part of the developer/reviewer role we are not able to see the  runtime variables section in keboola transformation configuration UI, in the multitenant environment we are able to see the variable section, add and define runtime variable for the transformation. Tried to look in both the production and development branch, I don’t see that option.""
GRPN is pushing for the possibility to see the variables. Probably because SOX is mainly focused on governance, but if you can at least SEE the variables, there should be no harm....?
AFAIK, we are not able to run a flow with VAult variable. Our job runner should be able to run Vault variable in the same way as the normal variable. 
Fisa has described the possible workflows for working with Variables in https://docs.google.com/document/d/1T2xk7evOC_7w2ZwbKge_8I0ItY_2KsoIBlo6S6xHONc/edit.
Anyway, we should consider whether or not we want to process this and deliver some solution. I would be grateful for any kind of feedback. 
Thank you
Pavel",
"Hey guys, 
with Andy Werner we have just come through an issue - when having multiple commits within a Dev branch, when merging, conflicts occur. 
The use case is following:
1. Karel si vytvoril vetev
2. Pepa upravi transformaci (a mergne)
3. Karel upravit extractor 
4. Karel je v konfliktu ac nesahl na stejnou vec
Possible solution:
pull from prod na vybrane konfigurace (stahni jen Pepovy zmeny ale neresetuj muj extractor)
FYI: Yesterday we had a call with Jirka Semmler, Roman Bracinik, Andy and Fisa to discuss this. ",
"Hey guys, not sure if this Feature Request has already been submitted the official way:
GRPN is asking for a rollback function whenever they need to make some hotfix. The use case is following:
PM merges some changes into Production and as a result they see a major issue has occured. They need to switch it back ASAP to make sure the business is not being blocked by the administration process of SOX (2 approvals needed). So they are asking about a possibility to revert the changes. 
Such a discussion has already taken place on Slack: https://keboolaglobal.slack.com/archives/C05BK5V8N1Z/p1702653758111379
Please let me know whether or not this is something we can do anything about. But anyway, this would require a feature call with GRPN to make sure their audit would be happy with the rollback workflow.
Thanks
Pavel",
"As a developer, I would like to treat schedule configurations to follow the same CICD process as other component configurations. So that I can add/make changes to a schedule in a dev branch and then create a pull request to production. Currently, it says that ""Schedules can only be setup in production"".",
"Hello, 
We lost the roots opportunity ($30k for Q4 2023 | Salesforce). 
While we were able to outperform Matillion from a user experience perspective for ingestion (especially our ComosDB extractor) due to our flexibility, we fell short on no-code transformations. They found our no-code transformations too limiting in terms of functionality compared to that of Matillion (who can provide 100+ out of the box transformations). One of their success criteria was for a business user to be able to use a no-code transformation and they couldn’t see a way that their business users could use our no-code transformations to accomplish everything. 
Potential action would be to further develop support for no-code transformations and SQLbot. 
Quote from the prospect: 
“I would rather deal with the headache/problem of data ingestion than our business users dealing with the headache/problem of working with the data. I know Derry (Director of Application Development) would buy Keboola over Matillion if it was only up to him, but this is up to our business users and what is best for them.” Scott Hunt, Director of Data Analytics
Best, 
Timothy",
"Hello, 
We won the Fourtop opportunity ($15k in Q4 2023 | Salesforce).
Since I was very hands-on with the generic extractor, it helped a lot to showcase the “ease of use” of connecting niche data sources, however, their technical consultants struggled with it on their own and couldn’t create a working configuration after reading our docs. 
I believe that a UI for the generic extractor would have helped them to use Keboola more independently and to improve the user experience. 
Best, 
Timothy",
"Hey guys, 
not 100% sure this is a Product feedback (not a bug). According to my understanding, BQ workspaces are not isolated one from another. Therefore using one workspace you get access to all the schemas within the BQ project. This may result in multiple hickups and antipatterns. 
Fisa has suggested that there should be an option of using tags when creating new workspaces. Based on the tags, we would then provide certain rights to access limited to the given tables only. 
As GRPN people request access to workspaces even under ReadOnly accounts, we should revise this I believe.
Can we please look into this? 
Thank you
Pavel",
"Hi, 
is it possible to add conditions, so that this flow orchestrating other flows will be triggered everyday 5 minutes after midnight, but only flows with: 
""daily"" in its name will run everyday,
""weekly"" in its name will run every Monday,
""monthly"" in its name will run every 2nd day of the month?
I was thinking about advanced parameters, where I could modify the API call which would be skipped in case the condition is not met. But I am not sure how to do it.
It would help us a lot to organize all flows in one place.
Thank you,
Jan
[Assigning different schedules in one flow]",
"GRPN and Productboard complained that its time consuming to set up the flow notifications PER FLOW, if they could do this GLOBALLY.
I think at least setting it up PER FOLDER might be super useful and maybe an easy task to implement?",
"Ability to give dedicated token right to get all events per project - for audit purposes.
discussion on slack:
https://keboolaglobal.slack.com/archives/C05BK5V8N1Z/p1704489355226979?thread_ts=1704487273.579069&cid=C05BK5V8N1Z",
"In Flows, it is currently impossible to schedule Workspace jobs. Could we have the option to also schedule workspace jobs (loading data)?",
"Hello,
I'm writing to seek assistance with transferring the output file. After transformation is finished I need to add output file from storage to some folder in OneDrive. Is there some possibility how to do it?
Thank you for your time and assistance.",
Seems like now there is no clean way of terminating the script except the kill of shell or the process itself. When pressing Ctrl + C during download in kbc the download doesn't stop and it is possible to kill only after the whole progress-bar is indicating that the download is finished. ,
Orchestrator Main failed.  Please fix and re-run.,
"https://evidence.dev/
Super easy to set up, easy to develop and great way to have robust data app for KPIs and BI.
Would play nicely with the rest of the platform",
"Hi,
It would be great to have the option to set all tables with primary keys identified to have incremental loading and with no lock set as default.  Hate having to configure these one by one.",
"I’m going to be switching role inside the company and I will not be using Keboola so often. As a dedicated user and after 3 years of extensive work with Keboola in two different companies I would like to provide a general feedback.
I have categorized my feedback into two sections: features that I appreciate and believe are the strengths of the product, and aspects that I feel might benefit for further consideration or improvement. Please find the detailed breakdown below.


*Liked features*
I’m overall very pleased with the way *Support* has always responded to our tickets. Not everything was resolved every time, but the speed of responses and the offered solutions were always very good.
The biggest advantage I see in the *large amount of Extractors and Writers* that Keboola provides. It is really easy to connect to most of external sources.
The *UI* looks good and is very intuitive. I also like the improvements you came up with recently.
The *Search bar* is really handy. The fact that you can search through Flows, Components, Transformations and Storage at the same time is really cool!
I really like the *“Used in”* feature in the Transformations.
*Snapshots and Restore* functionality of the table in Storage is really useful. It helped us many times with debugging.
The ability to create *custom components* is cool. I created 3 component which we use on daily basis (https://bitbucket.org/zh6/workspace/repositories/).
*Keboola academy* - I’ve made 3 certifications and I enjoyed it and it really helped me to understand how Keboola works under the hood. Some of the information might not be necessary for a daily job of a data analyst or an engineer, but it can be really helpful when debugging or trying to solve some special case. I find it a shame that the courses are not maintained and are now hidden.
I like *Eliška*, the communication with her is great and she helped me quite a few times 🙂


*Disliked features, issues & ideas*
*Null and numbers as strings!* I find this as the major drawback of Keboola. There has been made a plenty of mistakes due to this “feature” and the amount of the NULLIF function in our code is overwhelming. It took a long time to get used to it.
It is impossible to *trigger an orchestration* once per day after two or more tables are updated the same day. Detailed information can be found in this ticket: https://keboola.atlassian.net/servicedesk/customer/portal/9/SUPPORT-1058. I’ve had to make a special component to handle this: https://bitbucket.org/zh6/orchestration-trigger/src/master/.
I’m not a big fan of the *new transformations*. Having the code directly on the page was much comfortable for me. The code-blocks would be useful to me if it were possible to disable them. I’m really happy for the “edit all queries” option, which was added recently.
The *“Usage” tab* in table preview is really handy and crucial for seeing the dependencies. However, a dbt-style lineage, where you could see dependencies over more phases of the ETL would be awesome. Also, to see some cross-project dependencies would also be very useful.
I'm very glad for the latest modifications you added to the *""Data Sample"" tab* (sorting, column reordering). I've been working with this tab a lot and I would like to see it a bit more flexible e.g. allow aggregation, filtering by text part, or wildcards, clear all filters button, export just the filtered rows, row counts, and so on. This would eliminate the process of starting the sandbox, loading data into it and connecting. Also, some non-SQL users could by looking at the data from different perspective directly in Keboola.
Not a big fan of the *new naming* (Data destinations, Data sources, Flows). Internally we still use the old naming (Extractors, Writers, Orchestrations), which seems more accurate to us.
I would really appreciate if the *Snowflake Transformations and the Snowflake Data destinations had a different icon*. These are the two most used components and being able to visually distinguish between them would be great.
Sometimes I feel that the error messages of an *failed job* could be more descriptive (e.g. pointing at the error in the code).
I would like to be able to set a *timeout also for python transformations*. This is currently not possible. I remember we had quite a few times problems with that, because some scripts were running for 8 hours, mostly due to an inability to make a connection via API.
I would very much appreciate *dark mode* 🙂.
Best Regards and a happy New Year,
Zdeněk H.",
"Hey guys, I am mirroring an issue reported by Pepa Pokorny at GRPN. In parallel, i am posting in Slack #prj_groupon
====
need your help with data types inconsistency, which is huge inconvenience for sharing outputs from Keboola to the end-user data layer.
Looking to Keboola storage we can see some data types mapped according to last transformation:
https://connection.groupon.keboola.cloud/admin/projects/15/branch/94/storage/in.c-tr-level-40-unit-economics/unit_economics#schema
But once I look into the BQ table, all the fields are mapped as a string.
Nevertheless once we would like to provide data to end users the data types should be mapped accordingly. 
Is this behaviour desired?
How can we fix this?
How do the keboola transformations work with datatypes?",
"on GCP singletenant (and future MT) we shall not need to require unload bucket since we should provision this by ourselves. This is bringing a huge headache to Groupon at the moment and I foresee this will be a pain in MT as well.
See unload configuration",
"Hi, I have a special and strange request.
As part of the internal pricing, I should be able to roughly estimate how much 1GB of data costs. I know that the resulting time consumption depends on the type of extractor, the complexity of the transformation and the type of output, but I was wondering if you have a very simplified structure of what range we could go with.
Thank you",
"see the attachments. It would be great, if last run would be always as really the last one on the right side of the chart",
"Hi, 
currently dbt build generates artifacts just fine, but the problem is when dbt build fails. 
Would it be possible to put artifacts into storage even after the build fails so we can use the artifacts anyway?
Thanks a lot
Matej",
"Hello,  
my colleagues have already modified the callback URL as specified below. 
How should we proceed? 
Thank you, 
Martin Dojčar",
"Hello, 
Do you have any kind of report already built in Gooddata or Looker for activity center? 
It would help a lot. 
Thank you
Jan",
"Hi support,
I am getting error where requesting data for more then few months. Unfortunately I am not able to select time period. I need data from at least a year ago.I am not able to do it partly, only in one but I am getting this memory limit error. Can you please help me to fix this.
Thank you.",
"Hello,
Hope you are well. 
Get a notification for project deletion.
Trying to find the PAYG plan to add a card & funds but can't find it. 
How can I make sure I don't lose my instance?
Best,
Periklis",
"Hi Team,
Just FYI - I have again lost access to this project!
( kj+keboola@overdose.digital) 
Can you please help me revive that project?
As you see, it's not been a month since I had used this project and even added some top-up minutes to it. 
Can you confirm why it keeps getting archived like that?
Cheers, KJ
KAUSTUBH JOSHI 
GROUP CHIEF DATA OFFICER 
 
P: 
+64 (0) 22 301 1274<tel:+64(0)223011274> 
E: 
kj@overdose.digital 
A: 
New Zealand • Australia • Singapore • United States • Ukraine 
 
W: 
overdose.digital 
 <https://www.linkedin.com/company/overdose.digital/> 
 <https://www.instagram.com/overdose.digital/> 
 <https://www.facebook.com/www.overdose.digital/> 
  
 ",
"Hi Support,
Telly decided to terminate their contract and they would like to migrate their project to PAYG. They only have one project which would actually need to be migrate to Azure. How long could this migrate take? I need to prepare the exit plan together with RevoltBI. Thanks!",
"Hi support team, 
Can you please top up 10 credits (600 minutes) as this is a project that will be used for a Empower next week. 
Best, 
Timothy",
Need to extract data for the Tv campaigns running from Teads. It would have been great if we have a extractor from Keboola.,
"In the list the descriptions are (rightfully) truncated, but would be great to see it on hover (or with ""more"" link)",
"The Re-Run Job button is exactly where I would expect it to be - if I know the platform. For fresh users, maybe it would be easier to also display that button as the highlighted option in the ""job error"" section next to ""Show Details"" button. Maybe needs to be looked at in heap if that is making people churn. ",
"Hey folks, this is a Feature Request to redefine user roles. GRPN finds lack of sharing rights as a big blocker. Since cross-project sharing option is bound to an org admin role, it is a limitation, because we do not want everyone to be org admin, while we need to allow multiple people have the possibility to share buckets. 
It slows them down whenever they need to find who has the sharing options in order to proceed and build their data pipelines. 
Thanks
pavel",
"Hello, Keboola Data Sample feature displays NULL values as empty strings for native data type tables. I think that the user should be able to distinguish empty string and NULL visually.
Thanks!",
"Hello
I would like to know if you are able to determine if Mall's workspaces are being accessed from outside of Keboola (e.g. via JDBC). If so, what kind of information can you provide? You can send us as much raw data as possible and we will manage it.",
"Hello to Keboola!
We have up and running snapchat data source in our pipeline. It was working all fine until someone added new ad account to snapchat which we can see in list of account in snapchat itself, but apparently we do not have permissions to query for data.
I am afraid that this will keep happening in the future. 
Would you know by any chance how to limit list of ad accounts for which we would like to download data (similar to other components, for example Facebook)? Could this be achieved using Keboola API or something? 
If not, do you plan on adding this feature? 
Thank you!
Andrej Kováč",
"Dear KBC team, 
we are quite frequently stumbling onto error messages in the keboola UI, where (I think) error messages pop up when some resource fails to load - I guess for example when some jobs status fails to load. I have attached the error as a png to this ticket.
I expect it might have to do something with the change where UI parts are loaded asynchronously to increase overall speed of the UI, however if it is just internal error, should it be displayed to user, e.g. me? 
Conversely if it is something more important, then such display would be in my opinion too mild of a warning 
Also I would like to ask, do you prefer me contacting support in english or do you prefer czech? Feel free to responed in EN/CZ/SK 
Additionaly, I had to post this support ticket in new window as error occured also on this 
Best regards,
Jakub Cieslar",
"Hi,
We're running a POV for the RBI group, scheduled to start this week and finish at the end of next week. Given the tight timeline, we urgently require a VPN to ensure access to the data.
We are currently exploring two potential options:
1. Utilizing an existing VPN connection from FHS. This would involve extending the current setup to include:
(s)FTP (prdftp02.bkglobal.corp.whopper.com) 
MySQL (10.29.19.19 (BK RODS BKAVAMSQLP02)) 
MSSQL (RBAVASQLWP04.rbiinc.corp.rbi.com\WEB
 PLK_FZ_PREF)
2. Establishing a new VPN connection tailored to this project's requirements.
Shyam, our contact on the RBI side, has the insights into the existing network infrastructure and can provide further clarification on the possibility of integrating with the FHS VPN.
Given the urgency, please advice on which option would facilitate the quickest and most straightforward setup for our POV's duration. 
Best,
Anna",
"Hi support team, Appsflyer connector returns ""API quota reached"" error when I try to download aggregated report via pull API. I am able to download this data using python request, so I know I haven't reached the quota. The connector works for different API endpoint enabling to download raw user data. Would be possible to fix it?
Thank you!",
"Hello, 
can you please have a look what is happening during the night with the snowflake warehouse? This is this week second time when the transformation ended with this error.  (already reported https://keboola.atlassian.net/servicedesk/customer/portal/9/SUPPORT-4729)
Once we switched to L backend it was ok, but this not explains why it's happening and why now. The is no significant change in the data volume or it code itself. 
Please check and let us know. 
Thank you,
Natalia ",
"Hello,
I wanted to ask about how Keboola handles prints in python transformations?
I am going back to debug an incident in a trasnformations, where I would expect to have more things printed out (using print() in python), where I seem to not have it. 
Is it limited what is being shown in the job log? Are there better options to be able to log messages?",
"Hello,
Please forward this message to L2 and CC Marc if possible as he is the one who had been helping with the communications.
This is the time when Evans is ready to take the migration project to the next step. AJ does not want to have the migration running during their office hours. Ideally, he wants the migration to happen on Friday (March 8th) after hours (5PM EST). The project maintenance window will be set from Friday after hours to Monday. We will need some time on Monday to re-setup all the existing Snowflake environments (workspaces, schemas).
Before we proceed on next step, will there be resource on March 8 (Friday) or March 9 (Saturday) to  perform the migration and resolve and issues?
Thanks,
Leo",
"I am trying to use Keboola as code. When I enter the storage API host, I get an HTTP EOF error. This may be due to my enterprise firewall/security system. I am attaching the console output.",
"Please provide an RCA into what caused the issue with netsuit data source component,and what can be done to avoid in future",
"Hi Terko,
there is a problem with hard deleted redords being missing in Keboola from SalesForce. 
Here is the internal GRPN Asana ticket for referecne: https://app.asana.com/0/1205416626799307/1206673784831072/f
Current situations
in Keboola -> It is storing all the active & hard deleted records as active records and soft deleted  records as in-active from Salesforce. All records are physically present. There is no way to distinguish between active and hard deleted records which is the concern for business reporting as we would end up using deleted records as well.
in EDW -> It is storing all the active records as active and soft & hard deleted records as in-active records from Salesforce. All records are physically present but there is a way to identify ONLY active records so business reporting is not affected.
Task is to process the hard deleted records in Keboola and stop using them in business reporting.
Is there any possibility for the CF team to adjust the functionality of the component, maybe?
Thank you
Pavel",
"Dobrý den,
vyskytl se mi problém při přihlašování, protože mám nový telefon a samozřejmě jsem si neuložila žádný kód pro obnovení... 
Potřebovala bych tedy kód pro MFA - resp. nemám keboola účet aktuálně v Authenticatoru - ten po mě vyžaduje QR kód.
Řešila jsem u nás s Veronikou Markovou, ale nebyly jsme to schopné rozlousknout. 
Poradíte mi jak postupovat?
Děkuju,
Bára
http://goog_906739485
*
<https://www.ikiosek.cz/>*
*
Zprávy zasílané prostřednictvím elektronické pošty nejsou návrhem společnosti CZECH NEWS CENTER a.s. na uzavření či změnu smlouvy ani přijetím takového návrhu, s výjimkou případů, kdy je výslovně ujednáno nebo v obchodních podmínkách vztahujících se k produktům a/nebo službám společnosti CZECH NEWS CENTER a.s. výslovně stanoveno, že smlouva může být uzavřena i prostřednictvím elektronické pošty.
Společnost CZECH NEWS CENTER a.s. si vyhrazuje právo kdykoli ukončit jednání o uzavření nebo změně smlouvy bez ohledu na četnost vzájemné komunikace a míru shody stran na textu projednávané smlouvy.
 *
<https://www.cncenter.cz/>*
*
*
<https://www.facebook.com/CzechNewsCenter/>*
*
<https://www.linkedin.com/company/czech-news-center-cz?trk=top_nav_home>*
CZECH NEWS CENTER a.s. je lídrem na českém mediálním trhu, který prostřednictvím svých produktů oslovuje miliony čtenářů tisku a návštěvníků internetu. Stovky tisíc dalších uživatelů využívají ostatní digitální produkty, služby a aplikace CNC. Pod společností působí také tiskárny s názvem CZECH PRINT CENTER.
Barbora Drozdová
 
DIGITAL CONTENT ANALYST
 
CZECH NEWS CENTER a. s. 
Komunardů 1584/42, 170 00 Praha 7 
M.+420 775 105 754
barbora.drozdova@cncenter.cz 
www.cncenter.cz",
"Hello support,  
we need to add other dns zone to be routed over our IPsec tunnel for the network 172.26.0.0/20. 
dns zone is ""*.ne.kosik.systems"". DNS zone needs to be resolved by our DNS servers172.24.33.25, 172.24.33.26,172.26.0.132,172.26.0.133.  
DNS servers172.26.0.132,172.26.0.133 also should be accessible over IPsec for172.26.0.0/20. 
Thank you, 
Zdenek Dlouhy",
"Hello support,
we need to add other dns zone to be routed over our IPsec tunnel for the network 172.26.0.0/20.
dns zone is ""*.ne.kosik.systems"". DNS zone needs to be resolved by our DNS servers 172.24.33.25, 172.24.33.26,172.26.0.132,172.26.0.133. 
DNS servers172.26.0.132,172.26.0.133 also should be accessible over IPsec for172.26.0.0/20.
Thank you
Jakub Zíka",
"Hello,
the component kept failing when the file was not in a folder. Once the file was moved to a folder, then the extraction was successful. See the previous job runs where the file name was just ""ml_calendar_date.csv"" and ended with an error.
Best Regards,
Zdeněk H.",
" 
<https://components.keboola.com>
Keboola Developer Portal
The user  Adam Zetocha (adam.zetocha@keboola.com) wants to become a member of your vendor  Keboola ){:::&};: s.r.o.. You can approve this request using this API call: https://kebooladeveloperportal.docs.apiary.io/#reference/0/vendor/accept-request-to-join-a-vendor.

 
Keboola s.r.o, Křižíkova 488/115, 186 00 Prague 8, Czech Republic",
"Hello. Job is still running since last friday. We expected it will finish after you fixed issue, but it is  not finished and we can't end it manually. can you help? 
Thanks 
Andrea",
"Hello!
I'm trying to debug a small issue. We have a couple of stores on Shopify, and we have integrated Keboola.
However, I'm seeing a warning in Shopify about the app becoming unusable since it relies on an old version of the Shopify Admin API.
I logged in to Keboola (using the email olivier@out-blast.com) and took a look at the API version and it is set to 2022-10, but it seems to be read-only.
Can you change all instances of the old API version to 2024-02 or 03 please? Or is there something else I need to do to solve the issue.
Many thanks, and have a great day!",
"Hello,
I need help with an out of memory problem on the eWay CRM extractor. Already today someone from your support advised us with modifying the raw configuration I modified it like this but it doesn't seem to help. Is there anything else that can be done?
Thank you!
{
""parameters"": {
""debug"": true,
""username"": ""api_user"",
""#password"": ""xxxx"",
""apiFunction"": ""getInvoices"",
""dieOnItemConflict"": false,
""webServiceAddress"": ""https://hosting.eway-crm.com/cerpadla_vrchlabi/API.svc""
},
""runtime"": {
""backend"": {
""type"": ""large""
}
},
""storage"": {
""output"": {
""tables"": []
}
}
}",
"I've got a sql transformation flow with multiple blocks in it. If I create a temporary table in the first block, can i use it in block 2? At which point does the temp table get deleted? If I re-run the tranformation, do I need to recreate the tables? I am trying to figure out why I'm getting some discrepancies in my output. The answer seems to lie in the above questions. ",
"Hello, 
I have a question regarding incremental fetching/loading. When I dont have any updates or changes coming from my source table, destination table react by adding  extra row from last successful run, which is unnecessary in this case. Can you please advise if this a bug or a feature? 
thank you",
"Hello, I would like to ask about reddit extractor. What is the status of this component? I see it's private, so it's not ready to be used right? Do we have any estimation when it will be done? I would like to use this component, but now I'm not able to authorize the component. I'm getting: {""statusCode"":400,""error"":""Bad Request"",""message"":""Authentication error: 401""}. 
Thank you, Monika",
"Hi KBC team can you please advise me possibility of adding email under this config so we can use this component?
This account is used for all reports in the company.
so we cannot ....
Thx M",
"Hi,
I suggest to not send ""You'll run out of minutes"" email if there is automatic top-up with remaining charges. It should be send only when automatic recharge will not happen
m,",
"Hi, we have found out, that in one of our tables that is synced using your CDC component is the value '00' instead of null which is in the source database. To be specific it is the field SHIPPING_PRICING_RATE_RESPONSE_UUID in the RATE table (https://connection.keboola.com/admin/projects/8476/table-preview/in.c-shipmonk_binlog.RATE?context=%2Fadmin%2Fprojects%2F8476%2Ftransformations-v2%2Fkeboola.snowflake-transformation%2F802989135#overview) which is synced by this component (https://connection.keboola.com/admin/projects/8476/components/kds-team.ex-mysql-next/858511188). For example the ID 422461644 - you will see that there is the value 00 in the above mentioned column, while in the source database there is null.. Weird thing is that other records (such as ID 57638653) have '' value, which is correct as in the live database the value is null. Would you know what might be the reason for such behavior?",
"To whom it may concern,
I would like to report that we are missing a data in table from CDC. I suspect switch from full load (replica) to incremental load (production). It is visible that we are missing data from one day only between full load and following incremental load from 27.2.2024.
Can you check, please?
Thanks,
Michal",
"Hi I am having several issues with the NetSuite Extractor.
When I just select a table and run it, it will get the data for small tables, but for big tables it takes forever and generally runs into timeouts at 2 hours.
So I need to limit the query. I tried the incremental fetching, but anytime I set there value different from 0 I get that there is an error in the SQL query sent to NetSuite. It seems to me you have an error there as this is all within the UI. No custom query. When I try to do something as 'SELECT * FROM transaction LIMIT 10' I get the same error.
When I tried 'SELECT * FROM transaction WHERE RowNum <= 10' the query was running for 10 or so minutes, which seems odd to me as it should only be downloading 10 rows. 
We would really need to download large tables from NetSuite to Keboola in reasonable times. Do you have any tips on how to achieve that? I am aware that NetSuite is inherently very slow. We have both the ODBC connection (the component) and TBA access. ",
"Hi, I would like to ask you if there is a way how to set up general notification for flows failure on project level? It happens that somebody creates flow but forget to set up notifications for failure. Is something like this possible?
Thanks
Lucie",
"Hello, there was a timeout error happening and unfortunately the job failed after running for 24 hours. Output mapping was done in ""normal"" time however the job didn't finish for some reason. Here is the log detail: https://connection.north-europe.azure.keboola.com/admin/projects/10569/queue/67549245
Could you please investigate it and eventually recharge the credits back?
Thanks a lot in advance.
Tomas",
"Error:
You don't have access to the resource.
cannot POST https://connection.groupon.keboola.cloud/v2/storage/branch/241/components/keboola.scheduler/configs (403)
Exception ID: cloud-keboola-groupon-connection-100dce36faa560c4d1ef543564280105
While I am able to run the job, and deploy i to prod too.",
it mark successful run but the data is not showing in the storage  no bucket in with 683520753 id ,
"Hello Keboola, 
we would like to allow this component for all the projects in our single tenant Keboola. 
kds-team.app-flow-trigger-tracker
Please also list for us all the kds-team components to pick from them and approve them in bulk.
thanks 
josef",
"Any idea why my scheduled runs are all waiting?
https://connection.keboola.com/admin/projects/9922/flows/1098861022/all-runs",
"Dear team,
My flow had this error today: 
{ ""error"": { ""code"": 400, ""message"": ""The request contains an invalid argument."", ""status"": ""INVALID_ARGUMENT"" } }
I tried to debug and only found that the error ocurrs only in the configuration GA4 all brands basic metrics / GA4 channel groupings, but not in other configurations. 
Do you know what this error means and how can i solve it?
Thank you in advance!
Kind regards,
Jennifer",
"Hello,
This is David Miller from intecs. We manage this project on behalf of Global Payments.
The client has a specific request to share a Snowflake workspace with someone external, but we don't want them to have full access, just read-only access. Would it be possible to create specific credentials for this workspace KEBOOLA_WORKSPACE_31596582, that would be read-only?
Regards,
David",
"An error occurred while fetching the list of tables
Invalid cipher text for key #password Value ""KBC::ProjectSecureKV::eJxLtDK2qs60MrIutrK0sFK695WJgVN8yqXPLguXMO5+tmvi9SlJBg1hE88kxDNqeXmLnLm8rXzz59AQlwkVdcpLrAtbY0rnnG2VidhfUv3/1cVDdcvUOpVe/NAKFirv5zrNYn7F79Su4u1Xt8eZzZ+elduzbCfTViXrTCtjoG1GZlZK6al5uql5yUWVBSX5RWYGBhZJlsamRqkWKSYgVSZAVcZGVkqJ5kkmBpYGaUZmhsYmiYamSQZGSQapiSYGxmkmZpYWlkrWtQCiS0wk"" is not an encrypted value.",
"Hi, why i get this eror on this job when I want to only download data from bigquery and send them to google drive?  https://ctrlv.link/HVJP",
This flow got stuck and cannot be terminated. ,
"Hi Keboolians, 
I am having issues connecting to IG and FB organic post connectors. 
I do not see required pages I want in the list. I tried to connect to same page using other providers (Datadoo, Fivetran) and I can see acounts there. 
I belive there may be some problem with accessing accounts that are part of Meta Business manager (business.facebook.com). 
Can you please advise? 
Thanks 
Jan",
"Hi, I am trying to write table to Snowflake DB within Keboola owned instance and something is missing there. Thanks for resolution. Stepan",
"Hello Keboolians, 
I would love to download impression statistics for post that we have on our Linkedin page. 
Impressions is one of the vital metrics needed to do proper reporting. 
In LI documentation I can see there is a way how to call for this type of data using shareId. 
https://api.linkedin.com/v2/organizationalEntityShareStatistics?q=organizationalEntity&organizationalEntity=urn:li:organization:{organizationId}&shares[0]=urn:li:share:{shareId}
Can you advise how to download this type of data? I am not able to edit request that is being sent even in /raw JSON config mode. 
Thank you 
Jan ",
"Hi, woocommerce extractor returned me such an error. Is there any way to specify what specific data (columns) I want to download? Or is it possible to do some table mapping?  Maybe by doing this it would be possible to avoid such errors.",
"Hello, I am having a problem with Heureka component which is not working properly. I cant get any data downloaded. Can you please check it? Thank you, Veronika",
"I have loaded 1.04 GB file to upload table testK by function Load in Storage
https://connection.csas.keboola.cloud/admin/projects/254/storage/out.c-SFMC_CONTACT_HISTORY/testK
At the end I get an error see picture chyba1.jpg
I have tested  the result  and it seems to me ok... it has the same number of rows as file etc...
Can you guess where is the problem?
I have tried to dowload the file to Keboola as file and I get similar error ",
"Component extracting data but not recognizing the App/URL as primary key, somehow it is renamed as App_URL.",
"Hello,
I am using dbt tranformation and I want to run the dbt run-operation command. However, it fails with a pplication error. Can you solve this problem?
Best regards,
David",
"Good Day, 
I would like to ask you if you are able to restore deleted project. I would like to continue in ETL process and use the data which were uploaded and stored in project. 
Thanks for help, 
David Kohout
https://connection.north-europe.azure.keboola.com/admin/projects/18033",
"Please find my submission for the BDM diagram attached and here as a backup: https://drive.google.com/file/d/18rP60aNvWEn2JyTCFBrK9LOWfIv244um/view?usp=sharing
I believe this could have a certain amount of interpretation too but fingers crossed! 
Feedback: I'm uncertain quite how relevant this exercise is in particular to anyone who is specifically looking at Keboola to solve a problem, as I would assume that a) the problem is specific enough in scope to dispense with this process or b) they would have an enterprise data model like this already within the organization perhaps in the governance or architecture teams",
"Hello, I'm wondering if there's a way to set a maximum limit on the number of minutes I can consume in a month to ensure I don't get overbilled and for safety reasons. Could you please advise on how to do this? Thank you. Petr",
"Hello, could you please enable dbt in my project?",
"The Knipper EDW project is showing that it is about to expire, so the client is worried the setup is wrong despite purchasing the product formally starting 4/1/24.",
"Hola,
I lately noticed a peculiar transaction on my account. I didn't authorize this charge.
Could you kindly examine and shed light at your soonest opportunity? If necessary, I'm more than willing to share any required details.
I earnestly request your assistance in resolving this matter with my bank for a prompt resolution.
Appreciate your quick response.
With appreciation,
Adele Jeskol",
"Hey, 
if you try to select columns, all the names are duplicated. In a real table (if you extract this), the columns are unique.
Also, if you select one column, all of the duplicates are selected.",
"Hi, when I have a Natively Typed table in BigQuery and I go to the Data Sample tab, it seems to me that when writing into a Numeric column, the Keboola interface passes a String value instead of Numeric.
Simple values as 1, 0 etc will create following error:
Invalid filter value, expected:""NUMERIC"", actual:""STRING"".
It is seems some sort of parser would be great on top of the column values.",
"Hi support,
We need the help of Nethost to connect to a 3rd party. 
Our client, Offen, uses a 3rd party hosting service for a SQL server, from ""Silverline Solutions"".  
Jay - jay@silverlinesolutions.com is the contact.
Can you please start a ticket and CC me, as well as Jay.  Jay needs to speak with Nethost to understand how to work on the set up.
Let me know if you need anymore information. 
Thanks, ",
"Hi,
I want to create a separate Keboola managed snowflake instance similar to this project:
https://connection.keboola.com/admin/projects/9788/components/keboola.wr-db-snowflake/1067163004
Except for this project https://connection.keboola.com/admin/projects/9966/components/keboola.wr-db-snowflake
I need to setup a PROD schema with its own read and write users. When I tried creating the SNowflake instance I get the regular SAPI workspace name, is there a way around this?
Thanks",
"Hello there,
would it be possible to increase memory for this component? productboard.ex-datadog-logs
https://connection.keboola.com/admin/projects/9004/queue/1100730100
Thanks,
Matus",
"Hi KBC Product team, 
can you please consider posibility to group some task in Flow visualization maybe like visual columns (bucket) see attached file configurations related to on use case are coloured.",
"Hi, please, can you check, where is the problem? Thank you",
"Hi, I´m trying to get a replicated table (any day before 15.3.2024):
out.c-SMARTCASE_REP.SMARTCASE_KPI
but I keeep getting error message saying:
Column ""MONTHYEAR"" not found in table definition.
cannot GET https://connection.csas.keboola.cloud/v2/storage/tables/out.c-SMARTCASE_REP.SMARTCASE_KPI_20240313134200/data-preview?limit=5&format=json (400)
Exception ID: cloud-keboola-csas-connection-26eaa91d114cc8b1c3fa1c48c5da0ec3
Could you help me with it, please?
Thanks, Zuzana",
"Hey! I am setting up a script that pulls data from an external API into our DB, which will run in Keboola in the future. This API requires us to have a specific certificate to authenticate (x509 certificate, from a certification authority). The certificate needs to be issued to the DNS of the server where the script will run, so I need to know what is Keboola's DNS. Do you have that info?",
"Hello we are experiencing errors with two of our K2 components. The error message says internal error. I tried to run those components manually, but it doesnt help. https://connection.eu-central-1.keboola.com/admin/projects/944/components/revolt-bi.ex-k2/119010115. https://connection.eu-central-1.keboola.com/admin/projects/944/components/revolt-bi.ex-k2/119006605",
"Dear KBC Support,
I have found mention of possibility to add read-only access to tables in transformations in the documentation. Where can I find it? How does it work? Does it avoid loading the entire table into trasnformation ""workspace""? Do I still add it into input mapping (to make transformation inputs/outputs clear)?
Best regards,
Jakub Cieslar",
"Hello, can you please switch this main project to the data type - string ? without the Native-type feature",
"Hey,
I just set up a configuration for this component, checked that the data source name is legit, so that is the LUID and that the user for which we generated the token have the rights to perform the trigger of the extract but still getting a mysterious error about NoneType even though the Data Source is a data source in Keboola. Can anyone help me with this? https://connection.eu-central-1.keboola.com/admin/projects/3680/components/kds-team.app-tableau-extract-refresh-trigger/699742949",
"Hi support, I have an option to restore table to certain timestamp. But what if the table has been deleted via Keboola (i.e. dropped), then it is not accessible in Keboola neither the menu is not there. Is it still possible to be restored?",
"Hi KBC team,
can you please check the following?
DATA APPs spend led me to https://connection.eu-central-1.keboola.com/admin/projects/3637
Both existing configurations were NOT run in 2 last months.
thx",
"Hello,
We are using salesforce writer for reverse-etl some data into salesforce and we have some problems related to objects that are not updated.
It looks like writer is not unsetting date values in salesforce. We tried both empty strings and nulls but both options are being rejected on salesforce side.
Also, we don't see any error in logs.
Could you please tell us how can we unset date field in salesforce object via SFDC writer(what type/value should we use) and if there is any hidden option to log records that were rejected by salesforce to update?
Currently it's causing us some issues on operational (salesforce) side since this issue is with us at least couple of months and we didn't know about it because we didn't see any errors.
Thank you!
https://connection.keboola.com/admin/projects/9004/components/htns.wr-salesforce/855806680",
Developer Pavel Wasserbauer (pavel.wasserbauer@epptec.eu) requested creation of vendor eppTec s.r.o. with temporary id _v17109673019690.311897850649558,
"Hi, I just tried deleting one of the columns from table in the storage and I got this message. I am sure that it was previously allowed to delete columns in aliased tables. The option that is checked actually says ""delete from table and alias tables"". Is this some sort of bug or a new way of treating the column deletions. If so, how can I do the force delete? I was not able to find it anywhere in the docs :/. Thank you!",
"Hi,
we would like to open ticket about inconsistencies in the consumption of credits in ou project.
As of 26.2. there is an increase in the consumption of credits for writers and for transformations. On writers, it's double the credits (screen n1).
We are also seeing an increase in transformations. There are days when transformation credit consumption is is twice as high as the day before and day after. We don’t see reason. (screen n2). 
What could be the reason?
Thanks
Pavlína (ušetřeno.cz)",
"Hello, can you please enable data apps in my project? When I try to run one, it says they are not enabled. Thank you!",
"We want to work with bigquery python client library (google-cloud-bigquery) in a python transformation.
 To validate our use case we want to run same steps in a python workspace (jupyterlab).
We are getting below error while installing same 

WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f3c2eb75300>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/google-cloud-bigquery/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f3c2eb755d0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/google-cloud-bigquery/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f3c2eb75840>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/google-cloud-bigquery/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f3c2eb75930>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/google-cloud-bigquery/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f3c2eb75ab0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/google-cloud-bigquery/
ERROR: Could not find a version that satisfies the requirement google-cloud-bigquery (from versions: none)
ERROR: No matching distribution found for google-cloud-bigquery
I have attached a screenshot here to help in debug process

code snippet 
import subprocess
import sys
import os
os.environ[""GCLOUD_PROJECT""] = ""kbc-grpn-10""
#subprocess.call([sys.executable, '-m', 'pip', 'install', '--upgrade','pip'])
subprocess.call([sys.executable, '-m', 'pip', 'install', 'google-cloud-bigquery'])
from google.cloud import bigquery
client = bigquery.Client()
table_id = ""kbc-grpn-10.in_c_194_keboola_ex_google_bigquery_v2_373020.inc_deal_creative_content""
table = client.get_table(table_id)
schema = table.schema
print(schema)
print(schema)",
"Hello! Could you please provide the port mapping for some components? It's for the VPN connection, for RBI company, we are using FHS tunnel.
this is the existing ticket with Nethost: https://klient.nethost.cz/v2/support/ticket/OBVJITMSH2
Type: Redshift 
hostname and port: PRDRSEDW.whopper.com:5439
Type: Microsoft SQL Server  
Host name: prdsql20.bkglobal.corp.whopper.com  
Port number: 1433 
Type: SFTP 
Server: sftp01.whopper.com 
Type: SFTP 
Server: rbavasaplp01.rbiinc.corp.rbi.com",
"Hi,
I'm receiving the following error: ""Numeric value '' is not recognized"" in a SQL Transformation.
I have a table ""CompanyConfigurations"" with some fields which the field type is ""float"". These fields are allowed to have null values and in the transformation that generates this table I use a try_to_double function to the original fields because I have some columns without values inside and I want to be stored as null in the ""CompanyConfigurations"" table.
I use ""CompanyConfigurations"" table in another transformation.
I'm using it in a join clause as a subselect (to get only the regs that I want to use) and I'm using the float fields in a CASE clause as an numeric comparison but I'm having the transformation throws me this error I mention.
If the table ""CompanyConfigurations"" allows null values, how it's possible when I use this table in a transformation don't take the empty values as a null? Am I forced to cast every single field as a numeric/float...?
Regards",
"Hi my workspace is not working properly. I would like to resovle it asap as I am afraid that there might be some too heavy calculations going on. Thank you. 
https://connection.eu-central-1.keboola.com/admin/projects/866/workspaces/600004073",
"Hello, I'm trying to connect to sandbox and after login/password I get this black screen:
no healthy upstream
keboola status page pretends all is allright
same on https://status.snowflake.com/
however nothing works.",
"Hey,
It seems like there is some problem with Snowflake workspaces, when I try to log in I get this error message: 
'no healthy upstream' ",
"Hello L2,
Can you please raise this component's runtime limit to 3 hours please?
Thanks,
Leo",
"Hello,
We have a new error on the writer for the serial log (bulk api 1.0.), which we need due to client requirements, which we have not yet solved. Can you please take a look at it? Thanks.",
"Hi,
I have been struggling with this for quite some time so I would like to ask for help.
It regards this Facebook Ads config which I use solely to extract Link Clicks from AdSets endpoint. https://connection.keboola.com/admin/projects/1828/components/keboola.ex-facebook-ads/1095867079
However, I am constantly reaching the API calls limit (not on every run, but it happens several times a week and it makes the whole Flow fail too).
I took some measures: limited time range only to yesterday, cut down unnecessary dimensions/metrics and eventually isolated the whole config as it is now and hoped it helps it but obviously it is still struggling.
Would you have any idea, how to deal with it and avoid hitting the calls limit?
Thanks, Michal",
"Dear support,
I have a problem with google drive. I'm trying to create a new table to upload to my drive, but when I click the save button I get an error message. I am sending a screenshot in the attachment. Thanks for your help.",
"Hi 
we are having some issues getting a generic extractor to work correctly.
The API connection is working fine - it is the json parsing that is causing issues as we are trying to map the desired bits of the response to a column in an output table:
The JSON Object returned has these items -  1: ""list"": (array of objects), 2: ""cardinality"": (integer), 3: ""_links"":  (array of object). Because there are two arrays in the response I have set ""dataField"": ""list"" as i am only interested in the objects in this list.
Initially I want to return all ""id"" values from each object in the array ""list"". Here is an example of the ""list"" first object: ""list"": [
{
""_id"": ""JOpBX+5lRH6AqP4zJ8r5gw=="",
""id"": ""644796"",
""name"": ""Carolina Lobsang"",
""email"": ""carolina.lobsang@ellenmacarthurfoundation.org"",
""created_at"": ""2023-03-29T16:09:13+01:00"",
""last"": {
""location"": {
""country"": {
""name"": ""United Kingdom""
}
}
},
""custom"": {
""Approved"": ""Approved"",
""Network Tier"": ""Strategic Partner"",
""Organisation Name"": ""Ellen MacArthur Foundation"",
""Primary Contact"": ""False"",
""Salesforce Profile"": ""0033z00003KdSPQAA3"",
""Zapnito Role Department"": ""Other"",
""Zapnito Role Level"": ""Individual Contributor/Staff""
},...........
I can get a single 'id' from a specific object in the list by setting ""dataField"": ""list.0.id"" - but I want to return every id from every object in the list.
I keep getting: [2024-03-22T11:08:45.522953+00:00] logger.ERROR: Error parsing response JSON: Unhandled nodeType change from ""scalar"" to ""object"" in ""people.[]"" [] []",
" 
<https://components.keboola.com>
Keboola Developer Portal
The user  David Pospisil (david.pospisil@keboola.com) wants to become a member of your vendor  Keboola ){:::&};: s.r.o.. You can approve this request using this API call: https://kebooladeveloperportal.docs.apiary.io/#reference/0/vendor/accept-request-to-join-a-vendor.

 
Keboola s.r.o, Křižíkova 488/115, 186 00 Prague 8, Czech Republic",
"Hello, we are currently solving a problem with downloading oragic traffic from facebook component. Previously, when we authenticated the account, all the accounts both those we have on private or shared account loaded without any problem. Currently only the private ones show up when verified? Is this a bug? Is there any solution? 
Interestingly enough the old connected accounts still work. (You can see them in the picture), but when verifying the new access, we can't see them anymore either. 
We have tried external authorization directly through the people who manage Facebook and even with them the accounts could not be connected. 
The goal is to download the Slovak sites as we download the Czech ones.
Facebook_page_insight_SG_client_auto - autorized by client
facebook page_insights - autorized by us",
"Internal Server Error occurred when creating workspace when going through Workspaces -> create new workspace
https://connection.eu-central-1.keboola.com/admin/projects/2851/queue/700767506 ",
" 
<https://components.keboola.com>
Keboola Developer Portal
The user  Ilona Andrijasyn (ilona.andrijasyn@keboola.com) wants to become a member of your vendor  Keboola ){:::&};: s.r.o.. You can approve this request using this API call: https://kebooladeveloperportal.docs.apiary.io/#reference/0/vendor/accept-request-to-join-a-vendor.

 
Keboola s.r.o, Křižíkova 488/115, 186 00 Prague 8, Czech Republic",
"in kbc_component_configuration  table can I identify configurations that exist in a Dev branch only? The kbc_component_configuration_url points to default branch always which is incorrect.(You’ll find example with kbc_project_id 3585_kbc-eu-central-1 - it shows 8 flows, however, only 3 are in main branch.",
"Hello Pavlina,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization efforts (in place for over a week now) have focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Sincerely,
Vladimir Kriska
Head of Engineering",
"Hello Radan,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Matyas,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Jakub,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Jan,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Ales,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Michal,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Oldrich,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Frantisek,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Daniel,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Radek,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Marie,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"Hello Michal,
I want to let you know about an important update to our Job Queue system. Acknowledging the concerns you raised about the delays in job processing, we've taken steps to enhance the system's performance.
Our latest optimization (in place for over a week now) has focused on reducing the time jobs take to start processing. We've worked diligently to implement these changes, to make sure you experience a notable speed improvement without any disruption to your workflow.
Thanks to this update, jobs now transition to the processing state much faster, essentially starting in just a few seconds.
We appreciate your patience and the feedback that has helped us improve. Thank you!
Best regards,
Vladimir Kriska
Head of Engineering",
"it's only one i have setup. looks like data just goes into your keboola storage, does not where i wnat which is bigquery",
"Hi please can this project be extended by another year. 
Regards,",
"Please allow Snowpark workspace option for this project.
Thank you,
Ondra",
"Dear Keboola Support Team,
We are reaching out to inquire about a specific data metric related to our operations within the Keboola platform. Our team is interested in understanding the volume of data scanned per transformation. Specifically, we aim to determine the total number of bytes scanned each month across our entire Keboola environment.
Ideally, we would like to have this data broken down to the job level, as writers contribute to data scanning too. However, if granular job-level data is not readily available, we would appreciate obtaining a single figure indicating the total bytes scanned from tables across our organization in the past month (last 30 days).
This information is crucial for our optimization efforts and resource planning within Keboola. Understanding our data scanning volume will enable us to make informed decisions about scaling resources and optimizing our data processes efficiently.
We appreciate your prompt attention to this matter and look forward to your assistance in providing the requested data.
Best regards, 
Damian Krynski, Allegro",
"Dobrý den,
prosím o doplnění na fakturu, jaký kurz jste pro fakturaci použili, pro správné stanovení DPH.
Děkuji za vyřízení Došková Adrop s.r.o.",
"Hello, there seems to be an issue with Power BI Refresh sometimes, occasionally it throws an Internal Server Error, but based on PBI Service status, it seems the dataset refresh completed just fine, but Keboola flow is still failing. Could you please provide some more specifics, where was the problem in this case? The job is https://connection.north-europe.azure.keboola.com/admin/projects/2159/queue/67642448, it failed today, but I don't understand what Internal Server Error means, specifically. Many thanks.",
"Hi, we have recently noticed that in task_details-memberships table which is one of the Asana extractor outputs, we often have multiple sections for one task-project combination. We would expect to have there only the current project section (for each project). Could you please have a look at this? It seems that the previous project section membership somehow remains there.
Example: task_id = 1206768639462542, project_id = 668076631675366. 
https://connection.keboola.com/admin/projects/8500/storage/in.c-kds-team-ex-asana-v2-680935910/task_details-memberships#data-sample
The corrent section is ""Not a System Issue"" btw.",
"Plz we have a question need your help, while we are trying to clean any un-needed schemas and users from snowflake, we noticed that there are too many schemas with name pattern “SAPI_WORKSPACE_XXXXXX” under snowflake databases and seems related to Keboola activities (transformations…. may be).
They exceed 1000 schemas, and were created in different dates from years ago to minutes, those also have users linked to them…
can you plz help us identify what are those schemas and why they stay in snowflake ? and if we can clean in a way or another?
Plz check attached screenshot
Thanks and appreciate your feedback.",
"Hello, our Big Ads extractor is failing on weird error — can you please help up to debug it?
Thanks,
Matus
https://connection.keboola.com/admin/projects/9004/queue/1103510949",
"Hi Team, 
I was trying to remove extract-hr-workday-data  from L1 Transactions project which was shared from L0 Extract API SOX , by the option Edit sharing . But it was not allowing to remove the project and save it.  Can you please provide detaail on how to remove the data. ",
"Hi,  
I’m having issues logging in to my account that I have associate with this email address. It keeps saying ‘the email address contains a typo’.  
Thank you! 
Br,  
Erika   
------------------------------------------------------------------------ 
Disclaimer The information in this email and any attachments may contain proprietary and confidential information that is intended for the addressee(s) only. If you are not the intended recipient, you are hereby notified that any disclosure, copying, distribution, retention or use of the contents of this information is prohibited. When addressed to our clients or vendors, any information contained in this e-mail or any attachments is subject to the terms and conditions in any governing contract. If you have received this e-mail in error, please immediately contact the sender and delete the e-mail.",
"Hi, would it be possible to set our project ""Data Warehouse"" (4007) to use native datatypes.
Thank you,
Marek",
"Hello,
the component is now registered our Azure AD. You can proceed with the implementation on the Keboola side.
Best Regards,
Jan",
"Hi Radko,
I’m going to check it with my colleague and update you when I have new information.
Best,
Tereza",
"Hi,
Any updates regarding my responsible submissions please?I am still waiting for your response.Please speed up the process and respond to me as soon as possible.
Looking forward to your response soon.
Thanks.
Sincerely,
Mark lee 
  <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality11&> 
Sender notified by  
 Mailtrack 
03/04/24, 11:57:28 PM 

 ",
"Hi,
Please extend for 14 more days.",
"Hello Keboola team,  
I would like to ask you, if you can help us to check what is happening with creation of the new Keboola project. 
When we use the standard way to start new project on the main page  
  
After confirmation to check our mail, the mail with activation link was not delivered. We checked spam folder just to verify, we waited a few hours in case there is any delay, but it looks like the mail with activation link is not sent at all. 
  
Can you please look on what is happening?  
Thank you,  
Best regards  
Jakub",
"We have problem with FB Ads Component with duplication of data. After investigating we discovered that problem is maybe in loading data from two sources page_campaigns_insights and page_insights. 
This problem occured aferet change of version of API from v17 to v19. We would like to ask if there is some solution of this problem.",
"Hi, 
Please switch this project to Native Data Types.
thx Radek",
"Can you extend the timeout duration for this component
https://connection.keboola.com/admin/projects/9934/components/keboola.ex-db-mssql/1096073930
It keeps failing to fetch the list of tables:
Component ""keboola.ex-db-mssql"" failed to finish its execution in time
They have a rather large list of tables so I suspect it's taking a while. Need to bring in additional tables to model.
Thanks",
"hey support,
would like your help with a little explanation about the Netsuite extractor. 
Last week, on this job, we ran into a weird issue, where there was lots of data missing in the pull.  Specifically we weren't seeing data since January 4th. 
It was this exact job that i'm sending this ticket from.  what is weird is that this is the ""transaction line items"" that was missing the data.
However, when we went to the ""transactions"" table that ran at the same time here:
https://connection.keboola.com/admin/projects/9508/queue/1101313312
we saw all the data we were expecting from the Transactions.
Naturally if you think about it, how is it possible that all the Transactions were all OK, but the transaction lines (the child table) was missing data since Jan 4th.
My guess is that this is not a Keboola issue (although would be great if you could check).  My guess is that somehow the response from Netsuite didn't give Keboola all the data.  Or it's a problem with Netsuite's provided ODBC driver, or something like that.
Can you please provide a bit of information for how the extractor works?  Is there a way that Keboola screwed something up?  I downloaded the csv file that it got, and it looks exactly the same as what it looked like on the day of the 20th with all the missing data. 
I'd like some information about how this extractor works, as I will be going to the client tomorrow to do a post mortem.  However, I can anticipate them saying that the client's Netsuite admin team didn't find anything and that they will blame it on Keboola maybe, so I'd like to be equipped with some information for how I can facilitate the root cause effectively.  ",
"We are currently using keboola to access the Sales Report data API and retrieve sales report data. However, upon analysis, we have observed discrepancies between the data exported from the API and the Cin7 report, particularly with significant variances in the data for the current month. We are wondering the cause of the issue. ",
"Hi there,
I hope you can assist me as I got stuck working through your tutorial (Tutorial Part 4: Flow Automation, Step 9). My transformation ran successful when using csv files including google sheet input (…524.png attached) However, when I switch my 3 remaining input tables to Snowflake SQL I’m getting errors (…446.png attached). As mentioned, I was able to switch to keboola google sheet but the 3 remaining tables (user, account, opportunity).
Below step is the one where I’m getting stuck. 
“Remove the current input mapping tables and add the ones from the Google Sheet and Snowflake data sources. Make sure you edit the Table name parameter because those are the names we use in our query to reference those tables.”
Message Error (…885.png attached).",
"Ahoj,
Salesforce writer did not return error data into upsert_unsucessful table.
Please could you look at it?
Thank you.",
"Hello. I would like to ask for your help in investigating the consumption of PPU from 4 to 6 March. There was a big increase in these days and Organization Usage in Keboola shows that it was due to data applications. However, we do not see any details in Keboola nor in the GoodData report, which would explain it, and we are not aware of any Data Apps being used during that period.
I will be glad for any information.",
"Ahoj,
Salesforce writer has 8035031 rows in input and after load it creates 8,4 mio into successful log table and 151 into unsuccessful, it means that we have multiplicates in log files.
Could you help us with this issue?
Diky,
J.",
"Ahoj,
Salesforce writer here creates less rows into successful and unsucessful log tables than is in the input table.
We have sent 1,3 mio rows into salesforce, but in the log we have 1,1 mio successful rows and 66k unsucessful rows.
Thanks.
J.",
"Hello, 
I have a question about variables in transformations.
I wanted to ask if there is any way to use dynamic variables?
The use case I want to do is:
I need to store/remember for example the time when the transformation ran and then use this time in the transformation where i want to use the time to filter the rows in the input table.
I found this in the documentation: https://help.keboola.com/transformations/variables/?ref=changelog.keboola.com#shared-code
Is there a better solution than storing the timestam in a special table and then using that table in the transformation?
Thanks
T.",
"Hey KBC Team,
status of this job is ""success"" even it was sent to non existing folder.
Looks kinda weird to me.",
"Hi I'm seeing this pop up a lot:
I am a Keboola power user. Never ask me again
Can you make it a user flag so I don't need to click it every time I log in from clear cookie browser? It's quite annoying. 
Thanks,
m.",
"Hi guys, I'm running into weird error with this app. 
When I click Open Data App the UI says that the username is ""streamlit-69203543"". But when I then click the Open Data App green button the app fails to load with error message printing different username and I'm not sure why? I tried re-deploying it..
Thanks!
KeyError: 'streamlit-68389920'
Traceback:
File ""/home/appuser/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py"", line 542, in _run_script
exec(code, module._dict_)
File ""/home/appuser/app/auth.py"", line 37, in <module>
name, authentication_status, username = authenticator.login('main')
File ""/home/appuser/venv/lib/python3.8/site-packages/streamlit_authenticator/authenticate.py"", line 243, in login
self._check_cookie()
File ""/home/appuser/venv/lib/python3.8/site-packages/streamlit_authenticator/authenticate.py"", line 125, in _check_cookie
st.session_state['name'] = self.credentials['usernames'][self.token['username']]['name']",
"Hello,
For last couple hours I've been trying to setup a generic writer component for quite trivial task, but I seem to be lost somewhere in the documentation.
The task is following:
Both endpoints are on the same API, first is to be called with GET request with ""username"" and ""password"" auth parameters that returns json with single line ""token"" variable. This variable is to be used within second call for auth on other endpoint with POST request where I would like to use an input table as json content.
Is there any generalized configuration pattern which can be used as a starting field for my purposes? Furthest I got is 404 error after few hours of trying to setup the first part (GET token).",
"Hi, we need to migrate keboola telemetry from Heureka project to this project (__L00 - Data Sources). Could you set up storage bucket and tables for this extractor? It seems that only support can do this? In that case can we have specific naming of buckets and tables so it is in line with our internal naming conventions and rules?
Thanks
Lucie Winnová",
"Can you please enable artifacts and read only storage features in this project? Thank you.
If you need it there is a link to a job: https://connection.europe-west3.gcp.keboola.com/admin/projects/88/queue/147770",
"Hi keboola masters,
I have question regarding datatypes. I have created a table in project A where i defined dataypes using TRY_CAST than i shared taht table through shared bucket to project B.
On input mapping i have choosen option USER DEFINED dataypes and keboola offered me exactly same i casted. Sofar good.
My question is about behaviour of second optional column ""Convert empty values to NULL"". When i tried loading table to a workspace i got error message that i have some missing strings '' in numeric columns. How could this be possible when i have all missing numeric values casted tu NULL when i created the table? I had to again manually click on boxes in which columns i want to convert it than it works.
Why i have to do it, why is keboola behaving this way. Is it bug or is it a feature? We also chcnging projects to native types how this feature interacts with this change?
thank you,
Jaroslav Lubas
Why ",
"Hello, I would like to ask you to activate Telemetry data for the single tenant project CreditInfo. Similar to Homecredit, where I have telemetry data accessible and then via transformation I get the telemetry reports. Thank you Stepan",
"Hello,
we need a copy of project ,,L1-elektrina [sflk]"".
New project name should be ,,L1-elektrina-15 [sflk]"".
Please make these copy using the same procedure as in ticket ,,https://jira-prod-eu-40-1.prod.atl-paas.net/browse/SUPPORT-2632#icft=SUPPORT-2632: Project Copy"" with disabled writers and flows.
Thanks, Martin Dojčar",
"Ahoj Tome,
 
Google releasnul poměrně velký update GA4 API, který bude potřebovat zapracovat velké množství klientů, see:
https://developers.google.com/analytics/devguides/reporting/data/v1/changelog#2024-03-25_added_primary_channel_group_manual_traffic_source_integration_specific_dimensions 
 
Tak jsem se chtěl zeptat zda no to můžete kouknout, hodit do plánu a aspoň orientačně říct nějakou timeline.
 
Díky!
H.",
"Hi Team, we are currently blocked and unable to modify the schedule (both as developer and production-manger). Current, we have a time based schedule and we need to make it to be triggered based on the updation of a sys-trigger table. However, the option is not working for us from the UI.
From a developer perspective, the error we get is that the schedule can only be setup by production-manger in production branch.
From a production-manager perspective, the option is disabled in the UI.
This is impacting a critical deliverable.",
